{"path":"sem5/CAD/pdfs/eBOOK_AWS_CAT2_CONTENT.pdf","text":"Enterprises today have become exponentially more agile by leveraging the power of the cloud. In this chapter, we will highlight the scale of AWS Global Infrastructure and teach you about AWS networking foundations. - in the cloud and in this chapter, you will dive deep into AWS networking services. Every business is now running at a global scale and organizations need to target global popu- to scale globally and provide the same user experience across the globe. AWS helps solve these problems through edge networking, and you will learn more about deploying your application for global users without compromising their experience. Furthermore, you will learn about network security and building a hybrid cloud. In this chapter, you will learn about the following topics: • Learning about the AWS Global Infrastructure • AWS networking foundations • Edge networking • Building hybrid cloud connectivity in AWS • AWS cloud network security Without further ado, let’s get down to business. Networking in AWS106 are available in all AWS Regions worldwide, spread across 245 countries. Regardless of the type of technology application you are planning to build and deploy, AWS is sure to provide a service that will facilitate its deployment. AWS has millions of customers and thousands of consulting and technology partners worldwide. Businesses large and small across all industries rely on AWS to handle their workloads. Here are some statistics to give you an idea of the breadth of AWS’s scale. AWS provides the following as its global infrastructure: • 26 launched Regions and 8 announced Regions • 84 Availability Zones • Over 110 Direct Connect locations • Over 310 Points of Presence • 17 Local Zones and 32 announced LZs • 24 Wavelength Zones Now that we have covered how the AWS infrastructure is organized at a high level, let’s learn about the elements of the AWS Global Infrastructure in detail. and Zones. More formally, AWS calls them the following: • AWS Regions • Availability Zones (AZs) • Local Zones (LZs) IMPORTANT NOTE this, it would not be surprising for the numbers to have changed. 107 As shown in the following diagram, an AZ is comprised of multiple distinct data centers, each equipped with redundant power, networking, and connectivity, and located in separate facilities. AWS Regions exist in separate geographic areas. Each AWS Region comprises several independent and isolated data centers (AZs) that provide a full array of AWS services. AWS is continuously enhancing its data centers to provide the latest technology. AWS’s data cen- ters have a high degree of redundancy. AWS uses highly reliable hardware, but the hardware is not foolproof. Occasionally, a failure can happen that interferes with the availability of resources in each data center. Suppose all instances were hosted in only one data center. If a failure occurred with the whole data center, then none of your resources would be available. AWS mitigates this issue by having multiple data centers in each Region. By default, users will always be assigned a default Region. You can obtain more information about how to change and maintain AWS environment variables using the AWS CLI by visiting this link: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html. In the following subsection, we will look at AWS Regions in greater detail and see why they are essential. Networking in AWS108 AWS Regions are groups of data centers in one geographic location that are specially designed to be independent and isolated from each other. A single Region consists of a collection of data centers spread - ity and enhances fault tolerance and stability. While working on the console, you will see AWS your Region. Eventually, all services become generally available (GA) after example, Direct Connect Gateway (DXGW), Identity and Access Management (IAM), Cloud- Front, and Route53. Other services are not global but allow you to create inter-Region fault tolerance and availability. For example, Amazon Relational Database Service (RDS) allows you to create read replicas in https://aws.amazon.com/blogs/aws/cross- region-read-replicas-for-amazon-rds-for-mysql/. One of the advantages of using such an architecture is that resources will be closer to users, in- creasing access speed and reducing latency. Another obvious advantage is that you can serve your clients without disruption even if a whole Region becomes unavailable by planning your disaster recovery workload to be in another Region. You will be able to recover faster if something goes wrong, as these read replicas can be automat- ically converted to the primary database if the need arises. that is usually followed is to list the country code, followed by the geographic region, and the number. For example, the US East Region in Ohio is named as follows: • Location: US East (Ohio) • Name: us-east-2 few exceptions, such as Russia. If you live in Greenland, it may be a little further away from you In addition, AWS has dedicated - plications in this environment. AWS GovCloud offers the same services as other Regions, but it the US government. 109 Regions can be found here: https://docs.aws.amazon.com/AWSEC2/ latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions. As AWS continues to grow, do not be surprised if it offers similar Regions to other governments worldwide, depending on their importance and the demand they can generate. a Region are called AZs. A single AZ consists of multiple data- radius, which is far enough to avoid localized failures, yet achieves a faster data transfer between the data centers. AZs have multiple power sources, redundant connectivity, and redundant re- sources. All this translates into unparalleled customer service, allowing them to deliver highly available, fault-tolerant, and scalable applications. - erties: • Fully redundant • High-bandwidth • Low-latency • Scalable • Encrypted • Dedicated Depending on the service you are using, if you decide to perform a multi-AZ deployment, an AZ will automatically be assigned to the service, but for some services, you may be able to designate which AZ is to be used. Every AZ forms a completely segregated section of the AWS Global Infrastructure, physically de- tached from other AZs by a substantial distance, often spanning several miles. Each AZ operates on a dedicated power infrastructure, providing customers with the ability to run production applications and databases that are more resilient, fault-tolerant, and scalable compared to relying on a single data center. Let’s learn more about them. Networking in AWS110 While AZs focus on covering larger areas throughout regions, such as US-West and US-East, of highly populated cities through LZs. AWS LZs are newer components in the AWS infrastructure family. LZs place select services close to end users, allowing them to create AWS applications that deliver single-digit, millisecond responses. An LZ is the compute and storage infrastructure located close to high-population areas and industrial centers, and offers high-bandwidth, low-latency connectivity to the broader AWS infrastructure. Due to their proximity to the customer, LZs facilitate the delivery of applications that necessitate latency in single-digit milliseconds to end-users. As of March 2023, AWS had has 32 LZs. AWS LZs can run various AWS services, such as Amazon Elastic Compute Cloud, Amazon Virtual Private Cloud, Amazon Elastic Block Store, Amazon Elastic Load Balancing, Amazon FSx, Amazon EMR, Amazon ElastiCache, and Amazon RDS in geographic proximity to your end users. ex- ample, us-west-2-lax-2a https:// aws.amazon.com/about-aws/global-infrastructure/localzones/features/?nc=sn&loc=2. Now you have learned about the different components of the AWS Global Infrastructure, let’s using AWS’s cloud infrastructure: • Security it comes to the data center’s physical security. With AWS’s shared security responsibility that matters for your business. • Availability sure your application is highly available, which means you need to have your workload deployed in a physically separated geographic location to reduce the impact of natural disasters. AWS Regions are fully isolated, and within each Region, the AZs are further isolated partitions of AWS infrastructure. You can use AWS infrastructure with an on-de- mand model to deploy your applications across multiple AZs in the same Region or any Region globally. 111 • Performance which leads to terabits of capacity between regions. Also, you can use AWS Edge AZs for • Scalability your application. With AWS, you can quickly spin up resources, deploying thousands of servers in minutes to handle any user demand. You can also scale down when demand goes down and don’t need to pay for any overprovisioned resources. • Flexibility - ample, you can run applications globally by deploying into any of the AWS Regions and AZs worldwide. You can run your applications with single-digit millisecond latencies by choosing AWS LZs or AWS Wavelength. You can choose AWS Outposts to run applications on-premises. - you covered by providing network services that allow you to create your own secure logical data network services help you to establish connectivity to your users, employees, on-premises data centers, and content distributions. Let’s learn more about AWS’s networking services and how they can help you to build your cloud data center. connectivity is achieved by networking, without which you cannot do anything. Networking concepts are the same when it comes to the cloud. In this chapter, you will not learn what networking is, but instead how to set up your private network in the AWS cloud and es- tablish connectivity between the different servers in the cloud and from on-premises to an AWS in AWS is using Amazon VPC. Networking in AWS112 VPC is one of the core services AWS provides. Simply speaking, a VPC is your version of the AWS cloud, and as the name suggests, it is “private,” which means that by default, your VPC is a logically isolated and private network inside AWS. You can imagine a VPC as being the same as your own logical data center in a virtual setting inside the AWS cloud, where you have complete control over the resources inside your VPC. AWS resources like AWS servers, and Amazon EC2 and Amazon RDS instances are placed inside the VPC, including all the required networking components to Creating a VPC could be a very complex task, but AWS has made it easy by providing Launch VPC Wizard. You can visualize us-east-1a and us-east-1b: In the preceding diagram, you can see VPCs spread across two AZs, where each AZ has two sub- into a private subnet of the us-east-1 AZ. Before going into further details, let’s look at key VPC concepts to understand them better: • Classless Inter-Domain Routing (CIDR) blocks: CIDR is the IP address range allocated to your VPC. When you create a VPC, you specify its set of IP addresses with CIDR notation. 10.0.0.0/16 covers all IPs from 10.0.0.0 to 10.0.255.255, providing 65,535 IP addresses to use. All resources in your VPC must fall within the CIDR range. • Subnets: As the name suggests, the subnet is the VPC CIDR block subset. Partitions of the network are divided by the CIDR range within the range of IP addresses in your VPC. A VPC can have multiple subnets for different kinds of services or functions, like a frontend subnet (for internet access to a web page), a backend subnet (for business logic processing), and a database subnet (for database services). 113 Subnets create trusted boundaries between private and public resources. You should hosted in private subnets. You should use public subnets under controlled access and use them only when it is necessary. As you will keep most of your resources under restricted access, you should plan your subnets so that your private subnets have substantially more IPs available than your public subnets. • Route tables: A routing table contains a set of rules called routes. Routes determine where a new route table and assign subnets to it. For better security, use the custom route table for each subnet. • An Internet Gateway (IGW) between your VPC resources and the public network (the internet). By default, internet to your public subnet through the of your resources that require direct access to the internet (public-facing load balancers, • Network Address Translation (NAT) gateways- ternet access to the private subnet and prevents connections from being initiated from outside to your VPC resources. A private subnet blocks all incoming and outgoing internet - (such as database and application resources) should deploy inside your private subnet. • Security Groups (SGs) and outbound packets. You can only use allow statements in the SG, and everything else resources for one or more instances from the CIDR block range or another SG. As per the principle Internet Control Message Protocol (ICMP). • Network Access Control List (NACL): A NACL is - Networking in AWS114 You should use an SG in most • Egress-only IGWsoutbound communication from Internet Protocol ver- sion 6 (IPv6) instances in your VPC to the internet and prevent the inbound connection from the internet to your instances on IPv6. IPv6, the sixth iteration of the Internet Protocol, succeeds IPv4 and employs a 128-bit IP address. Like IPv4, it facilitates the provision of unique IP addresses required for internet-connected devices to communicate. • DHCP option setsnetwork information, such as DNS name server and domain name used by EC2 instances when they launch. • VPC Flow Logs - cepted and rejected instance. You can create alarms to notify you if can also create metrics to help you identify trends and patterns. log in to the server, always use public-key cryptography for authentication rather than a regular user ID and password method. A VPC resides only within an AWS Region where it can span across one or more AZs within the Region. In the following diagram, two AZs are being utilized within a Region. Furthermore, you can create one or more subnets inside each AZ, and resources like EC2 and RDS are placed inside and public subnet: 115 As shown in this diagram, VPC subnets can be either private or public. As the name suggests, a private subnet doesn’t have access to and from the internet, and a public subnet does. By default, 0.0.0.0/0, via the IGW. Networking in AWS116 However, unique route tables can be assigned to individual subnets. By default, all VPC sub- that can help to build and protect your infrastructure. If attackers can access one component, they must restrict to limited resources by keeping them in their isolated subnet. Due to the ease of VPC creation and building tighter security, organizations tend to create multiple VPCs, which makes simplify this, AWS provides Transit Gateway (TGW). Let’s learn more about it. As customers are spinning more and more VPCs in AWS, there is an ever-increasing need to con- nect various VPCs. Before one-to-one connection, which means that resources within peered VPCs only can communicate with each other. If multiple VPCs need to communicate with each other, which is often the case, it results in a complex mesh of VPC peering. For example, a shown in the diagram below, if you have 5 VPCs, you need 10 peering connections. 117 As you can see in the preceding diagram, managing so many VPC peering connections will be- needs one connection called an attachment to a VPC, and you can establish full- or part-mesh connectivity easily without maintaining so many peering connections. Networking in AWS118 and scalability and eliminates complex VPN or peering connection scenarios when connecting with multiple VPCs and on-premises infrastructure. You can connect attachments. 119 In this section, you have learned how to establish network communication between VPCs, but what about securely connecting to resources such as Amazon S3 that live outside of a VPC or and other AWS services. Let’s learn more about AWS PrivateLink. AWS PrivateLink establishes secure connectivity between VPCs and AWS services, preventing supported AWS services hosted by different AWS accounts. You can access AWS services from a VPC using the Gateway VPC endpoint and Interface VPC endpoint. Gateway endpoints do not support PrivateLink but allow for connection to Amazon services, an interface VPC endpoint can be created to establish a connection to services through AWS PrivateLink. Enabling PrivateLink in AWS requires the creation of an endpoint network interface within the desired subnet, and assignment of a private IP address from the subnet address range for each but you can’t manage it yourself. PrivateLink essentially provides access to the resources hosted in other VPC or other AWS ac- are reachable via a client VPC. As shown in the following diagram, AWS PrivateLink enables private connectivity between the Service Provider and Service Consumer using AWS infrastructure to exchange data without Service Provider creates an Endpoint Service in a private subnet. Networking in AWS120 In contrast, the Service Consumer creates an endpoint in a private subnet with the Service Pro- vider’s service API as the target. As shown in the preceding architecture diagram, the partner sets up an Endpoint Service to expose the service running behind the load balancer (NLB). An NLB is created in each Private Subnet. EC2 instances hosted inside a Private Subnet create a VPC Endpoint with the target as the Endpoint Service and use it to consume the service. Let’s look at another pattern shown in the following diagram, which depicts the use of PrivateLink between a Service Consumer on an AWS account and an on-premises Service Provider. 121 NLB in the Shared Service on-premise NLB is then exposed as an Endpoint ServiceService Consumer account can consume this Endpoint Service by creating a VPC Endpoint. Here, DirectConnect server and AWS Regions. You will learn more about DirectConnect in this chapter in the Building Hybrid Cloud Connectivity in AWS section. Now, many applications run globally and target to harness users in every corner of the world to accelerate their business. In such a situation, it becomes essential that your users have the same experience while accessing your application regardless of their physical location. AWS provides Networking in AWS122 Edge networking is like last-mile delivery in the supply chain world. When you have users across the world, from the USA to Australia and India to Brazil, you want each user to have the same experience regardless of the physical location of your server where the application is hosted. them in detail. Amazon Route 53 is a fully managed, simple, fast, secure, highly available, and scalable DNS service. It provides a reliable and cost-effective means for systems and users to translate names like www.example.com into IP addresses like 1.2.3.4. Route 53 is a domain register where you can register a new domain. You can choose an available domain and add it to the cart from the AWS and between accounts. In Route 53, AWS assigns four name servers for all domains, as shown in the screenshot: one for .com, one for .net, one for .co.uk, and one for .org with the .net DNS services, the other three continue to provide high availability for your domains. Route 53 supports both public and private hosted zones. Public hosted zones have a route to in- ternet-facing resources and resolve from the internet using global routing policies. Meanwhile, private hosted zones have a route to VPC resources and resolve from inside the VPC. It helps to integrate with on-premises private zones using forwarding rules and endpoints. 123 Route 53 provides IPV6 support with end-to-end DNS resolution and support for IPv6 forward 53 resolver will resolve it to the PrivateLink endpoint. Route 53 provides the following • Simple routing policyused for a single resource (for example, a web server cre- ated for the www.example.com website). • Failover routing policyactive-passive failover. • Geolocation routing policy • Geoproximity routing policy used for geolocation when users are shifting from one location to another. • Latency routing policy optimizes the best latency for the resources deployed in multiple AWS Regions. • Multivalue answer routing policy eight healthy, randomly selected records. • Weighted routing policy Networking in AWS124 In the preceding diagram, you can see the policy is Geolocation on the user’s location and its proximity to the nearest Region. In the second level, you have a nested policyWeighted https://docs.aws. amazon.com/Route53/latest/DeveloperGuide/routing-policy.html. Route 53 Resolver rules tell Route 53 to query a domain. For DNS zones that should resolve on-premises, add a forward rule and point toward the appropriate outbound resolver. Public VPC. Private hosted zones integrate with on-premises private zones using forwarding rules and endpoints. Route 53 is the only service in which AWS offers 100% SLA, which means AWS makes its best effort to ensure it is 100% available. If Route 53 does not meet the availability commitment, you will be eligible to receive a Service Credit. deployment Region. AWS provides CloudFront as a content distribution network to solve these latency problems. Let’s learn more about it. Amazon CloudFront is a content delivery service that accelerates the distribution of both static referred to as edge loca- tions. When you use CloudFront to distribute your content, users requesting content get served by the nearest edge location, providing lower latency and better performance. As of 2022, AWS has over 300 high-density edge locations spread across over 90 cities in 47 countries. All edge locations are equipped with ample cache storage space and intelligent rout- ing mechanisms to increase the edge cache hit ratio. AWS content distribution edge locations are connected with high-performance 100 GbE network devices and are fully redundant, with parallel global networks with default physical layer encryption. 125 Suppose you have an image distribution website, www.example.com, hosted in the US, which serves art images. Users can access the URL www.example.com/art.png, and the image is loaded. If your server is close to the user, then the image load time will be faster, but if users from other locations like Australia or South Africa want to access the same URL, the request has to cross As shown in the preceding diagram, when a viewer requests access to page content from the origin www.example.com the user to the CloudFront location. CloudFront uses the following rules for content distribution: • If the requested content is already in the edge data center, which means it is a “cache hit,” it will be served immediately. • If the content is not at the edge location (a “cache miss”), CloudFront will request the AWS backbone, is delivered to the customer, and a copy is kept for future requests. • If you are using CloudFront, it also provides an extra layer of security since your origin server is not directly exposed to the public network. CloudFront eliminates the need to go to the origin server for user requests, and content is served from the nearest location. CloudFront provides security by safeguarding the connection between end-users and the content edge, as well as between the edge network and the origin. By SSL termination to CloudFront, the performance of applications is improved since the burden of processing the required negotiation and SSL handshakes is removed from the origins. Networking in AWS126 CloudFront is a vast topic, and https://aws.amazon.com/ cloudfront/features/. In this section, you learned how CloudFront improves performance for both cacheable content AWS Global Accelerator (AGA), which improves the availability and performance of your appli- cations with local or global users. Let’s learn more about it. a single entry points, or multiple entry points, to AWS Regions, including ALBs, NLBs, and EC2 instances. AGA utilizes the AWS global network to optimize the path from users to applications, thereby improving the the event of an unhealthy endpoint detection. AGA and CloudFront are distinct services offered by AWS that employ the AWS global network and its edge locations. While CloudFront accelerates the performance of both cacheable (e.g., videos and images) and dynamic (e.g., dynamic site delivery and API acceleration) content, AGA with AWS Shield, providing protection against DDoS attacks. You will learn more about AWS Shield in Chapter 8, Best Practices for Application Security, Identity, and Compliance. checks will react to customer backend failure within 30 seconds, which is in line with other AWS load-balancing solutions (such as NLB) and Route 53. Where AGA raises the bar is with its ability reasons to use AGA are: • Accelerate your global applications users to the AWS-based application endpoint, providing consistent performance regard- less of their geographic location. • Improve global application availability endpoints, including but not limited to ALBs, NLBs, and EC2 instances. It instantly reacts endpoint when problems arise. As a result, your users experience higher availability. AGA delivers inter-Region load balancing, while ELB provides intra-Region load balancing. 127 ELB in a Region is a suitable candidate for AGA as it evenly distributes incoming appli- Region. AGA complements ELB by expanding these capabilities beyond any single Region, enabling you to create a global interface for applications with application stacks located in a single Region or multiple Regions. • Fixed entry point to your AWS application. Announced via anycast and delivered from AWS edge locations worldwide, these eliminate the complexity of managing the IP addresses of multiple endpoints and allow you to scale your application and maintain DDoS resiliency with AWS Shield. • Protect your applications ALBs and EC2 instances private. IPs is globally distributed, and end user requests are ingested through AWS’s closest edge location AWS Region an application runs in). Customers will like the simplicity of this managed service. As technology becomes more accessible with the high-speed networks provided by 5G, there is a need to run applications such as connected cars, autonomous vehicles, and live video recogni- tion with ultra-low latency. AWS provides a service called AWS Wavelength, which delivers AWS services to the edge of the 5G network. Let’s learn more about it. AWS Wavelength is designed to reduce network latency when connecting to applications from service in Wavelength Zones, as well as AWS compute and storage limit the full potential of the bandwidth and latency advancements of 5G. AWS Wavelength allows for the creation and implementation of real-time, low-latency appli- enables the deployment of emerging, interactive applications that require ultra-low latency to function effectively. Networking in AWS128 • Ultra-low latency for 5Gcombines the AWS core services, such as com- pute and storage, with low-latency 5G networks. It helps you to build applications with ultra-low latencies using the 5G network. • Consistent AWS experience AWS platform. • Global 5G network which enables ultra-low latency applications for a global user base. Wavelength Zones are connected to a Region and provide access to AWS services. Architecting edge applications using a hub-and-spoke model with the Region is recommended for scalable and cost-effective options for less latency-sensitive applications. cloud. Some applications need to run on-premises and still communicate with the cloud. Let’s learn about AWS services for setting up a hybrid cloud. - ises while creating your cloud migration strategy. You may have decided to keep them out of the cloud for various reasons, such as compliance and the unavailability of out-of-the-box cloud services such as a mainframe, when you need more time to re-architect them, or when you are waiting to complete your license term with an existing vendor. In such cases, you need highly reliable connectivity between your on-premises and cloud infrastructure. Let’s learn about the various options available from AWS to set up hybrid cloud connectivity. AWS VPN is a networking service to establish a secure connection between AWS, on-premises 129 It offers fully managed and highly available VPN termination endpoints at AWS Regions. You can add two VPN tunnels per VPN connection, which is secured with an IPsec Site-to-Site tunnel with VPN connection. As depicted in the preceding diagram, a connection has been established from the customer AWS Client VPN can be used with OpenVPN-based VPN client software to access your AWS re- sources and on-premises resources from any location across the globe. Networking in AWS130 AWS Client VPN also supports a split tunneling feature, which can be used if you only want to AWS Client VPN provides secure access to any resource in AWS and on-premises from anywhere using OpenVPN clients. It seamlessly integrates with existing infrastructure, like Amazon VPC, AWS Directory Service, and so on. AWS Direct Connect is a low-level infrastructure service that enables AWS customers to set up a dedicated network connection between their on-premises facilities and AWS. You can bypass any public internet connection using AWS Direct Connect and establish a private connection linking consistency of connections, and, counterintuitively, can often reduce network costs. 131 through a 1 Gbps, 10 Gbps, or 100 Gbps dedicated Ethernet connection for a single customer. Hosted connections are obtained via an AWS Direct Connect Delivery Partner, who provides the connectivity between your data center and AWS via the partner’s infrastructure. However, it is essential to note that AWS Direct Connect does not provide encryption in transit combine AWS Direct Connect with AWS Site-to-Site VPN to deliver an IPsec-encrypted private connection while, at the same time, lowering network costs and increasing network bandwidth throughput. - cryption for 10 Gpbs and 100 Gpbs dedicated connections between your data centers and AWS be split into several virtual interfaces (VIFs to reach publicly accessible services such as Amazon S3 by using an IP address space and private Connect interface types: • Public virtual interface: - vices globally, which are accessible via public IP addresses such as Amazon S3. A public VIF can access all AWS public services using public IP addresses. • Private virtual interface: You can connect to your VPCs using private VIFs using private IP addresses. You can connect to the AWS Direct Connect gateway using a private VIF, allowing you to connect to up to 10 VPCs globally with a single VIF, unlike connecting a private VIF to a Virtual Private Gateway associated with a single VPC. • Transit virtual interface gateway. A transit VIF is supported for a bandwidth of 1 Gbps or higher. Networking in AWS132 of various interfaces while designing your hybrid cloud connectivity. diagram shows the corporate data center connected to the AWS cloud using the Direct Connect location. Most of your application workloads, such as the web server, app server, and database server, run inside the VPC under a private restricted network, so a private VIF con- nects to VPCs across different AZs. Conversely, Amazon S3 is in the public domain, where you might host static pages, images, and videos for your applications connected through a public VIF. 133 AWS Direct Connect can reduce costs when workloads require high bandwidth. It can reduce these costs in two ways: • It transfers data from on-premises environments to the cloud, directly reducing cost com- mitments to Internet Service Providers (ISPs). • Direct Connect data transfer rates and not the internet data transfer rates, which are lower. Network latency and responses to requests can be highly variable. Workloads that use AWS Direct Connect have a much more homogenous latency and consistent user experience. Direct to optimize costs better. For such cases, you may want to use AWS VPN, as discussed in the AWS section. You have learned about different patterns of setting up network connectivity within an AWS AWS provides Cloud WAN to simplify this issue. Let’s learn more about it. - ample, Petco, which has over 1,500 locations. Imagine what happens on a network like that on Many use AWS Site-to-Site VPN connections for connectivity between their locations and AWS. Alternatively, you bypass the internet altogether and use AWS Direct Connect to create a dedi- cated network link to AWS. And some use broadband internet with SD-WAN hardware to create virtual overlay networks between locations. Inside AWS, you build networks within VPCs and - proaches to connectivity, security, and monitoring. As a result, you are faced with a patchwork of tools and networks to manage and maintain. slightly differently than the others. Ensuring your access policies are synced across the entire network quickly becomes daunting. Likewise, managing and troubleshooting your network is Networking in AWS134 Every new location, network appliance, and security requirement makes things more and more architects need a way to unify their networks so that there is one central place to build, manage, and between their And they need a backbone network that can smoothly adapt to these changes. AWS Cloud WAN is a global network service enabling you to create and manage your infrastruc- ture globally in any AWS Region or on-premises. Consider it a global router that you can use to connect your on-premises networks and AWS infrastructure. AWS Cloud WAN provides net- work segmentation to group your network or workloads, which can be located across any AWS Region. It takes care of the route propagation to connect your infrastructure without manually maintaining the routing. AWS Cloud WAN provides network segmentation, which means you can divide your global net- - When using Cloud WAN, you can see your entire network on one dashboard, giving you one place to monitor and track metrics for your entire network. Cloud WAN lets you spot problems early and respond quickly, which minimizes downtime and bottlenecks while helping you trouble- shoot problems, even when data comes from separate systems. Security is the top priority, and throughout this chapter, you have learned about the security aspect of network design. 135 Let’s go into more detail to learn about network security best practices. Security is always a top priority for any organization. As a general rule, you need to protect ev- watching and secure as if everybody is.” AWS provides various managed security services and a well-architected pillar to help you design a secure solution. You can implement secure network infrastructure by creating an allow-list of permitted protocols, ports, CIDR networks, and SG sources, and enforcing several policies on the AWS cloud. A NACL shown in the following diagram. will means you need both inbound and outbound rules to allow Networking in AWS136 the difference between SGs and NACLs. SG NACL operates at the instance level. A NACL works at the subnet level inside an AWS VPC. You can only add “allow” security rules. You can explicitly add a “deny” rule in addition to allow rules. For example, you can deny access add an inbound allow rule, it automatically adds an outbound allow rule. For example, for a CRM server to respond, you have only range. A NACL is stateless, meaning if you add an inbound allow rule, you must add an explicit allow rule. For a CRM server to respond, you must add both allow and deny rules to accept If you have added multiple rules in the instance, the SG will validate all rules before the rules in numerical order. As an SG is at the instance level, it applies to an instance only. As a NACL is at the subnet level, it automatically applies to all instances in the subnets it’s linked to. While the SG and NACL provide security inside the VPC, let’s learn more about overall AWS net- work security best practices. for your Amazon VPC, which offers a service-level agreement with an uptime commitment of 99.99%. ANFW scales - integration with Amazon S3, Amazon Kinesis, and Amazon CloudWatch, which can act as the destination for these logs. 137 You can learn about various ANFW deployment models by referring to the detailed AWS blog here https://aws.amazon.com/blogs/networking-and-content-delivery/deployment-models- for-aws-network-firewall/. Let’s learn more about network security patterns and anti-patterns, keeping SGs and NACL in mind. Several patterns can be used when creating an SG and NACL strategy for your organization, such as: • Create the SG before launching the instance(s), resource(s), or cluster(s) when creating these programmatically or via a CloudFormation/Landing Zone pipeline. Additionally, you should not use the default SG for your applications. • Logically construct SGs into functional categories based on their application tier or role they perform logically construct the SGs to match those functional components. For a typical three-tier architecture, a minimum of three SGs should be used (e.g., a web tier, an app tier, and a DB tier). • be appropriate, depending on your environment. Generally speaking, organizations can following pattern. • Restrict privileged administrative ports (e.g., SSH or RDP) to internal systems (e.g., bastion hosts) to as bastion hosts or jump boxes. • Create NACL rule numbers with the future in mind be used. For example, if there are 10 rules in an NACL and rule number 2 matches DENY SSH traffic on port 22, the other 8 rules never get evaluated, regardless of their content. rule has a rule number of 200. Networking in AWS138 If you have to add a rule between these rules in the future, you can add a new rule with rule number 150, which still leaves space for future planning. • In well-architected, high-availability VPCs, share NACLs based on subnet tiers subnet tiers in a well-architected, high-availability VPC should have the same resources should have the same inbound and outbound rule requirements. In this case, it is recom- mended to use the same NACL to avoid making administrative changes in multiple places. • Limit inbound rules to the minimum required ports and restrict access to commonly vulnerable ports minimum baseline for inbound rules required for an application tier to function. Reducing - ment of the NACLs. • Finally, audit and eliminate unnecessary, unused, or redundant NACL rules and SGs created or mistakenly left behind from previous changes. While the previous section described patterns for using VPC SGs and NACLs, there are a number • instance or any other AWS resource in your VPC, it is linked to the default SG. Using the default SG does not provide granular control. You can create custom SGs and add them to instances or resources. You can create multiple SGs as per your applications’ needs, such as a web server EC2 instance or an Aurora database server. • If you already have SGs applied to all instances, do not create a rule that references these • should have the same set of SGs applied to them. However, ensure a tier’s instances have a uniform consistency. Suppose web servers, application servers, and database servers co-mingle in the same tier. In this case, they should be separated into distinct tiers and Do not create unique SGs for related instances 139 • NACL rules may be the same now, there is no way to determine future rules required for the resources. Since the resources in the subnet differ (e.g., between different applications), resources in one subnet may need a new rule that isn’t needed for the resources in the other subnet. However, since the NACL is shared, opening up the rule applies resources • - tern shouldn’t be encountered because you are only using NACLs as guardrails. Evaluating performance degradation. It is recommended to periodically audit for SGs and NACLs rules that are unnecessary or redun- accidentally. look for more managed solutions. AWS’s extensive partner network builds managed AWS solutions network security needs. Some of the most popular network security solutions pro- vided by the following integrated software vendor (ISV) partners in AWS Marketplace are below: • Palo Alto Networks cloud security architects to incorporate inline threat and data loss prevention into their • Aviatrix Aviatrix controller (which manages the gateways and orchestrates all connectivity) and the Aviatrix Gateways that are deployed in VPCs using AWS IAM roles. • Check Point Security is a comprehensive security solution designed to protect your AWS cloud environment and assets. It provides advanced, VPN, antivirus, and anti-bot functionality. • Fortinet Networking in AWS140 • Cohesive Networkssoftware-only virtual appliance for connectivity, federation, and security in AWS. In addition to these, many more AWS-managed solutions are available through partners like list of network security solutions https://aws.amazon.com/marketplace/ search/results?searchTerms=network+security. AWS cloud security has multiple components, starting with networking as a top job. You can learn more about AWS security by visiting their security page at https://aws.amazon.com/security/. In this chapter, you started with learning about the AWS Global Infrastructure and understanding the AWS Global Infrastructure. you start with AWS, you create your VPC within AWS. You learned about using an AWS VPC with With the ease of creating VPC in AWS organizations, multiple VPCs tend to be created, whether it is intentional to give each team their own VPC, or unintentional when the dev team creates secure connections with services on the public internet or other accounts using AWS PrivateLink. connecting an on-premises server and an AWS cloud. Finally, you closed the chapter with the network security best practices, patterns and anti-patterns, and third-party managed network security solutions available via the AWS partner network. As you start your journey of learning about the AWS core services, the next topic is storage. AWS provides various types of storage to match your workload needs. In the next chapter, you will Building applications is all about data collection and management. If you design an e-commerce application, you want to show available inventory catalog data to customers and collect purchase data as they make a transaction. Similarly, if you are running an autonomous vehicle application, on that data. As of now, you have learned about networking, storage, and compute in previous chapters. In this chapter, you will learn the choices of database services available in AWS to complete the core architecture tech stack. With so many choices at your disposal, it is easy to get analysis paralysis. So, in this chapter, we will you are using the right tool for the job. In this chapter, you will learn the following topics: • A brief history of databases and data-driven innovation trends • Database consistency model • Database usages model • AWS relational and non-relational database services • Selecting the Right Database Service222 • Choosing the right tool for the job • Migrating databases to AWS By the end of this chapter, you will learn about different AWS database service offerings and how to choose a suitable database for your workload. Relational databases have been and rows in tables are associated with other rows in other tables by using the column values in each row as relationship keys. Another important feature of relational databases is that they normally use Structured Query Language (SQL) to access, insert, update, and delete records. SQL was created by IBM researchers Raymond Boyce and Donald Chamberlin in the 1970s. Relational databases and SQL have served us well for decades. As the internet’s popularity increased in the 1990s, we started hitting scalability limits with relational databases. Additionally, a wider variety of data types started cropping up. Relational Database Management Systems (RDBMSs) were development of new designs, and we got the term NoSQL databases. As confusing as the term is, it does convey the idea that it can deal with data that is not structured, and it deals with it NoSQL by Eric Evans and Johan Oskarsson to describe databases that were not relational. and query it. Let’s see an example of making a choice between a relational and non-relational every customer’s bank account to always be consistent and roll back in case of any error. In such a scenario, you want to use a relational database. For a relational database, if some information is not available, then you are forced to store null or some other value. Now take an example of - and education. In such cases, you want to use a non-relational database and store only the infor- mation provided by the user without adding null values where the user doesn’t provide details (unlike in relational databases). 223 that have the same user data. First Name Last Name City Country Maverick Doe Seattle USA Goose Henske NULL NULL John Gayle London NULL First Name Last Name City Country Maverick Doe Seattle USA Goose Henske John Gayle London country. You can see that only Maverick provided their full information while the other users in- formation with a NULL value across all columns, while in a non-relational database, that column doesn’t exist at all. It is nothing short of amazing what has occurred since then. Hundreds of new offerings have been developed, each trying to solve a different problem. In this environment, deciding the best service or product to solve your problem becomes complicated. And you must consider not only your current requirements and workloads, but also take into account that your choice of database will be able to cover your future requirements and new demands. With so much data getting generated, it is natural that much of innovation is driven by data. Let’s look in detail at how data is driving innovation. Since high-speed internet became available in the last decade, more and more data is getting generated. Before we - tive on data: • The surge of data: Our current era is witnessing an enormous surge in data generation. Managing the vast amount of data originating from your business applications is essential. Selecting the Right Database Service224 However, the exponential growth primarily stems from the data produced by network-con- devices, including but not limited to mobile phones, connected vehicles, smart homes, wearable technologies, household appliances, security systems, industrial equipment, machinery, and electronic gadgets, constantly generate real-time data. Notably, over one- third of mobile sign-ups on cellular networks result from built-in cellular connections in most modern cars. In addition, applications generate real-time data, such as purchase data from e-commerce sites, user behavior from mobile apps, and social media posts/ • Microservices change analytics requirements- izing organizations’ data and analytics requirements. Rather than developing monolithic applications, companies are shifting towards a microservices architecture that divides - faster. Microservices enable developers to break down their applications into smaller and it must be incorporated into every aspect of the business, rather than just being an after-the-fact activity. Monitoring the organization’s operations in real time is critical to fuel innovation and quick decision-making, whether through human intervention or • DevOps driving fast changes - automated development tools to facilitate continuous software development, deploy- ment, and enhancement. DevOps emphasizes effective communication, collaboration, rate of change and change management, enabling businesses to adapt to evolving market needs and stay ahead of the competition. While you see the trend that the industry is adopting, let’s learn some basics of databases and learn about the database consistency model in more detail. 225 In the context of databases, ensuring transaction data consistency involves restricting any da- tabase transaction’s ability to modify data in unauthorized ways. When data is written to the stringent process ensures that data integrity is maintained and that the information stored in the database is accurate and trustworthy. Currently, there are two popular data consistency models. We’ll discuss these models in the following subsections. When database sizes were measured in megabytes, we could have stringent requirements that enforced strict consistency. Since storage has become exponentially cheaper, databases can be much bigger, often measured in terabytes and even petabytes. For this reason, making databas- following: • Atomicity: For an operation to be considered atomic, it should ensure that transactions within the operation either succeed or fail. If one of the transactions fails, all operations should fail and be rolled back. Could you imagine what would happen if you went to the • Consistency structurally sound and consistent after completing each transaction. • Isolation are isolated and don’t contend with each other. Access to data from multiple users is moderated to avoid contention. Isolation guarantees that two transactions cannot coincide. • Durability: After a transaction is completed, any changes a transaction makes should be durable and permanent, even in a failure such as a power failure. BASE model, which we will describe next. If performance were not a consideration, using the ACID model would always be the right choice. BASE only came into the picture because the ACID model could not scale in many instances, especially with internet applications that serve a worldwide client base. Selecting the Right Database Service226 ACID was taken as the law of the land for many years, but a new model emerged with the advent of bigger-scale projects and implementations. In many instances, the ACID model is more pessi- mistic than required, and it’s too safe at the expense of scalability and performance. ACID requirements, such as data freshness, immediate consistency, and accuracy, to gain other AWS account and region. BASE Basic Availability, Soft-state, and Eventual consistency. Let’s explore what this means further: • Basic availability available for the majority of the time (but not necessarily all of data replication when writing a record. • Soft-state - ures provide fault tolerance. As sales come in and get written into the system, different readers numbers, and others may be a few milliseconds behind and not have the latest updates. In this case, the readers will have different results, but if they rerun the query soon after, getting the results fast versus being entirely up to date may be acceptable. • Eventual consistencydata exhibits consistency eventually and maybe not until the data is retrieved at a later point. looser than the ACID model ones, and a direct one-for-one in aggregate databases (including wide-column databases), key-value databases, and document databases. Let’s look at the database usage model, which is a crucial differentiator when storing your data. 227 - always be present. On the ingestion side, the data will be ingested in two different ways. It will either be a data change data capture (CDC) set, which is changes in existing data or accessing brand new data. But what drives your choice of database is not the fact that these two operations are present but rather the following: • • • • • • • • - ogies have been the standards to address these questions for many years: online transaction processing (OLTP) systems and online analytics processing (OLAP that needs to be answered is - is it more important for the database to perform during data ingestion or retrieval? need to be read-heavy or write-heavy. - tions (such as inserts and with less emphasis number of transactions executed in a given time (usually seconds). Data is typically stored using a schema that has been normalized, usually using the 3rd normal form (3NF). Before moving on, let’s quickly discuss 3NF. 3NF is a state that a relational database schema design can possess. Selecting the Right Database Service228 A table using 3NF will reduce data duplication, minimize data anomalies, guarantee referential Edgar F. Codd, the inventor of the relational model for database management. A database relation (for example, a database table) meets the 3NF standard if each table’s col- umns only depend on the table’s primary key. Let’s look at an example of a table that fails to meet columns, contains the employee’s supervisor’s name as well as the supervisor’s phone number. A supervisor can undoubtedly have more than one employee under supervision, so the supervisor’s resolve this issue, we could add a supervisor table, put the supervisor’s name and phone number in the supervisor table, and remove the phone number from the employee table. Conversely, OLAP databases do not process many transactions. Once data is ingested, it is usually retrieval is often performed using some query language (the Structured Query Language (SQL)). Queries in an OLAP environment are often complex and involve subqueries and aggregations. In the context of OLAP systems, the performance of queries is the relevant measure. An OLAP database typically contains historical data aggregated and stored in multi-dimensional schemas (typically using the star schema). For example, a bank might handle millions of daily transactions, storing deposits, withdrawals, and, once aggregated, for more extended reporting periods. 229 As you have learned about the database consistency model and its uses, you must be wondering which model is BASE can be applied for OLAP. Let’s go further and learn about the various kinds of database services available in AWS and how AWS offers a broad range of database services that are purpose-built for every major use case. are battle-tested and provide deep functionality, so you get the high availability, performance, reliability, and security required by production workloads. Selecting the Right Database Service230 - actional applications, such as Amazon RDS and Amazon Aurora, non-relational databases like Amazon DynamoDB for internet-scale applications, an in-memory data store called Amazon ElastiCache for caching and real-time workloads, and a graph database, Amazon Neptune, for developing applications with highly connected data. Migrating your existing databases to AWS is made simple and cost-effective with the AWS Database Migration Service. Each of these da- tabase services is so vast that going into details warrants a book for each of these services itself. database space, but relational databases have served us well for many years without needing any other type of database. A relational database is probably the So, let’s analyze the different relational options that AWS offers us. Given what we said in the previous section, it is not surprising that Amazon has a robust lineup possible to install your database into an EC2 instance and manage it yourself. Unless you have you consider all the costs, including system administration costs, you will most likely be better off and save money using Amazon RDS. Amazon RDS was designed by AWS to simplify the management of crucial transactional appli- cations by providing an easy-to-use platform for setting up, operating, and scaling a relational database in the cloud. With RDS, laborious administrative tasks such as hardware provisioning, - for memory, performance, or I/O, and supports six well-known database engines, including Amazon Aurora (compatible with MySQL and PostgreSQL), MySQL, PostgreSQL, MariaDB, SQL Server, and Oracle. If you want more control of your database at the OS level, AWS has now launched Amazon RDS Custom. It provisions all AWS resources in your account, enabling full access to the underlying Amazon EC2 resources and database environment access. 231 You can install third-party and packaged applications directly onto the database instance as they would have in a self-managed environment while RDS traditionally provides. • Community (Postgres, MySQL, and MariaDB): AWS offers RDS with three different open- source - • Amazon Aurora (Postgres and MySQL): As you can see, Postgres and MySQL are here, as they are in the within the Aurora wrapper can add many started offering the MySQL service in 2014 and added the Postgres version in 2017. Some of these are as follows: a. b. Fivefold performance increase over the vanilla MySQL version c. Automatic six-way replication across availability zones to improve availability and fault tolerance • Commercial (Oracle and SQLServer): Many organizations still run Oracle workloads, so AWS offer the cost of this service, there will be a licensing cost associated with using this service, which otherwise might not be present if you use a community edition. managed database service offered by AWS. Let’s look at its key attributes to make your database more resilient and performant. Multi-AZ deployments - Multi-AZ deployments in RDS provide improved availability and du- rability for database instances, making them an ideal choice for production database workloads. With Multi-AZ DB instances, RDS synchronously replicates data to a standby instance in a dif- ferent Availability Zone (AZ) for enhanced resilience. You can change your environment from Single-AZ to Multi-AZ at any time. Each AZ runs on its own distinct, independent infrastructure and is built to be highly dependable. Selecting the Right Database Service232 In the event of an infrastructure failure, RDS initiates an automatic failover to the standby instance, allowing you to resume database operations as soon as the failover is complete. Additionally, the endpoint for your DB instance remains the same after a failover, eliminating manual adminis- trative intervention and enabling your application to resume database operations seamlessly. Read replicas - RDS makes it easy to create read replicas of your database and automatically keeps them in sync with the primary database (for MySQL, PostgreSQL, and MariaDB engines). Read replicas are helpful for both read scaling and disaster recovery use cases. You can add read repli- cas to handle read workloads, so your master database doesn’t become overloaded with reading requests. Depending on the database engine, you may be able to position your read replica in a different region than your master, providing you with the option of having a read location that in case of an issue with the master, ensuring you have coverage in the event of a disaster. While both Multi-AZ deployments and read replicas can be used independently, they can also be used together to provide even greater availability and performance for your database. In this case, you would create a Multi-AZ deployment for your primary database and then create one or failover capabilities of Multi-AZ deployments and the performance improvements provided by read replicas. Automated backup - With RDS, a scheduled backup is automatically performed once a day during retention period for your backups, which can be up to 35 days. While automated backups are available for 35 days, you can retain longer backups using the manual snapshots feature provided by RDS. RDS keeps multiple copies of your backup in each AZ where you have an instance de- ployed to ensure their durability and availability. During the automatic backup window, storage cause a brief period of elevated latency. However, no I/O suspension occurs help achieve high performance if your application is time-sensitive and needs to be always on. Database snapshots - You can manually create backups of your instance stored in Amazon S3, which are retained until you decide to remove them. You can use a database snapshot to create a new instance whenever needed. Even though database snapshots function as complete backups, you are charged only for incremental storage usage. 233 Data storage - Amazon RDS supports the most demanding database applications by utilizing Amazon Elastic Block Store (Amazon EBS two SSD-backed storage options to choose from: a cost-effective general-purpose option and a volumes to improve performance based on the requested storage amount. Scalability - You can often scale your RDS database compute and storage resources without and price requirements. You may want to scale your database instance up or down, including scaling up to handle the higher load, scaling down to preserve resources when you have a lower load, and scaling up and down to control costs if you have regular periods of high and low usage. Monitoring - RDS offers a set of 15-18 monitoring metrics that are automatically available for you. monitor crucial aspects such as CPU utilization, memory usage, storage, and latency. You can view the metrics in individual or multiple graphs or integrate them into your existing monitoring tool. Additionally, RDS provides Enhanced Monitoring, which offers access to more than 50 additional metrics. By enabling Enhanced Monitoring, you can specify the granularity at which you want for all six database engines supported by RDS. Amazon RDS Performance Insights is a performance monitoring tool for Amazon RDS databases. It allows you to monitor the performance of your databases in real-time and provides insights and recommendations for improving the performance of your applications. With Performance Insights, you can view a graphical representation of your database’s performance over time and potential performance bottlenecks or issues and take action to resolve them. Performance Insights also provides recommendations for improving the performance of your - to improve the overall performance of your application. Security - Controlling network access to your database is made simple with RDS. You can run your database instances in Amazon Virtual Private Cloud (Amazon VPC) to isolate them and Additionally, most RDS engine types offer encryption at rest, and all engines support encryption in transit. RDS offers a wide range of compliance readiness, including HIPAA eligibility. Selecting the Right Database Service234 You can learn more about RDS by visiting the AWS page: https://aws.amazon.com/rds/. As you have learned about RDS, let’s dive deeper into AWS cloud-native databases with Amazon Aurora. Amazon Aurora is a relational database service that blends the availability and rapidity of high- end commercial databases with the simplicity and cost-effectiveness of open-source databases. Aurora is built with full compatibility with MySQL and PostgreSQL engines, enabling applications construct serverless and machine learning (ML managed and automates time-intensive administration tasks, including hardware provision- ing, database setup, patching, and backups. It provides commercial-grade databases’ reliability, availability, and security while costing only a fraction of the price. Amazon Aurora has many key features that have been added to expand the service’s capabilities since it launched in 2014. Let’s review some of these key features: • - tomatically start up, shut down, and adjust its capacity based on the needs of your appli- cation. Amazon Aurora Serverless v2 scales almost instantly to accommodate hundreds ensure the right resources for your application. You won’t have to manage the database capacity, and you’ll only pay for your application’s resources. Compared to peak load provisioning capacity, you could save up to 90% of your database cost with Amazon Aurora Serverless. • Global Database across multiple AWS regions, allowing for faster local reads and rapid disaster recovery. Global Database utilizes storage-based replication to replicate your database across var- ious regions, typically resulting in less than one-second latency. By utilizing a secondary region, you can have a backup option in case of a regional outage or degradation and can quickly recover. Additionally, it takes less than one minute to promote a database in the secondary region to full read/write capabilities. 235 • Encryption - With Amazon Aurora, you can encrypt your databases by using keys you create and manage through AWS Key Management Service (AWS KMS). When you use Amazon Aurora encryption, data stored on the underlying storage and automated backups, snapshots, and replicas within the same cluster are encrypted. Amazon Aurora secures data in transit using SSL (AES-256). • Automatic, continuous, incremental backups and point-in-time restore - Amazon Au- - capability is known as Amazon Simple Storage Service (Amazon S3 - mental, continuous, and automatic, and they have no impact on database performance. • Multi-AZ Deployments with Aurora Replicas - In the event of an instance failure, Amazon Aurora leverages Amazon RDS Multi-AZ technology to perform an automated failover to one of the up to 15 Amazon Aurora Replicas you have established across three AZs. If you have not provisioned any Amazon Aurora Replicas, in the event of a failure, Amazon RDS will automatically attempt to create a new Amazon Aurora DB instance for you. • Compute Scaling - You can scale the provisioned instances powering your deployment process of compute scaling typically takes only a few minutes to complete. • Storage auto-scaling - Amazon Aurora automatically scales the size of your database volume to storage to handle the future growth of your database. • Fault-tolerant and self-healing storage - Each 10 GB chunk of your database volume is replicated six times across three Availability Zones, making Amazon Aurora storage fault-tolerant. It can handle the loss of up to two data copies without affecting write availability, and up to three copies without affecting read availability. Amazon Aurora storage is also self-healing, continuously scanning data blocks and disks for errors and replacing them automatically. • Network isolation - Amazon Aurora operates within Amazon VPC, providing you with the ability to segregate your database within your own virtual network and connect to network access to your DB instances. Selecting the Right Database Service236 • Monitoring and metrics - Amazon Aurora offers a range of monitoring and performance tools to help you keep your database instances running smoothly. You can use Amazon CloudWatch metrics at no additional cost to monitor over 20 key operational metrics, such as compute, memory, storage, query throughput, cache hit ratio, and active connections. If you need more detailed insights, you can use Enhanced Monitoring to gather metrics from the operating system instance that your database runs on. Additionally, you can use Amazon RDS Performance Insights, a powerful database monitoring tool that provides an easy-to-understand dashboard for visualizing database load and detecting performance problems, so you can take corrective action quickly. • Governance AWS infrastructure, providing you with oversight over storage, analysis, and corrective actions. You can ensure your organization remains compliant with regulations such as platform enables you to capture and unify user activity and API usage across AWS Regions and accounts in a centralized, controlled environment, which can help you avoid penalties. • Amazon Aurora machine learning - With Amazon Aurora machine learning, you can incorporate machine learning predictions into your applications through SQL program- ming language, eliminating the need to acquire separate tools or possess prior machine learning experience. It offers a straightforward, optimized, and secure integration between Aurora and AWS ML services, eliminating the need to create custom integrations or move data between them. Enterprise use cases for Amazon Aurora span multiple industries. Here are examples of some of for each: • Revamp corporate applications - Ensure high availability and performance of enterprise applications, including CRM, ERP, supply chain, and billing applications. • Build a Software-as-a-Service (SaaS) application scaling to support dependable, high-performing, and multi-tenant SaaS applications. Amazon Aurora is a good choice for building a SaaS application, as it provides the scal- ability, performance, availability, and security features that are essential for successful SaaS applications. Amazon Aurora automatically creates and maintains multiple replicas SaaS application is always available, even in the event of an outage or failure. 237 Amazon Aurora provides several security features, such as encryption at rest and in transit, to help protect your data and ensure compliance with industry standards and regulations. • Deploy globally distributed applications - Achieve multi-region scalability and resilience for internet-scale applications, such as mobile games, social media apps, and online requirements, databases are often split across multiple instances, but this can lead to over-provisioning or under-provisioning, resulting in increased costs or limited scalability. Aurora Serverless solves this problem by automatically scaling the capacity of multiple - • Variable and unpredictable workloads - If you run an infrequently-used application where you need to provision for peak, it will require you to pay for unused resources. A You will not need to manage or upsize your servers manually. You need to set a min/max capacity unit setting and allow Aurora to scale to meet the load. Amazon RDS Proxy works scalability by enabling applications to share and pool connections established with the database. Let’s learn more details about RDS Proxy. Amazon RDS Proxy is a service that acts as a database proxy for Amazon Relational Database Service (RDS). It is fully managed by AWS and helps to increase the scalability and resilience of • Improved scalability: RDS Proxy automatically scales to handle a large number of con- current connections, making it easier for your application to scale. • Better resilience to database failures: RDS Proxy can automatically failover to a standby replica if the primary database instance becomes unavailable, reducing downtime and improving availability. • Enhanced security: RDS Proxy can authenticate and authorize incoming connections, helping to prevent unauthorized access to your database. It can also encrypt data in transit, providing an extra security measure for your data. Selecting the Right Database Service238 database passwords are managed by AWS Secrets Manager. Aurora put across 2 Availability Zones (AZs) to achieve high availability, where AZ1 hosts Aurora’s primary database while AZ2 has the Aurora read replica. With RDS Proxy, when a failover happens, application connections are preserved. Only trans- actions that are actively sending or processing data will be impacted. During failover, the proxy Amazon RDS Proxy is a useful tool for improving the performance, availability, and security of your database-powered applications. It is fully managed, so you don’t have to worry about the underlying infrastructure, and it can help make your applications more scalable, resilient, and secure. You can learn more about RDS Proxy by visiting the AWS page: https://aws.amazon. com/rds/proxy/. High availability and performance are a database’s most essential and tricky parts. But this prob- lem can be solved in an intelligent way using machine learning. Let’s look at RDS’s newly launched feature, Amazon DevOps Guru, to help with database performance issues using ML. 239 DevOps Guru for Amazon RDS is a recently introduced capability that uses machine learning to automatically identify and troubleshoot performance and operational problems related to rela- or problematic SQL queries and provide recommendations for resolution, helping developers address these issues quickly. DevOps Guru for Amazon RDS utilizes machine learning models to deliver these insights and suggestions. DevOps Guru for RDS aids in quickly resolving operational problems related to databases by notifying developers immediately via Amazon (SNS and EventBridge when issues arise. It also provides diagnostic information, as well as intelligent remediation recommendations, and details on the extent of the issue. AWS keeps adding innovations for Amazon RDS as a core service. Recently, they launched Ama- zon RDS instances available on AWS’s chip Graviton2, which helps them offer lower prices with database near your workload in an on-premise environment. You can learn more about RDS Proxy by visiting the AWS page: https://aws.amazon.com/devops-guru/. Besides SQL databases, Amazon’s well-known NoSQL databases are very popular. Let’s learn about some NoSQL databases. When it comes to NoSQL databases, you need to understand two main categories, key-value and document, as those can needs high throughput, low latency, reads and writes, and endless scale, while the document database stores documents and quickly accesses querying on any attribute. Document and key-value databases are close cousins. Both that in a document database, the value stored (the document) will be transparent to the database and, therefore, can be indexed to assist retrieval. In the case of a key-value database, the value is opaque and will not be scannable, indexed, or visible until the value is retrieved by specifying the key. Retrieving a value without using the key would require a full table scan. Content is stored as Selecting the Right Database Service240 are three simple API operations: • Retrieve the value for a key. • Insert or update a value using a key reference. • Delete a value using the key reference. Binary Large Objects (BLOBs). Data stores keep the value without regard for the content. Data in key-value database records is accessed using the key (in rare instances, are neither supported nor a secondary consideration. Key-value stores often use the hash table pattern to store the keys. No column-type relationships exist, which keeps the implementation details simple. Starting in the key-value category, AWS provides Amazon DynamoDB. Let’s learn more about Dynamo DB. DynamoDB is a fully managed, multi-Region, multi-active database that delivers exceptional performance, with single-digit-millisecond latency, at any scale. It is capable of handling more than 10 trillion daily requests, with the ability to support peaks of over 20 million requests per sec- ond, making it an ideal choice for internet-scale applications. DynamoDB offers built-in security, backup and restore features, and in-memory caching. One of the unique features of DynamoDB is its elastic scaling, which allows for seamless growth as the number of users and required I/O throughput increases. You pay only for the storage and I/O throughput you provision, or on a to support millions of users making thousands of concurrent requests every second. In addition, control and support for end-to-end encryption to ensure data security. • Fully managed • Supports multi-region deployment • Multi-master deployment • Fine-grained identity and access control 241 • Seamless integration with IAM security • In-memory caching for fast retrieval • Supports ACID transactions • Encrypts all data by default DynamoDB provides the option of on-demand backups for archiving data to meet regulatory Additionally, you can enable continuous backups for point-in-time recovery, allowing restoration to any point in the last 35 days with per-second granularity. All backups are automatically en- discoverable and helps meet regulatory requirements. DynamoDB is built for high availability and durability. All writes are persisted on an SSD disk and DynamoDB Accelerator (DAX) is a managed, highly available, in-memory cache for DynamoDB even at high request rates. DAX eliminates the need for you to manage cache invalidation, data population, or cluster management, and delivers microseconds of latency by doing all the heavy lifting required to add in-memory acceleration to your DynamoDB tables. DynamoDB is a table-based database. While creating the table, you can specify at least three components: 1. Keys second, a sort key to sort and retrieve a batch of data in a given range. For example, trans- action ID can be your primary key, and transaction date-time can be the sort key. 2. WCU: Write capacity unitat what rate you want to write your data in DynamoDB. 3. RCU: Read capacity unitat what rate you want to read from your given DynamoDB table. - visioning capacity of the table are equally distributed for all partitions. Selecting the Right Database Service242 - base, and within the table, you have items. two. As more items are added to the table in DynamoDB, it becomes apparent that attributes can differ between items, and each item can have a unique set of attributes. Additionally, the primary useful for establishing one-to-many relationships and facilitating in-range queries. Sometimes you need to query data using the primary key, and sometimes you need to query by provides two kinds of indexes: • Local Secondary Index (LSI) create an LSI with the same primary key (order_ID) and a different secondary key (fulfilled). Now your the same partition as the item in the table, ensuring consistency. Whenever an item is updated, the corresponding LSI is also updated and acknowledged. 243 the parent table can have a different sort key. In the index, you can choose to have just the the partition storage of the original table. • Global Secondary Index (GSI) -A GSI is an extension of the concept of indexes, allowing for more complex queries with various attributes as query criteria. In some cases, using the existing parallel or secondary table with a partition key that is different from the original table and an alternate sort key. When creating a GSI, you must specify the expected workload for read and write capacity units. Similar to an LSI, you can choose to have just the keys want to be returned with the query. Local Secondary Index (LSI) Global Secondary Index (GSI) created during table creation only.table creation. LSI shares WCU/RCU with the main table, so you must have enough read/write capacity to accommodate LSI need. GSI WCU/RCU is independent of the table, so it can scale without impacting the main table. LSI size is a maximum of 10 GB in sync with the primary table partition, which is limited to 10 GB size per partition. As GSI is independent of the main table, so it has no size limits. You can create a maximum of 5 LSIs. You can create up to 20 GSIs. As LSIs tie up to the main table, they offer “strong consistency,” which means their current access to the most updated data. GSI offer “eventual consistency,” which means there may be a slight lag on data updates and lowers the chances you get stable data. So, you may ask when when you want to query data based on an alternate sort key within the same partition key as the base table. Selecting the Right Database Service244 scenar- hand, allows you to query data based on attributes that are not part of the primary key or sort key of the base table. It’s useful when you want to perform ad-hoc queries on different attributes of the data or when you need to support multiple access patterns. A GSI can be used to scale read queries beyond the capacity of the base table and can also be used to query data across partitions. In general, if your data size is small enough, and you only need to query data based on a different sort key within the same partition key, you should use an LSI. If your data size is larger, or you need to query data based on attributes that are not part of the primary key or sort key, you should use a GSI. However, keep in mind that a GSI comes with some additional cost and complexity in terms of provisioned throughput, index maintenance, and eventual consistency. If an item collection’s data size exceeds 10 GB, the only option is to use a GSI as an LSI limits the data size in a particular partition. If eventual consistency is acceptable for your use case, a GSI can be used as it is suitable for 99% of scenarios. DynamoDB is very useful in designing serverless event-driven architecture. You can capture the item-level data change, e.g., putItem, updateItem, and delete, by using DynamoDB Streams. You can learn more about Amazon DynamoDB by visiting the AWS page: https://aws.amazon. com/dynamodb/. You may want a more sophisticated database when storing JSON documents for index and fast search. Let’s learn about the AWS document database offering. In this section, let’s talk about Amazon DocumentDB, “a fully managed MongoDB-compatible da- tabase service designed from the ground up to be fast, scalable, and highly available”. DocumentDB is a purpose-built document database engineered for the cloud that provides millions of requests per second with millisecond latency and can scale-out to 15 read replicas in minutes. It is com- patible with MongoDB 3.6/4.0 and managed by AWS, which means no hardware provisioning, auto-patching, quick setup, good security, and automatic backups are needed. If you look at the evolution of document databases, what was the need for document databases data modeling within applications. Using JSON in the application and then trying to map JSON to relational databases introduced friction and complication. 245 Object Relational Mappers (ORMs) were created to help with this friction, but there were com- plications with performance and functionality. A crop of document databases popped up to solve • Data is stored in documents that are in a JSON-like format, and these documents are documents are stored as values or data types, in DocumentDB, documents are the key design point of the database. • semi-structured data. Additionally, powerful indexing capabilities make querying such documents much faster. • data between your application and the database. • across documents, making it easier to extract insights from data. - - applications quickly over time and build applications faster. In the case of a document structured or semi-structured data value is called a document. It is typically stored using Exten- sible Markup Language (XML), JavaScript Object Notation (JSON), or Binary JavaScript Object Notation (BSON) format types. • Content management systems • E-commerce applications • Analytics • Blogging applications • Requirements for complex queries or table joins • Selecting the Right Database Service246 Compared to traditional relational databases, Amazon DocumentDB offers several advantages, such as: • On-demand instance pricing: You can pay by the hour without any upfront fees or long- term commitments, which eliminates the complexity of planning and purchasing database and testing. • Compatibility with MongoDB 3.x and 4.x: Amazon DocumentDB supports MongoDB 3.6 drivers and tools, allowing customers to use their existing applications, drivers, and tools API on a distributed, fault-tolerant, self-healing storage system, Amazon DocumentDB offers the performance, scalability, and availability necessary for operating mission-critical MongoDB workloads at scale. • Migration support: You can use the AWS Database Migration Service (DMS) to migrate MongoDB databases from on-premises, or on Amazon EC2, to Amazon DocumentDB at no additional cost (for up to six months per instance) with minimal downtime. DMS allows you to migrate from a MongoDB replica set or a sharded cluster to Amazon DocumentDB. • Flexible schema data being stored has a complex or hierarchical structure, or when the data being stored is subject to frequent changes. • High performance: DocumentDB is designed for high performance and can be well suited for applications that require fast read and write access to data. • Scalability: DocumentDB is designed to be horizontally scalable, which means that it can be easily expanded to support large amounts of data and a high number of concurrent users. • Easy querying it easy to retrieve and manipulate data within the database. DocumentDB has recently introduced new features that allow for ACID transactions across mul- tiple documents, statements, collections, or databases. You can learn more about Amazon Docu- mentDB by visiting the AWS page: https://aws.amazon.com/documentdb/. If you want sub-millisecond performance, you need your data in memory. Let’s learn about in-memory databases. 247 Studies have shown that if your site is slow, even for just a few seconds, you will lose customers. A slow site results in 90% of your customers leaving the site. 57% of those customers will purchase Business News Daily (https://www.businessnewsdaily.com/15160-slow-retail-websites- lose-customers.html). Additionally, you will lose 53% of your mobile users if a page load takes longer than 3 seconds (https://www.business.com/articles/website-page-speed-affects- behavior/). are demanding times for services, and to keep up with demand, you need to ensure that users aren’t waiting to purchase your service, products, and offerings to continue growing your business Applications and databases have changed dramatically not only in the past 50 years but also just in the past 10. Where you might have had a few thousand users who could wait for a few seconds - the application and database world to rethink how data is stored and accessed. It’s essential to use the right tool for the job. In-memory data stores are used when there is a need for maximum throughput as you are caching data in memory, which helps to increase the performance by taking In-memory databases, or IMDBs for short, usually store the entire data in the main memory. Contrast this with databases that use a machine’s RAM for optimization but do not store all the data simultaneously in primary memory and instead rely on disk storage. IMDBs generally perform better than disk-optimized databases because disk access is slower than direct memory access. In-memory operations are more straightforward and can be performed using fewer CPU cycles. In-memory data access seeks time when querying the data, which enables faster and more - mance, in-memory operations are usually measured in nanoseconds, whereas operations that require disk access are usually measured in milliseconds. So, in-memory operations are usually about a million times faster than operations needing disk access. Some use cases of in-memory databases are real-time analytics, chat apps, gaming leaderboards, Selecting the Right Database Service248 As shown in the following diagram, based on your data access pattern, you can use either lazy caching or write-through. In lazy caching, the cache engine checks whether the data is in the cache and, if not, gets it from the database and keeps it in the cache to serve future requests. Lazy caching is also called the cache aside pattern. to the caching engine, which tries to load data from the cache. If data is not available in the cache, - azon ElastiCache service is an AWS-provided cache database. Let’s learn more details about it. Amazon ElastiCache is a cloud-based web service that enables users to deploy and manage an in-memory cache with ease. By storing frequently accessed data in memory, in-memory caches can enhance application performance, enabling faster data access compared to retrieving data from a slower backing store such as a disk-based database. ElastiCache offers support for two popular open-source in-memory cache engines: Memcached and Redis. Both engines are well known for their reliability, scalability, and performance. With ElastiCache, you can quickly and easily set up, manage, and scale an in-memory cache in the cloud. 249 clusters, and monitoring the health of the cache environment, allowing you to focus on developing and deploying your application. In addition, ElastiCache integrates seamlessly with other AWS services, making it easy to use the improve the performance and scalability of your overall application architecture. Amazon ElastiCache build data-intensive applications. In-memory data storage boosts applications’ performance by retrieving data directly from memory. Since ElastiCache offers two user of Memcached. If your organization already has committed to Memcached, it is likely not the future, but as of this writing, Redis continues to gain supporters. Here is a comparison of the features and capabilities of the two: Selecting the Right Database Service250 key differences between Redis and Memcached. You can choose by validating your options, and you can learn more about Amazon ElastiCache by visiting the AWS page: https://aws.amazon.com/elasticache/. AWS recently launched Amazon MemoryDB for Redis due to Redis’s popularity. It is a durable, in-memory database service that provides ultra-fast performance and is compatible with Redis is stored in memory, allowing for microsecond read and single-digit millisecond write latency, as well as high throughput. You can learn more about Amazon MemoryDB by visiting the AWS page: https://aws.amazon.com/memorydb/. Now data is getting more complicated with many-to-many relationships and several layers in their common likes. Let’s look at graph databases to solve this problem. Graph databases are data stores that In traditional databases, relationships are often an afterthought. In the case of relational data- bases, relationships are implicit and manifest themselves as foreign key relationships. In graph databases, relationships are these relationships are called edges. certain use cases, they offer much better data retrieval performance than traditional databases. As you can imagine, graph databases are particularly suited for use cases that place heavy im- portance on relationships among entities. constant time. With graph databases, it is not uncommon to be able to traverse millions of edges per second. Graph databases can handle nodes with many edges regardless of the dataset’s number of nodes. You only need a pattern and an initial node to traverse a graph database. Graph databases can easily navigate the adjacent edges and nodes around an initial starting node while caching and aggregating data from the visited nodes and edges. As an example of a pattern and a starting point, you might have a database that contains ancestry information. In this case, the starting point might be you, and the pattern might be a parent. 251 components of a graph database: • Nodes: Nodes are elements attributes, or key-value pairs. Nodes can be given tags, which constitute roles in the do- main. Node labels can be employed to assign metadata (such as indices or constraints) to the nodes. • Edges between two nodes. An edge has a direction, a type, a start node, and an end node. Like a node, an edge can also have properties. In some situations, an edge can have quantitative properties, can share edges regardless of the quantity or type without a performance penalty. Edges 200, 300, or 400. project. It provides an imperative traversal language, called Gremlin, that can be used to write traversals on property graphs, and many open-source and vendor implementations support it. You traversal language could be a favorable option as it offers a method to navigate through property graphs. You might also like openCypher, an open-source declarative query language for graphs, as it provides a familiar SQL-like structure to compose queries for graph data. Selecting the Right Database Service252 Resource Description Framework (RDF), standardized by the W3C in a set of standards labeled, directed multi-graph, but it uses the concept of triples, subject, predicate, and object, to encode the graph. Now let’s look at Amazon Neptune, which is Amazon’s graph database service. Amazon Neptune is a managed service for graph databases, which uses nodes, edges, and prop- suited to represent the complex relationships found in many types of data, such as the relation- ships between people in a social network or the interactions between different products on an e-commerce website. Neptune supports the property graph and W3C’s RDF standards, making it easy to integrate with other systems and tools that support these standards. Neptune also provides a query language called Gremlin which is powerful and easy to use, which makes it easy to perform complex graph traversals and data manipulation operations on the data stored in the database. In addition, Neptune is highly scalable and available, with the ability to support billions of ver- tices and edges in a single graph. It is also fully managed, which means that Amazon takes care of the underlying infrastructure and performs tasks such as provisioning, patching, and backup and recovery, allowing you to focus on building and using your application. You can learn more about Amazon Neptune by visiting the AWS page: https://aws.amazon.com/neptune/. Let’s learn more about it. A time-series database (TSDB) is a database not only important to track what happened but just as important to track when unit of measure to use for the time depends on the use case. For some applications, it might be enough to know on what day the event happened. But for other applications, it might be required • Performance monitoring • Networking and infrastructure applications 253 • Adtech and click stream processing • • Event-driven applications • Financial applications • Log analysis • Industrial telemetry data for equipment maintenance • Other analytics projects types and require different optimization techniques. • Millions of inserts from disparate sources potentially per second • Summarization of data for downstream analytics • Access to individual events properties (which might not be present with other data types): • • • RDBMSes can store this data, but they are not optimized to process, store, and analyze this type is probably an excellent solution to your problem. generate events that need to be tracked and measured, sometimes with real-time requirements. Selecting the Right Database Service254 query processing engine that can make heads or tails of has features that can automate query rollups, retention, tiering, and data compression. Like depending on how much data is coming into the streams. Also, because it’s serverless and fully - ing are not the responsibility of the DevOps team, allowing them to focus on more important tasks. records feature, instead of one measure per table row, making it easier to migrate existing data automatically and periodically run the queries and store the results in a separate table. Addi- the AWS page: https:// aws.amazon.com/timestream/. serve this purpose, let’s learn about ledger databases. A ledger database (LDB and transparent transaction log orchestrated by a central authority: • LDB immutability: Imagine you deposit $1,000 in your bank. You see on your phone that re- after the fact. In other words, only inserts are allowed, and updates cannot be performed. • LDB transparency: In this context, transparency refers to the ability to track changes to - imum, should include who changed the data, when the data was changed, and what the value of the data was before it was changed. 255 • : How can we ensure that our transaction will be im- the transaction is recorded, the entire transaction data is hashed. In simple terms, the string of data that forms the transaction is whittled down into a smaller string of unique characters. Whenever the transaction is hashed, it needs to match that string. In the ledger, the hash comprises the transaction data and appends the previous transaction’s hash. Doing this ensures that the entire chain of transactions is valid. If someone tried to enter another transaction in between, it would invalidate the hash, and it would detect that the foreign transaction was added via an unauthorized method. the ledger records all credits and debits related to the bank account. It can then be followed from a point in history, allowing us to calculate the current account balance. With immutability and other methods, such as RDBMSes, all transactions could be changed or erased. Amazon QLDB is a fully managed service that provides a centralized trusted authority to manage an immutable, transparent, and - tion value changes and manages a track of the history of transactions that need high availability and reliability. Some examples that need this level of reliability are as follows: • Financial credits and debits • • Amazon QLDB offers various blockchain services, such as anonymous data sharing and smart contracts, while still using a centrally trusted transaction log. QLDB is designed to act as your system of record or source of truth. When you write to QLDB, your transaction is append-only interactions and - actions, such as reads, inserts, updates, and deletes, like a transaction and catalogs everything sequentially in this journal. Selecting the Right Database Service256 Once the transaction is committed to the journal, it is immediately materialized into tables and indexes. QLDB provides a current state table and indexed history as a default when you create a new ledger. Leveraging these allows customers to, as the names suggest, view the current states learn more about Amazon QLDB by visiting the AWS page: https://aws.amazon.com/qldb/. Sometimes you need a database for large-scale applications that need fast read and write perfor- mance, which a wide-column store can achieve. Let’s learn more about it. Wide-column databases can sometimes be referred to as column family databases. A wide-column database is a NoSQL database that can store petabyte-scale amounts of data. Its architecture relies on persistent, sparse matrix, multi-dimensional mapping using a tabular format. Wide-column databases are generally not relational. When is it a good idea to use • • Geolocation data • User preferences • Reporting • • Logging applications • Many inserts, but not many updates • Low latency requirements hoc queries: • Heavy requirement for joins • High-level aggregations • Requirements change frequently • Apache Cassandra is probably the most popular wide-column store implementation today. Its architecture allows deployments without single points of failure. It can be deployed across clus- ters and data centers. 257 Amazon Keyspaces (formerly Amazon Managed Apache Cassandra Service, or Amazon MCS) is a fully managed service that allows users to deploy Cassandra workloads. Let’s learn more about it. Amazon Keyspaces (formerly known as Amazon Cassandra) is a fully managed, scalable, and highly available NoSQL database service. NoSQL databases are a type of database that does not use the traditional table-based relational database model and is well suited for applications that require fast, scalable access to large amounts of data. Keyspaces is based on Apache Cassandra, an open-source NoSQL database that is widely used for applications that require high performance, scalability, and availability. Keyspaces provides the with other AWS services. Keyspaces supports both table and Cassandra Query Language (CQL) APIs, making it easy to migrate existing Cassandra applications to Keyspaces. It also provides built-in security features, such as encryption at rest and network isolation, using Amazon VPC and integrates seamlessly with other AWS services, such as Amazon EMR and Amazon SageMaker. Servers are automatically spun up or brought down, and, as such, users are only charged for the servers Cassandra is using at any one time. Since AWS manages it, users of the service never have shell, which is called cqlsh. With cqlsh, you can create tables, insert data into the tables, and access the data via queries, among other operations. Keyspaces supports the Cassandra CQL API. Because of this, the current code and drivers devel- oped in Cassandra will work without changing the code. Using Amazon Keyspaces instead of just Apache Cassandra is as easy as modifying your database endpoint to point to an Amazon MCS service table. In addition to Keyspaces being wide-column, the major difference from DynamoDB is that it supports composite partition keys and multiple clustering keys, which are not available in Dy- namoDB. However, DynamoDB has better connectivity with other AWS services, such as Athena, Kinesis, and Elasticsearch. Selecting the Right Database Service258 Keyspaces provides an SLA for 99.99% availability within an AWS Region. Encryption is enabled by default for tables, and tables are replicated three times in multiple AWS Availability Zones to ensure high availability. You can create continuous backups of tables with hundreds of terabytes of data with no effect on your application’s performance, and recover data to any point in time within the last 35 days. You can learn more about Amazon Keyspaces by visiting the AWS page: https://aws.amazon.com/keyspaces/. In the new world of cloud-born Modern organizations will not only use multiple types of databases for multiple applications, but you can choose the following three options available in AWS based on your workload. Managing and scaling databases in a legacy infrastructure, whether on-premises or self-managed in the cloud (on EC2), can be a tedious, time-consuming, and costly process. You have to worry about operational • - ating backups can be time-consuming and laborious • Performance and availability issues • Scalability issues, such as capacity planning and scaling clusters for computing and storage • Security and compliance issues, such as network isolation, encryption, and compliance programs, including PCI, HIPAA, FedRAMP, ISO, and SOC Instead of dealing with the challenges mentioned above, you would rather spend your time in- novating and creating new applications instead of managing infrastructure. With AWS-managed databases, you can avoid the need to over- or under-provision infrastructure to accommodate costs such as software licensing, hardware refresh, and maintenance resources. AWS manages everything for you, so you can focus on innovation and application development, rather than infrastructure management. You won’t need to worry about administrative tasks such as server your clusters to ensure your workloads are running with self-healing storage and automated scal- ing, allowing you to focus on higher-value tasks such as schema design and query optimization. 259 Let’s look at the next accommodate a decade-old relational database. Purpose-built databases help you to achieve maximum output and performance as per the nature of your application. In the 60s and 70s, mainframes were the primary means of building applications, but by the 80s, Applications became more distributed, but the underlying data model remained mostly struc- tured, and the database often functioned as a monolith. With the advent of the internet in the 90s, three-tier application architecture emerged. Although client and application code became more distributed, the underlying data model continued to be mainly structured, and the database remained a monolith. For nearly three decades, developers typically built applications against a single database. And that is an interesting data point because if you have been in the industry for a while, you often bump into folks whose mental model is, “Hey, I’ve been building apps for a long time, and it’s always against this one database.” - tions are built in the cloud. Microservices have now extended to databases, providing developers with the ability to break down larger applications into smaller, specialized services that cater to Selecting the Right Database Service260 database with a single storage and compute engine that struggles to handle every access pattern. lower latency and the ability to handle millions of transactions per second with many concurrent users. As a result, data management systems have evolved to include specialized storage and trade-offs between functionality, performance, and scale. Plus, what we’ve seen over the last few years is that more and more companies are hiring tech- nical talent in-house to take advantage of the enormous wave of technological innovation that microservices, where they compose the different elements together using the right tool for the powering the most demanding workloads in the cloud. Many factors contribute to the performance, scale, and availability requirements of modern apps: • Users businesses to touch millions of new customers. • Data volume even petabyte-scale data. • Locality users, which complicates the architectures of their solutions/products. • Performance • Request rate experiences in more markets, they need their apps and databases to handle unprecedented levels of throughput. • Access/scale billions of smartphones worldwide, and businesses connect smartphones, cars, manufacturing devices are connected to the cloud. 261 • Economics and hope they’ll succeed, that model is unrealistic in 2023. Instead, they have to hedge their success by only paying for what they use, without capping how much they can grow. applications have wildly different database requirements, which are more advanced and nuanced than simply running everything in a relational database. complex applications into smaller components and choosing the most appropriate tool for each a given task often varies by use case, leading developers to build highly distributed applications using multiple specialized databases. Now you have learned about the different types of AWS databases, let’s go into more detail about moving on from legacy databases. Numerous legacy applications have been developed on conventional databases, and consumers have had to grapple with database providers that are expensive, proprietary, and impose punish- ing licensing terms and frequent audits. Oracle, for instance, announced that they would double licensing fees if their software is run on AWS or Microsoft. As a result, customers are attempting to switch as soon as possible to open-source databases such as MySQL, PostgreSQL, and MariaDB. Customers who are migrating to open-source databases are seeking to strike a balance between - cial-grade databases. Achieving the same level of performance on open-source databases as on AWS introduced Amazon Aurora, a cloud-native relational database that is compatible with MySQL and PostgreSQL to address this need. Aurora aims to provide a balance between the performance and availability of high-end commercial databases and the simplicity and cost-effectiveness of open-source databases. It boasts 5 times better performance than standard MySQL and 3 times better performance than standard PostgreSQL, while maintaining the security, availability, and reliability of commercial-grade databases, all at a fraction of the cost. Additionally, customers can migrate their workloads to other AWS services, such as DynamoDB, to achieve application scalability. Selecting the Right Database Service262 database services that you have learned about, so let’s put them together and learn how to choose the right database. In the previous sections, you learned how to classify databases and the different database ser- vices that AWS provides. In a nutshell, you learned about the following database services under different categories: When you think about the collection of databases shown in the preceding diagram, you may think, “Oh, no. You don’t need that many databases. I have a relational database, and it can take care of all this for you”. Swiss Army knives are hardly the best solution for anything other than the most straightforward task. If you want the right tool for the right job that gives you the expected So no one tool rules the world, and you should have the right tool for the right job to make you spend less money, be more productive, and change the customer experience. Consider focusing on common database categories to choose the right database instead of brows- ing through hundreds of different databases. One such category is ‘relational,’ which many people are familiar with. Suppose you have a workload where strong consistency is crucial, where you that will be asked of the data and require consistent answers. In that case, a relational database 263 Popular options for this category include Amazon Aurora, Amazon RDS, open-source engines like PostgreSQL, MySQL, and MariaDB, as well as RDS commercial engines such as SQL Server and Oracle Database. AWS has developed several purpose-built non-relational databases to facilitate the evolution of application development. For instance, in the key-value category, Amazon DynamoDB is a data- base that provides optimal performance for running key-value pairs at a single-digit millisecond querying data in the same document model used in your application code, then Amazon Docu- - ed an XML data type to become an XML database. However, this approach had limitations, as have replaced XML databases. Amazon DocumentDB, launched in January 2019, is an excellent example of such a database. If your application requires faster response times than single-digit millisecond latency, consider an in-memory database and cache that can access data in microseconds. Amazon ElastiCache offers management for Redis and Memcached, making it possible to retrieve data rapidly for real-time processing use cases such as messaging, and real-time geospatial data such as drive distance. Suppose you have large datasets with many connections between them. For instance, a sports company should link its athletes with its followers and provide personalized recommendations based on the interests of millions of users. Managing all these connections and providing fast queries can be challenging with traditional relational databases. In this case, you can use Amazon data. not just a timestamp or a data type that you might use in a relational database. Instead, a time-series database’s core feature is that the primary axis of the data model is time. an example of a purpose-built time-series database that provides fast and scalable querying of time-series data. Amazon QLDB is a fully managed ledger database service. A ledger is a type of database that is used to store and track transactions and is typically characterized by its immutability and the ability to append data in sequential order. Selecting the Right Database Service264 means that once data is written to the ledger, it cannot be changed, and new data can only be A wide-column database is an excellent choice for applications that require fast data processing - ment, and route optimization. Amazon Keyspaces for Apache Cassandra provides a wide-column database option that allows you to develop applications that can handle thousands of requests per second with practically unlimited throughput and storage. trying to solve. Some of the questions the requirements should answer are as follows: • • • • • Why are these questions - In instances where there is a lot of data and it needs to be accessed quickly, NoSQL databases might be a better solution. SQL vendors realize this and are constantly trying to improve their offerings to better compete with NoSQL, including adopting techniques from the NoSQL world. For example, Aurora is a SQL service, and it now offers Aurora Serverless, taking a page out of the NoSQL playbook. As services get better, the line between NoSQL and SQL databases keeps on blurring, making might want to draw up a Proof of Concept using a couple of options to determine which option Another reason to choose SQL or NoSQL might be the feature offered by NoSQL to create sche- However, tread carefully. Not having a schema might come at a high price. 265 which becomes too variable and creates more problems than it solves. Just because we can create databases without a schema in a NoSQL environment, we should not forgo validation checks before creating a record. If possible, a validation scheme should be implemented, even when using a NoSQL option. It is true that going schema-less increases implementation agility during the data ingestion phase. However, it increases complexity during the data access phase. So, make your choice by making a required trade-off between data context vs. data performance. maintain your relational databases as they scale, consider switching to a managed database service such as Amazon RDS or Amazon Aurora. With these services, you can migrate your workloads and applications without the need to redesign your application, and you can continue to utilize your current database skills. Consider moving to a managed relational database if: • Your database is currently hosted on-premises or in EC2. • You want to reduce the burden of database administration and allocate DBA resources to application-centric work. • You prefer not to rearchitect your application and wish to use the same skill sets in the cloud. • You need a straightforward path to a managed service in the cloud for database workloads. • You require improved performance, availability, scalability, and security. Self-managed databases like Oracle, SQL Server, MySQL, PostgreSQL, and MariaDB can be mi- grated to Amazon RDS using the lift and shift approach. For better performance and availability, MySQL and PostgreSQL databases can be moved to Amazon Aurora, which offers 3-5 times better throughput. Non-relational databases like MongoDB and Redis are popularly used for document and in-memory databases in use cases like content management, personalization, mobile apps, maintain non-relational databases at scale, organizations can move to a managed database ser- vice like Amazon DocumentDB for self-managed MongoDB databases or Amazon ElastiCache for to manage the databases without rearchitecting the application and enable the same DB skill sets to be leveraged while migrating workloads and applications. Selecting the Right Database Service266 As you understand the different choices of databases, then the question comes of how to migrate • Self-service - For many migrations, the self-service path using the DMS and Schema Conversion Tool (SCT) offers the tools necessary to execute with over 250,000 migrations completed through DMS, customers have successfully migrated their instances to AWS. Using the Database Migration Service (DMS), you can make homogeneous migrations from your legacy database service to a managed service on AWS, such as from Oracle to possible, such as converting from SQL Server to Amazon Schema Conversion Tool (SCT) assesses the source compatibility and recommends the best target engine. • Commercially licensed to aws databases looking to move away from the licensing costs of commercial database vendors and avoid vendor lock-in. Most of these migrations have been from Oracle and SQL Server to open- source databases and Aurora, but there are use cases for migrating to NoSQL databases as well. For example, an online store may have started on a commercial or open-source database but now is growing so fast that it would need a NoSQL database like DynamoDB to scale to millions of transactions per minute. Refactoring, however, typically requires application changes and takes more time to migrate than the other migration methods. AWS provides a Database Freedom program to assist with such migration. You can learn more about the AWS Database Freedom program by visiting the AWS page: https://aws. amazon.com/solutions/databasemigrations/database-freedom/. • MySQL Database Migrations - Standard MySQL import and export tools can be used for MySQL database migrations to Amazon Aurora. Additionally, you can create a new Amazon Aurora database from an Amazon RDS for MySQL database snapshot with ease. Migration operations based on DB snapshots typically take less than an hour, although the duration may vary depending on the amount and format of data being migrated. • PostgreSQL Database Migrations - For PostgreSQL database migrations, standard Post- greSQL import and export tools such as pg_dump and pg_restore can be used with Amazon Aurora. Amazon Aurora also supports snapshot imports from Amazon RDS for PostgreSQL, and replication with AWS DMS. 267 • The AWS Data Lab is a service that helps customers choose their platform and understand the differences between self-managed and managed services. It involves a 4-day intensive engagement between the customer and AWS database service teams, supported by AWS solutions architecture resources, to create an actionable deliverable that accelerates the customer’s use and success with database services. Customers work directly with Data Lab architects and each service’s product managers and engineers. At the end of a Lab, the customer will have a working prototype of a solution that they can put into production at an accelerated rate. A Data Lab is a mutual commitment between a customer and AWS. Each party dedicates key personnel for an intensive joint engagement, where potential solutions will be eval- days to create usable deliverables to enable the customer to accelerate the deployment of large AWS projects. After the Lab, the teams remain in communication until the projects are successfully implemented. In addition to the above, AWS has an extensive Partner Network of consulting and software vendor partners who can provide expertise and tools to migrate your data to AWS. In this chapter, you learned about many of the database options available in AWS. You started by revisiting a brief history of databases and innovation trends led by data. After that, you explored You further explored different types of databases and when it’s appropriate to use each one, and - ple database choices available in AWS, and you learned about making a choice to use the right database service for your workload. In the next chapter, you will learn about AWS’s services for cloud security and monitoring. Organizations migrating to the cloud need management and governance to ensure best practices - vices-based architectures, containers, serverless stacks, or legacy applications that have been re-hosted or re-architected for the cloud, you will realize that traditional application development and operations processes could be more effective. avoiding human error disruption. However, you will still observe many manual tasks in most become more critical due to its pay-as-you-go model. Automation helps you improve productivity, has become key to reducing daily operational costs and cloud organizations spending more on operations than upfront capital investment. Automation is crucial for cost and other aspects, such as enduring application security and reli- ability. Automation goes hand in hand with monitoring and alerts. Automation will only work if you have proper monitoring, alerting your automation script when to take a certain action, for example, if you want to run your production app server without compromising the user expe- rience due to a capacity crunch. It’s always recommended to monitor server capacity, such as if the server exhausted memory capacity to 80% or CPU capacity to 70%, and send an alert to autoscaling for server scaling as needed. 312 AWS provides several services and tools to automate your cloud infrastructure and application with CloudOps fully or if you want to automate security using DevSecOps. In this chapter, you will learn the following topics in detail to understand the need for cloud automation, monitoring, and alerts: • • AWS CloudOps pillars • DevOps and DevSecOps in AWS • AWS cloud management tools for automation • Cloud automation best practice By the end of this chapter, you will understand various automation strategies to manage cloud operations. You will learn about various AWS services available at your disposal for cloud auto- mation, monitoring, and alerts and how to use them in your workload. CloudOps, or the cloud operational model, encompasses a collection of guidelines and safeguards that are established, tracked, and adjusted as needed to manage expenses, boost productivity, and mitigate potential security risks. It can help guide your people, processes, and the technology associated with your cloud infrastructure, security, and operations. An operational model also helps you develop and implement controls to manage security, budget, and compliance across your workloads in the cloud. Implementing cloud automation empowers organizations to construct streamlined cloud op- erational models through the - es. Although the concept of cloud computing initially promised the ability to utilize services as required, many organizations still rely on manual processes to provision resources, conduct in substantial labor, error-proneness, and expense. Businesses migrating to the cloud need management and governance to ensure best practices in Chapter 9 313 • Organizations can unlock the speed and agility that comes with the cloud and accelerate their cloud adoption and application modernization efforts as part of their digital trans- formation journey. • Use the power of automation for routine tasks to reduce manual errors and interventions. • Continue to scale your businesses with the certainty that cloud governance spans all different environments uniformly and at scale. • Use your skilled personnel effectively to deliver business outcomes. • Avoid unexpected cost overruns. rewards are considerable. Once the initial hurdles are overcome, the ability to perform intricate tasks with just a single click can transform an organization’s operations. In addition to minimizing manual labor, cloud automation offers several other advantages, including: • Improved security and resilience: Automation helps you improve security as there are always chances of security lapses due to human error or changes that can be left out, which is outside of individual knowledge for setting up security credentials for newly added dev environment. Also, automation helps to improve resiliency by automated recovery of the action by adding more CPU or memory to avoid downtime. • Improved backup processes: Automated backup is one of the most important things for your business process continuation. Protecting your data helps minimize business loss by winning customer trust and rebuilding your environment quickly in case of a disaster recovery event. Automated backup helps you ensure all backups are secure and do not rely on one individual who can forget to take a backup. • Improved governance: Automation helps you to improve governance by making sure all activity is captured across the environment. For example, you need to know what is accessing those environments. All these things are possible through an automated governance model. AWS provides a set of services and third-party tools for modern enterprises as they adopt the cloud operation model. It can help you drive more innovation, faster cloud adoption, improved application performance, and quicker response times to customer feedback while maintaining governance and compliance. 314 While planning your CloudOps model, you need to take a 360-degree look. You want to provision and operate your environment for business agility and governance control. Establishing a Clou- dOps model, regardless of your cloud migration journey, helps you attain consistent governance resources to deliver business outcomes and time-to-market faster while improving safety, ease, As shown in the preceding diagram, the following are the key pillars of CloudOps: 1. Set up Governance: Set up a well-architected, multi-account AWS environment with guardrails to build the foundation for governance. AWS environments with a Well-Ar- chitected Framework checklist ensure you have covered all best practices to monitor and set alerts on your environment for security, operational excellence, cost, reliability, and performance. 2. Enable Compliance: Continuously - sources, remediate failures in an automated fashion, and gather evidence for audits. 3. Provision & Orchestrate: Speed up application and resource provisioning with infra- structure-as-code (IaC) while maintaining consistency and compliance. Chapter 9 315 4. Monitor & Observe: Measure and manage your applications and resources to identify and resolve issues quickly. 5. Centralize Operations entire application portfolio while maintaining safety, security, and compliance. 6. Manage Costs: Helps manage costs with transparency, control, regular forecasting, and - form their operations. Let’s look at what each of these pillars helps you to achieve in your efforts to enable governance - quirement of each pillar. laying a very strong foundation for your governance. In the AWS environment, it begins by setting up a well-architected, multi-account AWS environment and setting up guardrails in each account. You learned about the Well-Architected Framework in , , where you saw that AWS has a comprehensive checklist to make sure your environment is set up properly to monitor cost, security, performance, reliability, and high availability. You can refer to AWS’s well-architected labs here at https://www.wellarchitectedlabs.com/, which provide a very comprehensive, practical, hands-on guide to enable those guardrails against each well-architected - tation and innovation as you grow your footprint on AWS. You need it to scale with your usage. Your business needs to evolve continuously, so you should keep yourself from a single mode of tend to grow with their business. You want to ensure your landing zone grows with your busi- ness without encumbrance while adhering to organizational policies. AWS Landing Zone is an infrastructure that encompasses key services, standardized AWS account architecture, and robust environment that serves as a starting point for new AWS accounts. It helps customers get started with AWS faster by providing a set of reusable blueprints for common patterns and practices, such as account VPCs, security controls, and identity and access management. 316 • Many Teams: Multiple teams could be in the same account, overstepping one another. • Isolation: Each team could have different security needs and want to isolate themselves • Security Controls: Different applications might have different controls around them to address security and compliance. For example, talking to an auditor is far easier than point- ing to a single account hosting the PCI solution. But even within an organization, security controls provide the ability to isolate certain things based on security isolation needs. • Business Process business units (BUs) or products. For ex- ample, the Sales BU is different from the HR BU with an entirely different business process. • Billing: An account is the primary way to divide items at a billing level. Each AWS account that if you have multiple accounts within an organization, each account will have its own billing and cost allocation data. AWS Organizations that help you to establish a multi-ac- count structure for centralized governance. You can use it to establish granular control over your AWS accounts, manage across accounts easily, and apply policies as broadly or as narrowly as you need. In the previous chapter, Chapter 8, Best Practices for Application Security, Identity, and Compliance, you learned about AWS Organizations. For automated setup, AWS provides AWS Control Tower, a self-service solution to set up and govern a secure, compliant multi-account AWS environment. It abstracts multiple AWS services under the covers, so you can use it to set up your environment, based on best practices, without • Automate the setup of your landing zone based on best-practice blueprints • Apply guardrails for ongoing governance over your AWS workloads • • Get dashboard visibility into your organizational units, accounts, and guardrails Chapter 8, Best Practices for Appli- cation Security, Identity, and Compliance. AWS professional services provide AWS Landing Zone Accelerator (LZA), which is a set of tools and resources provided by AWS to help customers accelerate the deployment of a secure, multi-ac- count AWS environment. Chapter 9 317 LZA builds on top of the AWS Landing Zone service, which provides a pre-built, opinionated framework for setting up a secure, multi-account environment. requirements and leverages automation to speed up the deployment process. It also provides access to AWS experts who can provide guidance and best practices to help ensure a successful deployment. You can learn more about LZA by referring AWS user guide here: https://aws. amazon.com/solutions/implementations/landing-zone-accelerator-on-aws/. LZA is designed to accelerate the process of setting up a landing zone for workloads that are mi- grating to AWS. LZA provides a modular set of landing zone components that can be customized control over the individual components of their environment. - dashboard for managing and monitoring multiple accounts, making it easier for customers to customers who need to manage multiple AWS accounts and want a pre-built set of governance policies to enforce best practices. It is extremely important to have the correct foundation when you are starting with your cloud - what allows us to succeed in the long term. After setting up governance, you must ensure your applications meet compliance requirements. Let’s learn more about automating compliance. As you migrate workloads to the cloud, you need to know that you can maintain cloud compliance and get assurance for your workloads. Once compliance mechanisms and processes are in place, you can empower your development teams to build and innovate while having peace of mind that they are staying compliant. 318 - your AWS environment and keep a running audit log to get visibility into your organization’s Many customers follow the Institute of Internal Auditors (IIA) guidance for the three lines of defense: • 1st Line: How to automate compliance management and manage risk automation of compliance management and risk mitigation within an AWS environment. • 2nd Line: How to implement continuous oversight and oversee risk CloudWatch and AWS Security Hub, it is possible to understand the operational health and security status of AWS accounts. • 3rd Line: How to assess and independently gather assurance of risk management Manager helps assess their security, change management, and software licensing controls. Chapter 8, Best Practices for Application Security, Identity, and Compliance. Let’s learn about the other services, de- pending on mentioned above for managing cloud audits and compliance. You can use a combina- In AWS, all interactions with AWS services and resources are handled through AWS API calls, and made in your AWS account and provides a complete history of all user activity and API usage. are encrypted using Amazon S3 server-side encryption (SSE), which provides an additional layer of security for your logs. It’s also of whether they come directly from a user or on behalf of a user by an AWS service. Chapter 9 319 - curity and compliance purposes. for AWS accounts by logging and monitoring account activity related to actions across the AWS history includes actions taken through the AWS Management Console, AWS SDKs, command-line tools, and other AWS services. - pliance risks in real time. also have several different data event sources you can opt into depending on your application security analysis, resource change tracking, and troubleshooting. You can learn more about AWS https://aws.amazon.com/cloudtrail/. , Storage in AWS – Choosing the Right Tool for the Job, in the context of S3. AWS learn about these in more detail in the context of CloudOps: • to your AWS • Resource relationship tracking between AWS resources in your account. For example, if a new Amazon Elastic Compute Cloud (Amazon EC2) security group is associated with an Amazon EC2 instance, AWS and the Amazon EC2 instance. 320 • changes within your Amazon EC2 instances and servers running on-premises or with • changes for compliance against built-in or custom rules and automate the remediation of non-compliant resources. You can customize pre-built rules provided by AWS • Conformance packs - • Automatic remediation enables you to remediate non-compliant resources using Systems Manager Automation documents. • Cloud governance dashboard easily identify non-compliant resources and take the necessary corrective action. You can customize the dashboard to monitor resources based on cost and security. • Multi-account, multi-region data aggregation from multiple AWS accounts and regions, providing you with a centralized view of your useful for enterprise-scale organizations. • whether Amazon EC2 instances have an associ- 22 (SSH): { \"Name\": \"ec2-security-group-has-inbound-rules-on-port-22\", \"Description\": \"Checks whether the security group associated with an EC2 instance has inbound rules that allow traffic on port 22\", Chapter 9 321 \"Scope\": { \"ComplianceResourceTypes\": [ \"AWS::EC2::Instance\" ] }, \"Source\": { \"Owner\": \"AWS\", \"SourceIdentifier\": \"EC2_INSTANCE_HAS_SECURITY_GROUP_WITH_INBOUND_ RULES_ON_PORT_22\" }, \"InputParameters\": \"{\\\"allowedProtocols\\\":\\\"tcp\\\",\\\"portNumber\\\":22}\" } - 22. If any instances do not have such a security group, they non-compliant. visiting the AWS page here: https://aws.amazon.com/config/. Let’s look at the next service for tracking and auditing licenses. AWS License Manager is a one-stop solution for managing licenses from various software vendors across hybrid responsible for managing license true-ups and vendor audits. In contrast, users spin up instances and use the licensed software on those instances. With AWS License Manager, the administrators can now easily manage licenses. Users in the organization are not required to do additional work to manage licenses and can focus on business as usual. You can complete your licensing true-ups and audits using AWS License Manager. Administra- Management Console, CLI, or API. Furthermore, administrators can enforce licensing rules by attaching them to instance launches. Once rules are enforced, the service automatically keeps track of instances as users spin them up and down. 322 software after spinning up instances. Finally, administrators can keep track of usage through the AWS License Manager’s built-in dashboard. AWS License Manager automatically keeps track of instance launches, and the built-in dashboard is populated. Administrators can view usage limit alerts and take actions such as procuring more licenses as needed. When there is an upcoming license true-up or audit, administrators no longer have to determine which instances use which licenses. With AWS License Manager’s built-in is no longer a challenge. You can learn more about AWS License Manager by visiting the AWS page here: https://aws.amazon.com/license-manager/. CloudWatch is one of the essential services for running your cloud operation. It allows you to monitor your AWS workload and take action based on alerts. In addition, CloudWatch provides observability for your AWS resources on a single platform across applications and infrastructure. Amazon CloudWatch is a powerful monitoring service that is designed to help you optimize your AWS resources and applications. It offers a wide range of capabilities, including: • Data and operational insights: CloudWatch provides valuable insights into the perfor- mance and health of your AWS resources and applications. With CloudWatch, you can • Resource monitoring: CloudWatch can monitor a variety of AWS resources, including to quickly identify and troubleshoot any issues that arise. • Custom metrics: CloudWatch allows you to create custom metrics based on the data over the monitoring process. • Log monitoring - to your application code. Amazon CloudWatch is an essential tool for anyone running applications on the AWS cloud platform. Its powerful monitoring capabilities can help you optimize your resources, improve application performance, and maintain the operational health of your systems. Chapter 9 323 CloudWatch alarms actions based on the rules of metrics and set up alerts that notify you when certain conditions are met. For example, you can send an email alert to the admin whenever the average network latency of an Amazon RDS database exceeds 10 seconds or when the CPU usage of an Amazon EC2 instance falls below 10%. You can also create more complex alarms that automatically trigger actions, such as launching demand. utilization, application performance, or other key metrics, CloudWatch alarms can help you stay on top of your cloud infrastructure and ensure that it is always running at peak performance. In addition, CloudWatch provides data for the past two weeks so that you can access historical data for analysis of past events. It also integrates with other AWS services, such as Amazon EC2 Auto Scaling, Amazon SNS, and AWS Lambda, enabling you to use CloudWatch to react to changes in your resources and applications. Some key features of CloudWatch include the following: • Metrics: CloudWatch allows you to collect metrics for your resources and applications, such as CPU usage, network in the CloudWatch console or use the CloudWatch API to retrieve them programmatically. • Alarms breached. For example, you can schedule an alarm to send an email or SMS message to you if the CPU usage on one of your Amazon EC2 instances exceeds a certain threshold. • Logs can use CloudWatch Logs Insights to search and analyze your log data or use CloudWatch Logs to export your log data to third-party tools for further analysis. • Dashboards: You can use CloudWatch dashboards to create custom views of your metrics and log data to quickly get an overview of your system’s health and performance. 324 Amazon CloudWatch Events is a service that allows you to respond to changes in state in your AWS resources in real time. With CloudWatch Events, you can monitor for operational changes in your resources and automatically trigger actions based on those changes. For example, you can create a CloudWatch Events rule that triggers an AWS Lambda function whenever a new - plications on the instance. CloudWatch Events allows you to react to operational changes in your AWS resources quickly that trigger your actions and specify the targets for executing those actions. CloudWatch Events rules enable you to match event patterns and take actions in response to those patterns. A rule can have one or more event patterns, and you can specify the type of action that CloudWatch Events takes when it detects a pattern. For example, you can set up a rule to send an email message when a new Amazon EC2 instance is launched or to stop an Amazon EC2 instance when the CPU utilization is too high. automatic stopping of Amazon EC2 instances so that you don’t incur charges for no longer needed instances. You can learn more about AWS CloudWatch by visiting the AWS page here: https:// aws.amazon.com/cloudwatch/. your applications, integrated SaaS applications, and AWS services. By creating event-driven architectures, EventBridge allows various applications and services to communicate with each information to Elasticsearch, allowing you to analyze and troubleshoot the failure. Chapter 9 325 - cessing, enabling you to create complex and resilient architectures for your applications. With from AWS DevOps services and automation application deployment pipelines. You can explore more about EventBridge by visiting the AWS page here: https://aws.amazon.com/eventbridge/. and compliance controls and provides the follow- • Easily map your AWS usage to controls: Use pre-built compliance frameworks or build custom frameworks to collect evidence for compliance controls. • Save time with an automated collection of evidence across accounts: Focus on reviewing the relevant evidence to ensure your controls are working as intended. • Be continually prepared to produce audit-ready reports: Evidence is continuously collect- ensure controls are working as intended. • Ensure assessment report and evidence integrity: When it is time for an audit, build as- sessment reports with evidence that has been continuously collected, securely stored, and remains unaltered. 326 You can learn more about AWS Audit Manager by visiting the AWS page here: https://aws. amazon.com/audit-manager/. AWS Systems Manager is a management service that allows you to take actions on your AWS re- sources as necessary. It provides you with a quick view of operational data for groups of resources, making it easy to detect any issues that could impact applications that rely on those resources. You can group resources by various criteria, such as applications, application layers, or production vs. development environments. Systems Manager displays operational data for your resource groups on a single dashboard, eliminating the need to switch between different AWS consoles. For example, you can create a resource group for an application that uses Amazon EC2, Amazon S3, and Amazon RDS. Systems Manager can check for software changes installed on your Amazon EC2 instances, changes in your S3 objects, or stopped database instances. Chapter 9 327 account: AWS Systems Manager provides detailed insights into the current state of your resource groups, - AWS Systems Manager offers several features to help maintain security and compliance in your among other things. Systems Manager also enables you to manage your servers at scale remotely be especially helpful in large-scale environments, where managing resources individually can be time-consuming and error-prone. including plain text items such as database strings and secrets like passwords. By separating your and simplify your development and deployment processes. 328 You can use System Manager to achieve all components of the second pillar as it provides a one- stop shop, as shown in the left-hand navigation bar in the below screenshot: AWS Systems Manager is the hub of your operation for managing all your AWS applications and resources along with your on-premise environments, keeping everything in one place for easy monitoring and auditing. AWS has done a great job explaining how Systems Manager works, which you can learn about by referring to this link: https://docs.aws.amazon.com/systems- manager/latest/userguide/what-is-systems-manager.html. various AWS services that can help to achieve that. Now, let’s learn about the following step: provision and orchestration. Once you have set up the environment, you need to speed up the provisioning and orchestration of your applications and any associated resources in a repeatable and immutable fashion. Chapter 9 329 where you used wikis and playbooks, which were sometimes outdated. Most of us can relate to a mentioned, and we encountered situations during the provisioning which needed to be stated times in the past and hopes that this person was not on vacation and still working at the company! great because Bash was not designed to build complex deployment frameworks, so it was hard As digital transformation increasingly occurs within an organization, more applications either - tive, and innovate. You need highly specialized tools to manage the applications and to be able to choose from a varied set of tools based on their use cases. Managing infrastructure is a big part of managing cloud complexity, and one of the ways to manage infrastructure is by treating infrastructure as code. Infrastructure-as-code (IaC) templates help you to model and provision resources, whether AWS-native, third-party or open source, and applications on AWS and on-premises. So, you can speed up application and resource provisioning while improving consistency and compliance. For example, a developer wants to provision an S3 bucket that meets their company’s security require- properties to set or how to set them. Instead, they can reuse a pre-built Secure S3 bucket module to provision a bucket quickly while automatically aligning with their company’s requirements. compute (Amazon EC2, containers), databases, streams, security groups, users, roles, etc. All these servers and resources are the infrastructure components of your cloud application. Managing cloud applications involves managing the life cycle of their resources: create, update, or delete. By codifying your infrastructure, you can manage your infrastructure code in a way that is similar to advantages of using IaC are numerous, including: • A single source of truth for deploying the entire stack. • 330 • • Automatic rollback to the previous working state in case of failures. • AWS offers multiple services that help customers manage their infrastructure. AWS CloudFor- mation is the provisioning engine. It helps speed up cloud provisioning with IaC. AWS Service Catalog allows organizations to software, and databases to complete multi-tier application architectures. Users can then browse they are using approved resources that meet compliance and security requirements. Additionally, You can choose to combine these services as per your workload needs. Let’s learn about these services in more detail. CloudFormation is a tool that supports the implementation of IaC. With CloudFormation, you can write your IaC using the CloudFormation template language, available in YAML and JSON formats. You can start from scratch or leverage any of the pre-existing sample templates to create your infrastructure. You can use the CloudFormation service through a web-based console, com- CloudFormation helps you model, provision, and manage AWS resources. It allows you to use a template to create and delete multiple related AWS resources in a predictable and consistent way. Here is a simple example of a CloudFormation template written in YAML syntax: --- AWSTemplateFormatVersion: '2010-09-09' Resources: MyEC2Instance: Type: AWS::EC2::Instance Properties: ImageId: ami-101013 InstanceType: m4.xlarge Chapter 9 331 KeyName: gen-key-pair SecurityGroups: - !Ref ServerSecurityGroup ServerSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: Allow ssh access SecurityGroupIngress: - IpProtocol: tcp FromPort: '22' ToPort: '22' CidrIp: 10.1.1.16/0 EC2 instance. servertemplate.yml) and then use the AWS CLI to create a stack: aws cloudformation create-stack --stack-name sa-book-stack --template-body file://servertemplate.yml You can then use the AWS Management Console, the AWS CLI, or the CloudFormation API to monitor the progress of the stack creation. Once the stack is created, you will have an Amazon EC2 instance running in your AWS account. You can make changes to the stack by updating the template and using the update-stack command. You can learn more about AWS CloudFormation by visiting the AWS page here: https://aws.amazon.com/cloudformation/. AWS Service Catalog provides a and make them available for their employees through a self-service portal. AWS Service Management Connectors allow IT service management (ITSM) administrators to enhance the governance of provisioned AWS and third-party products. 332 For instance, by integrating with AWS Service Catalog, ServiceNow and Jira Service Desk can re- quest, provision, and manage AWS and third-party services and resources for their users, stream- AWS Service Catalog AppRegistry is a centralized repository that enables organizations to man- age and govern their application resources on AWS. It provides a single place for collecting and managing application metadata, including their name, owner, purpose, and associated resources. better collaboration between teams that work on different parts of the application stack. With AppRegistry, you can track and manage all your applications’ resources, including AWS resources, third-party software, and external resources such as domain screenshot shows an app registry in the system catalog: helps business stakeholders have up-to-date information about the application’s contents and metadata. In addition, AppRegistry provides a central place to track and manage changes to your application resources, helping you maintain a comprehensive view of your applications and their dependencies. You can learn more about AWS Service Catalog by visiting the AWS page here: https://aws.amazon.com/servicecatalog/. Chapter 9 333 AWS Proton is a managed application deployment service that allows developers to quickly and easily deploy and manage container and serverless applications on AWS. It provides a fully man- developers can focus on writing code and building applications while the service handles the process, reduce the risk of errors, and improve overall application quality. AWS provides sample Proton templates that help you start building your application’s infra- structure. You can fork those samples using AWS samples code link: (https://github.com/aws- samples/aws-proton-cloudformation-sample-templates) and refer to them while building the Proton environment. AWS Proton can help you update out-of-date applications with a single click when you adopt compliant with industry standards. Additionally, by providing a consistent architecture across your organization, Proton helps improve collaboration and reduces the risk of errors or miscon- https://aws. amazon.com/proton/. - Script, JavaScript, Python, Java, and C#. It provisions and deploys the infrastructure using AWS With CDK, you will work much faster because you are using your familiar language, concepts, classes, and methods without a context switch. You also have all the tool support from the pro- important part is that you can build your abstractions and components of the infrastructure and you can start quickly. 334 using high-level programming languages. You can create and structure apps that consist of one grouping of AWS resources that are provisioned and managed as a single unit. Each stack is mapped one-to-one to a CloudFormation stack and can be independently deployed, updated, or deleted. It is good practice to divide resources into stacks with different life cycles: i.e., you would create one stack for network infrastructure such as a VPC, another stack would have an Elastic Container Service cluster, and yet another stack would be the application that is running in this cluster. security considerations in mind to provide an excellent developer experience, ease of use, and structure, inspect deployment differences, and deploy your project quickly to AWS. Here is an import * as cdk from 'aws-cdk-lib'; import * as s3 from 'aws-cdk-lib/aws-s3'; class MyBookStack extends cdk.Stack { constructor(scope: CDK.App, id: string, props?: CDK.StackProps) { super(scope, id, props); new s3.Bucket(this, 'BookBucket', { bucketName: 'aws-sa-book-bucket', publicReadAccess: true }); } } const app = new cdk.App(); new MyBookStack(app, 'MyBookStack'); app.synth(); template for the stack. You can then use the cdk deploy command to deploy the stack to your AWS account. Chapter 9 335 AWS CDK provides a paradigm shift in how you provision multiple environments. With Cloud- Formation, you can use one template with parameters for multiple environments, i.e., dev and test. But with CDK, you have a shift where multiple templates are generated for each environment, environments by having less expensive resources in the dev environment. You can learn more about AWS CDK by visiting the AWS page here: https://aws.amazon.com/cdk/. AWS Amplify is a suite of specialized tools designed to help developers quickly build feature-rich, full-stack web and mobile applications on AWS. It allows developers to utilize a wide range of web or mobile app, visually create a web frontend UI, connect the two, and manage app content without needing to access the AWS console. At a high level, AWS Amplify provides the following features, tools, and services: • Amplify Libraries: Frontend developers can use purpose-built Amplify libraries for inter- acting with AWS services. You can use the case-centric Amplify Libraries for connecting frontend iOS, Android, web, and React Native apps to an AWS backend and UI compo- nents for auth, data, and storage. Customers can use Amplify Libraries to build a new app backend or connect an existing backend. • Amplify Hosting: Amplify Hosting is a fully managed CI/CD service for modern web apps. It offers hundreds of global points of presence for fast and reliable hosting of static and server-side rendered apps that scale with your business needs. With Amplify Hosting, you is then deployed and hosted globally using CloudFront. Amplify Hosting supports mod- ern web frameworks such as React, Angular, Vue, Next.js, Gatsby, Hugo, Jekyll, and more. • Amplify Studio: Amplify Studio is a visual development environment that provides an abstraction layer on top of the Amplify CLI. It allows you to create full-stack apps on AWS by building an app backend, creating custom UI components, and connecting a UI to the app backend with minimal coding. With Amplify Studio, you can select from dozens of popular React components, such as buttons, forms, and marketing templates, and design prototyping tool, Figma, as clean React code for seamless collaboration. Amplify Studio exports all UI and infrastructure artifacts as code so you can maintain complete control over your app design and behavior. 336 • The Amplify CLI and integration with existing CI/CD tools through up backend resources in the cloud easily. It’s designed to work with the Amplify JavaScript manages the mobile or web backend AWS, or modify Amplify deployment operations to comply with your enterprise DevOps guidelines. Here’s a code example of how to use AWS Amplify in a web application to store and retrieve data from a cloud database: import { API, graphqlOperation } from 'aws-amplify' // Add a new item to the cloud database async function addItem(item) { const AddItemMutation = `mutation AddItem($item: ItemInput!) { addItem(item: $item) { id name description } }` const result = await API.graphql(graphqlOperation(AddItemMutation, { item })) console.log(result) } // Retrieve a list of items from the cloud database async function listItems() { const ListItemsQuery = `query ListItems { listItems { items { id name description } } }` Chapter 9 337 const result = await API.graphql(graphqlOperation(ListItemsQuery)) console.log(result) } API object provided by Amplify enables you to call GraphQL operations to interact with the addItem function uses the addItem mutation to create a new item in the database, while the listItems function uses the listItems query to retrieve a list of items. AWS Amplify ties into a broader array of tools and services provided by AWS. You can customize your Amplify toolkit and leverage AWS services’ breadth and depth to service your modern ap- plication development needs. You can choose the services that suit your application and business more about AWS Amplify by visiting the AWS page here: https://aws.amazon.com/amplify/. In this section, you were introduced to AWS CDK, which is a provisioning and orchestration solution that facilitates the consistent and repeatable provisioning of resources. By utilizing AWS CDK, you can scale your organization’s infrastructure and applications on AWS in a sustainable manner. Additionally, AWS CDK allows you to create your infrastructure as code using program- ming languages. You can simplify and accelerate the governance and distribution of IaC templates using the AWS Service Catalog to create repeatable infrastructure and application patterns with best practices. Now let’s learn about the next step to set up monitoring and observations from your applications. As the saying goes, you manage what you measure, so after you’ve provisioned your application, you have to be able to start measuring its health and performance. Monitoring and observability tools help to collect metrics, logs, traces, and event data. You can quickly identify and resolve ap- plication issues for serverless, containerized, or other applications built using microservices-based architectures. AWS provides native monitoring, logging, alarming, and dashboards with CloudWatch and tracing through X-Ray. When deployed together, they provide the three pillars of an observability solution: metrics, logs, and traces. X-Ray (tracing) is fundamental to observability and is therefore included in the motions alongside CloudWatch. Furthermore, AWS provides open-source observability observability strategy implemented with CloudWatch and X-Ray provides insights and data to monitor and respond to your application’s performance issues by providing a consolidated view of the operation. 338 to use as per your workload needs or combine them together. AWS CloudWatch helps you collect, view, and certain thresholds are breached. Here you can see a screenshot of the billing metrics dashboard in CloudWatch: - stances, and other resources and troubleshoot issues with your applications. You learned about AWS CloudWatch earlier in this chapter. AWS X-Ray is a distributed tracing service that allows developers to analyze and debug their ap- plications, especially those built using a microservices architecture. It helps identify performance bottlenecks and errors, allowing developers to optimize application performance and enhance the end-user experience. Chapter 9 339 each component of your application: and see the performance of each trace segment. You can also use the console to search for traces - AWS SDKs or the X-Ray daemon. You can then view and analyze the data using the X-Ray console or the X-Ray API. You can learn more about AWS X-Ray by visiting the AWS page here: https:// aws.amazon.com/xray/. 340 Amazon Managed Service for Prometheus (AMSP) is a fully managed service that makes it easy to run and scale Prometheus, an open-source monitoring and alerting system, in the cloud. AMSP automatically handles tasks such as scaling and maintenance, allowing you to focus on moni- toring your applications. It also includes integration with other AWS services, such as Amazon CloudWatch and Amazon SNS, which allows you to view and analyze your monitoring data and • Fully managed: AMSP takes care of the underlying infrastructure and maintenance tasks, so you can focus on monitoring your applications. • Scalability: AMSP automatically scales to handle changes in workload, so you don’t have to worry about capacity planning. • Integration with other AWS services: AMSP integrates with other AWS services, such as CloudWatch and SNS, which allows you to view and analyze your monitoring data and • Security: AMSP includes built-in security measures, such as encryption at rest and network isolation, to help protect your monitoring data. • Cost-effective: AMSP is a pay-as-you-go service, which means you only pay for the re- Prometheus infrastructure. You can learn more about AMSP by visiting the AWS page here: https://aws.amazon.com/ prometheus/. Amazon Managed Service for Grafana (AMG) is a fully visualization and analysis of operational data at scale. It leverages the popular open-source an- alytics platform Grafana to query, visualize, and alert on metrics stored across AWS, third-party traces without the heavy lifting in production. You can learn more about AMG by visiting the AWS page here: https://aws.amazon.com/grafana/. In this section, you learned that an observable environment reduces risk, increases agility, and improves customer experience. Observability provides insights and context about the environ- ment you monitor. AWS enables you to transform from monitoring to observability so that you can have full-service visibility from metrics, logs, and traces by combining AWS CloudWatch and X-Ray. Let’s learn about the next step in building centralized operations. Chapter 9 341 - data from multiple AWS services. You can then automate operational tasks on applications and - vice limits, taking backups, and resizing instances. You laid down all without compromising any safety, security, or compliance guardrails in the foundation stage. use AWS Systems Manager. Systems Manager is a fully It provides a central location and interfaces to view operational data from multiple AWS services. Customers can then use it to automate operational tasks on their applications and resources, taking backups, and resizing instances. You learned about AWS Systems Manager earlier in this chapter. Here you will learn about Sys- four stages of implementing CloudOps using AWS Systems Manager: 1. Build a foundation for cloud operations maintain an inventory of infrastructure resources and application and OS data starting with your environment and account structure. Simple, automated setup processes allow you to quickly enable operational best practices, such as continuous vulnerability scanning and collecting insights into improving an application’s performance and availability. It also helps you to automate operational best practices by codifying operations runbooks rules enable continuous compliance by enforcing security, risk, and compliance policies. 2. Enable visibility into applications and infrastructure visibility into applications and infrastructure by continuously tracking key technical and business measures, providing visibility, and triggering automation and actions if needed. 342 Systems Manager provides operational visibility and actionable insights to customers through a widget-based, customizable dashboard that can be tailored for users such as up operational event management and reporting for their infrastructure and resources data across multiple AWS accounts and AWS Regions, such as inventory and CMDB, patch, - on outstanding operational issues. 3. Automate operations at scale Manager gives teams the ability to automate patch management to keep resources se- cure. Systems Manager provides change management capabilities with built-in approval and environment changes. Only approved changes can be deployed to the environment by authorized resources, and these come with detailed reporting. You can automate serv- perform common administrative tasks. Additionally, you can manage and troubleshoot actions are required by connecting directly from the console. You can also operationalize risk management by creating rules. Here is an example AWS Systems Manager rule that operationalizes risk management by checking for security vulnerabilities in installed software packages on Amazon EC2 instances: { \"Name\": \"ec2-check-for-security-vulnerabilities\", \"Description\": \"Scans installed software packages on EC2 instances for security vulnerabilities\", \"ResourceId\": \"*\", \"ResourceType\": \"AWS::EC2::Instance\", \"ComplianceType\": \"NON_COMPLIANT\", \"RulePriority\": 1, \"Operator\": \"EQUALS\", \"Parameters\": { \"ExecutionFrequency\": \"OneTime\", Chapter 9 343 \"OutputS3BucketName\": \"sa-book-s3-bucket\", \"OutputS3KeyPrefix\": \"ec2-security-scans/\" }, \"Actions\": [ { \"Type\": \"RunCommand\", \"Properties\": { \"Comment\": \"Scan installed software packages for security vulnerabilities\", \"OutputS3BucketName\": \"sa-book-s3-bucket\", \"OutputS3KeyPrefix\": \"ec2-security-scans/\", \"DocumentName\": \"AWS-RunShellScript\", \"Parameters\": { \"commands\": [ \"apt update\", \"apt-get install -y unattended-upgrades\", \"apt-get install -y --only-upgrade bash\", \"apt-get install -y --only-upgrade glibc\", \"apt-get install -y --only-upgrade libstdc++6\", \"apt-get install -y --only-upgrade libgcc1\", \"apt-get install -y --only-upgrade libc6\", \"apt-get install -y --only-upgrade libc-bin\", \"apt-get install -y --only-upgrade libpam-modules\", \"apt-get install -y --only-upgrade libpam-runtime\", \"apt-get install -y --only-upgrade libpam0g\", \"apt-get install -y --only-upgrade login\", \"apt-get install -y --only-upgrade passwd\", \"apt-get install -y --only-upgrade libssl1.0.0\", \"apt-get install -y --only-upgrade openssl\", \"apt-get install -y --only-upgrade dpkg\", \"apt-get install -y --only-upgrade apt\", \"apt-get install -y --only-upgrade libapt-pkg4.12\", \"apt-get install -y --only-upgrade apt-utils\", \"apt-get install -y --only-upgrade libdb5.3\", \"apt-get install -y --only-upgrade bzip2\", \"apt-get install -y --only-upgrade libbz2-1.0\", \"apt-get install -y --only-upgrade liblzma5\", 344 \"apt-get install -y --only-upgrade libtinfo5\", \"apt-get install -y --only-upgrade libreadline7\", \"apt ] } By running analyses to proactively detect and remediate risks across applications and infrastructure, such automation runbooks with automated reporting. 4. Remediate issues and incidents: Finally, when unexpected issues arise, you must be able to remediate issues and incidents quickly. With the incident, event, and risk management capabilities within Systems Manager, you can employ various AWS services to trigger relevant issues or incidents. It integrates with Amazon GuardDuty for threat detection and AWS Inspector for security assessments, keeps a running check on vulnerabilities in the environment, and allows automated remediation. It provides a consolidated view of incidents, changes, operational risks and failures, operational alarms, compliance, and vulnerability management reports. It allows operations teams to take manual or automat- ed action to resolve issues. Systems Manager speeds up issue resolution by automating common, repeatable remediation actions, such as failover to a backup system and cap- turing failed state for root cause analysis. Systems Manager’s incident management capability automates a response plan for application issues by notifying the appropriate people to respond, providing them with relevant troubleshoot- ing data, and enabling chat-based collaboration. You can easily access and analyze operational data from multiple AWS services and track updates related to incidents, such as changes in alarm status and response plans. Operators can resolve incidents manually by logging into the instance or executing automation runbooks. Systems Manager integrates with AWS Chatbot to invoke commands in the appropriate channel for an incident so Cloud adoption has enabled technology teams to innovate faster by reducing approval, procure- failure cost, as cloud resources can be terminated with just a few clicks or API calls. As a result, technology teams are no longer just builders, but they also operate and own their products. Chapter 9 345 - erations teams, such as procurement and deployment. Cloud Financial Management (CFM) enables - and accelerate economic and business value creation while balancing agility and control. CFM has the following four dimensions to manage and save costs: When planning for future cloud spending, you and track the monthly cost of the project with the cost and usage data available in their AWS Cost reports can be customized to include only the needed data and can be delivered to an Amazon S3 bucket or an Amazon SNS topic. You can also use the data in these reports to create custom cost and usage reports, set up cost and usage alarms, and create budgets. You can decide on a project’s budget based on the growth trend of the project as well as the avail- for cost or resource usage. You can also use AWS Budgets to set coverage and utilization targets for your AWS usage, in exchange for committing to a certain usage level over a Reserved Instances are a type of pricing model that allows you to save up to 75% on your Amazon EC2 and RDS usage costs by committing to a one- or three-year term. With Reserved Instances, you pay a discounted hourly rate for the usage of a you can choose between Standard and Convertible Reserved Instances. AWS Savings Plans is a new pricing model that allows you to save up to 72% on your AWS usage costs by committing to a one-year or three-year term. With Savings Plans, you pay a discounted hourly rate for your AWS usage, and you can choose between Compute Savings Plans and Ama- zon EC2 Instance Savings Plans. Compute Savings Plans offer a discount on a wide range of AWS services, including Amazon EC2, Fargate, and Lambda, while Amazon EC2 Instance Savings Plans only offer a discount on Amazon EC2 usage. 346 - lio by comparing actual costs with the budgeted costs and forecasted costs with the budgeted Service (SNS). You can learn more about AWS Budgets by visiting the AWS page here: https:// aws.amazon.com/aws-cost-management/aws-budgets/. After planning your budget, let’s learn about managing it. As businesses grow and scale on AWS, you need to give your team the freedom to experiment and innovate in the cloud while maintaining control over cost, governance, and security. And while fostering innovation and speed is essential, you also want to avoid getting surprised by the bill. You can achieve this by establishing centralized ownership through a center of excellence. Cost management elements are shared responsibilities across the entire organization. A centralized team is essential to design policies and governance mechanisms, implement and monitor the effort, and help drive company-wide best practices. You can utilize services like Identity and Access Management (IAM) to ensure secure access control to AWS resources. IAM allows you to create and manage user identities, groups, and control and restrict individual and group access to AWS resources, ensuring the security of your infrastructure and data. Use AWS Organizations to enable automatic policy-based account creation, management, and billing at scale. Finally, using the AWS Billing Console, you can easily track overall spending and view cost breakdown by service and account by accessing Billing Dashboard. You can view the overall monthly spending from last month, the current month, and the current forecasted month. Chapter 9 347 other controls. You can learn more about the AWS Billing Console by visiting the AWS page here: https://aws.amazon.com/aws-cost-management/aws-billing/. AWS Purchase Order Management enables you to use purchase orders (POs) to procure AWS them to your invoices, and access the invoices generated against those POs. Additionally, you can for contacts to receive alerts when POs are running low on balance or close to their expiration date. You can learn more about AWS Purchase Order Management by visiting the AWS page here: https://aws.amazon.com/aws-cost-management/aws-purchase-order-management/. 348 AWS Cost Anomaly Detection is a machine learning service that automates cost anomaly alerts and root cause analysis. It can save time investigating spending anomalies by providing automated types (e.g., data transfer cost), regions, and member accounts. You can learn more about AWS Cost Anomaly Detection by visiting the AWS page here: https://aws.amazon.com/aws-cost- management/aws-cost-anomaly-detection/. As you manage your cost, let’s learn how to track it. You need to ask three questions to understand is, What is causing our bill to increase? AWS provides AWS Cost Explorer to help answer this question. Cost Explorer are leading to increased spending: Chapter 9 349 In the above Cost Explorer dashboard, you can see service expense grouping in the costs chart showing AWS Neptune has the highest cost, followed by SageMaker. It also provides the ability to download the AWS page here: https://aws.amazon.com/aws-cost-management/aws-cost-explorer/. second question is a common follow-up: Which of my lines of business, teams, or organizations drove increased spending and usage? AWS Cost Explorer’s data becomes even more valuable when - nities. You can also use the AWS Cost and Usage Report (CUR) to bring in cost and usage data, https://aws.amazon.com/aws-cost-management/aws-cost-and-usage-reporting/. Finally, the third question is, How do I understand the cost impact of Reserved Instances? AWS Cost Explorer provides multiple ways to view and dive deep into this data, such as tag service with - counts, tags, services, and charge types with custom rules. You can use CUR to deliver cost data to the S3 bucket, which can be integrated with Amazon Athena and/or ingested into your ERP system. Furthermore, AWS Billing Conductor is a billing and cost management tool that helps you mon- itor and optimize your AWS costs and usage and provides insights into how you are using your resources. AWS Billing Conductor provides recommendations for ways to optimize your costs, such as by identifying idle or underutilized resources that can be turned off or scaled down. You can learn more about AWS Billing Conductor by visiting the AWS page here: https://aws.amazon. com/aws-cost-management/aws-billing-conductor/. is a service that allows you to collect and correlate usage data allocating costs to individual tenants or customers. You can learn more about AWS Application https://aws.amazon.com/aws-cost-management/ aws-application-cost-profiler/ increase savings. 350 Cost optimization is about making sure you pay only for what you need. AWS offers tools and services that make it easier for you to saving as you scale on AWS. While there are hundreds of ways you can pull to minimize spend- ing, at its core, two of the following most impactful levers you can pull to optimize spending are detailed next. Using the right pricing models - mand, pay-as-you-go, commitment-based Reserved Instances, and Spot instances for up to 90% discount compared to on-demand pricing. Amazon EC2 Spot Instances are a type of AWS comput- instances are spare capacities in the AWS cloud that can be interrupted at any time, based on the by specifying the maximum price you are willing to pay per hour (known as the “bid price”). If the current Spot price is less than your bid price, your Spot Instance will be launched, and you will be charged the current Spot price. However, if the current Spot price exceeds your bid price, your Spot Instance will be interrupted, and you will not be charged for the usage. Suppose you have predictable, steady workloads on Amazon EC2, Amazon ECS, and Amazon RDS. If you use Reserved Instances, you can save up to 75% over on-demand capacity. Reserved Instances are a pricing option in AWS that allows you to pay up front for a commitment to use a three options for purchasing Reserved Instances: • All up-front (AURI) requires you to pay for the entire term of the Reserved • Partial up-front (PURI) smaller discount than the AURI option. • No upfront payments (NURI)option does not require any up front payment and provides the smallest discount. Chapter 9 351 By choosing the AURI option, you can receive the largest discount on your Reserved Instances. upfront payments altogether. AWS offers Reserved Instances, and Savings Plans purchase recommendations via Cost Explorer - your purchase preference. From there, you can track your investment using cost explorer. Any usage above the commitment level will be charged at on-demand rates. You can revisit commitment levels, make incremental purchases, and track coverage and utilization using pre-built reports in cost explorer. With AWS purchase option recommendations, you can receive tailored Reserved Instance, or Savings Plans purchase recommendations based on your historical usage. You can select param- eters for recommendations, such as the type of plan, term commitment, and payment option that makes sense for your business. Identify and eliminate idle or over-provisioned resources: AWS Cost Explorer resources can be used for top-level key performance indicators (KPIs), rightsizing, and instance selection. Cost Explorer will estimate monthly savings, which is the sum of the projected monthly savings associated with each recommendation. Cost Explorer rightsizing recommendations generate recommendations by identifying idle and underutilized instances and searching for smaller instance sizes in the same instance family. Idle with CPU utilization between 1% and 40%. In this section, you learned about various ways to manage your cost and make the cloud more services are available in AWS can make a huge difference when operating your workload in the how to apply automation everywhere. In this chapter, you learned about the cloud operation model and the six pillars of CloudOps, which help you to understand how to plan your cloud","libVersion":"0.5.0","langs":""}