{"path":"sem5/AI/pdf/12_uncertainty.pdf","text":"1 Representing Uncertainty for Probabilistic Inference Chapter 13 1 Uncertainty in the World â€¢ An agent can often be uncertain about the state of the domain since there is often ambiguity and uncertainty â€¢ Plausible/probabilistic inference â€¢ Iâ€™ve got this evidence; whatâ€™s the chance that this conclusion is true? â€¢ Iâ€™ve got a sore neck; how likely am I to have meningitis? â€¢ A mammogram test is positive; whatâ€™s the probability that the patient has breast cancer? 2 Probabilistic Inference How do we use probabilities in AI? â€¢ You wake up with a headache â€¢ Do you have the flu? â€¢ H = headache, F = flu Logical Inference: if H then F (but the world is usually not this simple) Statistical Inference: compute the probability of a query/diagnosis/decision given (i.e., conditioned on) evidence/symptom/observation, i.e., P(F | H) [Example from Andrew Moore] 3 Uncertainty â€¢ Say we have a rule: if toothache then problem is cavity â€¢ But not all patients have toothaches due to cavities, so we could set up rules like: if toothache and Â¬gum-disease and Â¬filling and ... then problem = cavity â€¢ This gets complicated; better method: if toothache then problem is cavity with 0.8 probability or P(cavity | toothache) = 0.8 the probability of cavity is 0.8 given toothache is observed 4 2 Uncertainty in the World and our Models â€¢ True uncertainty: rules are probabilistic in nature â€“ quantum mechanics â€“ rolling dice, flipping a coin â€¢ Laziness: too hard to determine exception-less rules â€“ takes too much work to determine all of the relevant factors â€“ too hard to use the enormous rules that result â€¢ Theoretical ignorance: don't know all the rules â€“ problem domain has no complete, consistent theory (e.g., medical diagnosis) â€¢ Practical ignorance: do know all the rules BUT â€“ haven't collected all relevant information for a particular case 6 Logics Logics are characterized by what they use as \"primitives\" Logic What Exists in World Knowledge States Propositional facts true/false/unknown First-Order facts, objects, relations true/false/unknown Temporal facts, objects, relations, times true/false/unknown Probability Theory facts degree of belief 0..1 Fuzzy degree of truth degree of belief 0..1 7 Probability Theory â€¢ Probability theory serves as a formal means for â€“ Representing and reasoning with uncertain knowledge â€“ Modeling degrees of belief in a proposition (event, conclusion, diagnosis, etc.) â€¢ Probability is the â€œlanguageâ€ of uncertainty â€“ A key modeling tool in modern AI 8 Sample Space â€¢ A space of events in which we assign probabilities â€¢ Events can be binary, multi-valued, or continuous â€¢ Events are mutually exclusive â€¢ Examples â€“ Coin flip: {head, tail} â€“ Die roll: {1, 2, 3, 4, 5, 6} â€“ English words: a dictionary â€“ High temperature tomorrow: {-100, â€¦, 100} 10 3 Random Variable â€¢ A variable, X, whose domain is a sample space, and whose value is (somewhat) uncertain â€¢ Examples: X = coin flip outcome X = tomorrowâ€™s high temperature â€¢ For a given task, the user defines a set of random variables for describing the world â€¢ Each variable has a set of mutually exclusive and exhaustive possible values 11 13 Probability for Discrete Events â€¢ An agentâ€™s uncertainty is represented by P(A=a) or simply P(a) â€“ the agentâ€™s degree of belief that variable A takes on value a given no other information related to A â€“ a single probability called an unconditional or prior probability 14 Probability for Discrete Events â€¢ Examples â€“ P(head) = P(tail) = 0.5 fair coin â€“ P(head) = 0.51, P(tail) = 0.49 slightly biased coin â€“ P(first word = â€œtheâ€ when flipping to a random page in R&N) = ? â€¢ Book: The Book of Odds 15 4 Source of Probabilities â€¢ Frequentists â€“ probabilities come from data â€“ if 10 of 100 people tested have a cavity, P(cavity) = 0.1 â€“ probability means the fraction that would be observed in the limit of infinitely many samples â€¢ Objectivists â€“ probabilities are real aspects of the world â€“ objects have a propensity to behave in certain ways â€“ coin has propensity to come up heads with probability 0.5 â€¢ Subjectivists â€“ probabilities characterize an agent's belief â€“ have no external physical significance 18 Probability Distributions Given A is a RV taking values in ã€ˆa1, a2, â€¦ , akã€‰ e.g., if A is Sky, then value is one of <clear, partly_cloudy, overcast> â€¢ P(a) represents a single probability where A=a e.g., if A is Sky, then P(a) means any one of P(clear), P(partly_cloudy), P(overcast) â€¢ P(A) represents a probability distribution â€“ the set of values: ã€ˆP(a1), P(a2), â€¦, P(ak)ã€‰ â€“ If A takes n values, then P(A) is a set of n probabilities e.g., if A is Sky, then P(Sky) is the set of probabilities: ã€ˆP(clear), P(partly_cloudy), P(overcast)ã€‰ â€“ Property: âˆ‘ P(ai) = P(a1) + P(a2) + ... + P(ak) = 1 â€¢ sum over all values in the domain of variable A is 1 because the domain is mutually exclusive and exhaustive 19 Probability Table â€¢ Weather â€¢ P(Weather = sunny) = P(sunny) = 200/365 â€¢ P(Weather) = ã€ˆ200/365, 100/365, 65/365ã€‰ â€¢ Weâ€™ll obtain the probabilities by counting frequencies from data 65/365100/365200/365 rainycloudysunny 20 The Axioms of Probability 1. 0 â‰¤ P(A) â‰¤ 1 2. P(true) = 1, P(false) = 0 3. P(A âˆ¨ B) = P(A) + P(B) â€“ P(A âˆ§ B) Note: Here P(A) means P(A=a) for some value a and P(A âˆ¨ B) means P(A=a âˆ¨ B=b) 21 5 The Axioms of Probability Â§ 0 â‰¤ P(A) â‰¤ 1 Â§ P(true) = 1, P(false) = 0 Â§ P(A âˆ¨ B) = P(A) + P(B) â€“ P(A âˆ§ B) Sample space The fraction of A canâ€™t be smaller than 0 22 The Axioms of Probability Â§ 0 â‰¤ P(A) â‰¤ 1 Â§ P(true) = 1, P(false) = 0 Â§ P(A âˆ¨ B) = P(A) + P(B) â€“ P(A âˆ§ B) Sample space The fraction of A canâ€™t be bigger than 1 23 The Axioms of Probability Â§ 0 â‰¤ P(A) â‰¤ 1 Â§ P(true) = 1, P(false) = 0 Â§ P(A âˆ¨ B) = P(A) + P(B) â€“ P(A âˆ§ B) Sample space Valid sentence: e.g., â€œX=head or X=tailâ€ 24 The Axioms of Probability Â§ 0 â‰¤ P(A) â‰¤ 1 Â§ P(true) = 1, P(false) = 0 Â§ P(A âˆ¨ B) = P(A) + P(B) â€“ P(A âˆ§ B) Sample space Invalid sentence: e.g., â€œX=head AND X=tailâ€ 25 6 The Axioms of Probability Â§ 0 â‰¤ P(A) â‰¤ 1 Â§ P(true) = 1, P(false) = 0 Â§ P(A âˆ¨ B) = P(A) + P(B) â€“ P(A âˆ§ B) Sample spaceA B 26 Some Theorems Derived from the Axioms â€¢ P(Â¬A) = 1 â€“ P(A) â€¢ If A can take k different values a1, â€¦, ak: P(A=a1) + â€¦ + P(A=ak) = 1 â€¢ P(B) = P(B âˆ§ Â¬A) + P(B âˆ§ A), if A is a binary event â€¢ P(B) = âˆ‘i=1â€¦kP(B âˆ§ A=ai), if A can take k values Called Addition or Conditioning rule 27 Joint Probability â€¢ The joint probability P(A=a, B=b) is shorthand for P(A=a âˆ§ B=b), i.e., the probability of both A=a and B=b happening A P(A=a,B=b), e.g., P(1st =â€œSanâ€, 2nd =â€œFranciscoâ€) = 0.0007 P(B=b), e.g., P(2nd word = â€œFranciscoâ€) = 0.0008 P(A=a), e.g., P(1st word on a random page = â€œSanâ€) = 0.001 (possibly: San Francisco, San Diego, â€¦) (possibly: San Francisco, Don Francisco, Pablo Francisco â€¦) 28 Full Joint Probability Distribution (FJPD) â€¢ P(Temp=hot, Weather=rainy) = P(hot, rainy) = 5/365 = 0.014 â€¢ The full joint probability distribution table for n random variables, each taking k values, has kn entries cold hot 5/36540/365150/365 60/36560/36550/365 rainycloudysunny Weather Temp 29 7 Full Joint Probability Distribution (FJPD) Bird Flier Young Probability T T T 0.0 T T F 0.2 T F T 0.04 T F F 0.01 F T T 0.01 F T F 0.01 F F T 0.23 F F F 0.5 Sums to 1 3 Boolean random variables â‡’ 23 â€“ 1 = 7 â€œdegrees of freedomâ€ (DOF) or â€œindependent valuesâ€ 30 Computing from the FJPD â€¢ Marginal Probabilities â€“ P(Bird=T) = P(bird) = 0.0 + 0.2 + 0.04 + 0.01 = 0.25 â€“ P(bird, Â¬flier) = 0.04 + 0.01 = 0.05 â€“ P(bird âˆ¨ flier) = 0.0 + 0.2 + 0.04 + 0.01 + 0.01 + 0.01 = 0.27 â€¢ Sum over all other variables â€¢ â€œSumming Outâ€ â€¢ â€œMarginalizationâ€ 31 Unconditional / Prior Probability â€¢ Oneâ€™s uncertainty or original assumption about an event prior to having any data about it or anything else in the domain â€¢ P(Coin = heads) = 0.5 â€¢ P(Bird = T) = 0.0 + 0.2 + 0.04 + 0.01 = 0.22 â€¢ Compute from the FJPD by marginalization 32 Marginal Probability The name comes from the old days when the sums were written in the margin of a page cold hot 5/36540/365150/365 60/36560/36550/365 rainycloudysunny Weather Temp âˆ‘ 200/365 100/365 65/365 P(Weather) = ã€ˆ200/365, 100/365, 65/365ã€‰ Probability distribution for r.v. Weather 33 8 Marginal Probability This is nothing but P(B) = âˆ‘i=1â€¦kP(B âˆ§ A=ai), where A can take k values cold hot 5/36540/365150/365 60/36560/36550/365 rainycloudysunny Weather Temp P(Temp) = ã€ˆ195/365, 170/365ã€‰ âˆ‘ 195/365 170/365 34 Conditional Probability â€¢ Conditional probabilities â€“ formalizes the process of accumulating evidence and updating probabilities based on new evidence â€“ specifies the belief in a proposition (event, conclusion, diagnosis, etc.) that is conditioned on a proposition (evidence, feature, symptom, etc.) being true â€¢ P(a | e): conditional probability of A=a given E=e evidence is all that is known true P(a | e) = P(a âˆ§ e) / P(e) = P(a, e) / P(e) â€“ conditional probability can viewed as the joint probability P(a, e) normalized by the prior probability, P(e) 35 Conditional Probability The conditional probability P(A=a | B=b) is the fraction of time A=a, within the region where B=b A P(B=b), e.g. P(2nd word = â€œFranciscoâ€) = 0.0008 P(A=a), e.g. P(1st word on a random page = â€œSanâ€) = 0.001 P(A=a | B=b), e.g. P(1st=â€œSanâ€ | 2nd =â€œFranciscoâ€) = ? (possibly: San, Don, Pablo â€¦) 36 Conditional Probability â€¢ P(san | francisco) = #(1st=s and 2nd=f) / #(2nd=f) = P(san âˆ§ francisco) / P(francisco) = 0.0007 / 0.0008 = 0.875 P(s)=0.001 P(f)=0.0008 P(s,f)=0.0007 A P(B=b), e.g. P(2nd word = â€œFranciscoâ€) = 0.0008 P(A=a | B=b), e.g. P(1st=â€œSanâ€ | 2nd =â€œFranciscoâ€) = 0.875 (possibly: San, Don, Pablo â€¦) Although â€œSanâ€ is rare and â€œFranciscoâ€ is rare, given â€œFranciscoâ€ then â€œSanâ€ is quite likely! 37 9 Conditional Probability Conditional probabilities behave exactly like standard probabilities; for example: 0 â‰¤ P(a | e) â‰¤ 1 conditional probabilities are between 0 and 1 inclusive P(a1 | e) + P(a2 | e) + ... + P(ak | e) = 1 conditional probabilities sum to 1 where a1, â€¦, ak are all values in the domain of random variable A P(Â¬a | e) = 1 âˆ’ P(a | e) negation for Boolean random variable A 38 Computing Conditional Probability P(Â¬B|F) = ? P(F) = ? Note: P(Â¬B|F) means P(B=false | F=true) and P(F) means P(F=true) 40 Full Joint Probability Distribution Bird (B) Flier (F) Young (Y) Probability T T T 0.0 T T F 0.2 T F T 0.04 T F F 0.01 F T T 0.01 F T F 0.01 F F T 0.23 F F F 0.5 Sums to 1 3 Boolean random variables â‡’ 23 â€“ 1 = 7 â€œdegrees of freedomâ€ or â€œindependent valuesâ€ 41 Computing Conditional Probability P(Â¬B|F) = P(Â¬B, F)/P(F) = (P(Â¬B, F, Y) + P(Â¬B, F, Â¬Y))/P(F) = (0.01 + 0.01)/P(F) P(F) = P(F, B, Y) + P(F, B, Â¬Y) + P(F, Â¬B, Y) + P(F, Â¬B, Â¬Y) = 0.0 + 0.2 + 0.01 + 0.01 = 0.22 Marginalization 42 10 Computing Conditional Probability â€¢ Instead of using Marginalization to compute P(F), can alternatively use Normalization: â€¢ P(Â¬B|F) = .02/P(F) from previous slide â€¢ P(Â¬B|F) + P(B|F) = 1 by definition â€¢ P(B|F) = P(B,F)/P(F) = (0.0 + 0.2)/P(F) â€¢ So, 0.02/P(F) + 0.2/P(F) = 1 â€¢ Hence, P(F) = 0.22 43 Normalization â€¢ In general, P(A | B) = âº P(A, B) where âº = 1/P(B) = 1/(P(A, B) + P(Â¬A, B)) â€¢ P(Q | E1, â€¦, Ek) = âº P(Q, E1, â€¦, Ek) = âº âˆ‘Y P(Q, E1, â€¦, Ek, Y) Addition rule 44 Conditional Probability with Multiple Evidence P(Â¬B | F, Â¬Y) = P(Â¬B, F, Â¬Y) / P(F, Â¬Y) = P(Â¬B, F, Â¬Y) / (P(Â¬B, F, Â¬Y) + P(B, F, Â¬Y)) = .01 /(.01 + .2) = 0.048 45 Conditional Probability â€¢ P(X1=x1, â€¦, Xk=xk | Xk+1=xk+1, â€¦, Xn=xn) = sum of all entries in FJPD where X1=x1, â€¦, Xn=xn divided by sum of all entries where Xk+1=xk+1 , â€¦, Xn=xn â€¢ But this means in general we need the entire FJPD table, requiring an exponential number of values to do probabilistic inference (i.e., compute conditional probabilities) 46 11 The Chain Rule â€¢ From the definition of conditional probability we have P(A, B) = P(B) * P(A | B) = P(A | B) * P(B) â€¢ It also works the other way around: P(A, B) = P(A) * P(B | A) = P(B | A) P(A) â€¢ It works with more than 2 events too: P(A1, A2, â€¦, An) = P(A1) * P(A2 | A1) * P(A3| A1, A2) * â€¦ * P(An | A1, A2, â€¦, An-1) Called â€œProduct Ruleâ€ Called â€œChain Ruleâ€ 48 Probabilistic Reasoning How do we use probabilities in AI? â€¢ You wake up with a headache â€¢ Do you have the flu? â€¢ H = headache, F = flu Logical Inference: if H then F (but the world is usually not this simple) Statistical Inference: compute the probability of a query/diagnosis/decision given (i.e., conditioned on) evidence/symptom/observation, i.e., P(F | H) [Example from Andrew Moore] 52 Example Statistical Inference: Compute the probability of a diagnosis, F, given symptom, H, where H = â€œhas a headacheâ€ and F = â€œhas fluâ€ That is, compute P(F | H) You know that â€¢ P(H) = 0.1 â€œone in ten people has a headacheâ€ â€¢ P(F) = 0.01 â€œone in 100 people has fluâ€ â€¢ P(H | F) = 0.9 â€œ90% of people who have flu have a headacheâ€ [Example from Andrew Moore] 53 Inference with Bayesâ€™s Rule â€¢ P(H) = 0.1 â€œone in ten people has a headacheâ€ â€¢ P(F) = 0.01 â€œone in 100 people has fluâ€ â€¢ P(H|F) = 0.9 â€œ90% of people who have flu have a headacheâ€ â€¢ P(F|H) = (0.9 * 0.01) / 0.1 = 0.09 â€¢ So, thereâ€™s a 9% chance you have flu â€“ much less than 90% â€¢ But itâ€™s higher than P(F) = 1% since you have a headache Thomas Bayes, â€œEssay Towards Solving a Problem in the Doctrine of Chances,â€ 1764 )( )()|( )( ),( )|( HP FPFHP HP HFP HFP == Def of cond. prob. Product rule 54 12 Bayesâ€™s Rule 55 Inference with Bayesâ€™s Rule P(A|B) = P(B | A)P(A) / P(B) Bayesâ€™s rule â€¢ Why do we make things this complicated? â€“ Often P(B|A), P(A), P(B) are easier to get â€¢ Some terms: â€¢ Prior: P(A): probability of A before any evidence â€¢ Likelihood: P(B|A): assuming A, how likely is the evidence B â€¢ Posterior: P(A|B): probability of A after knowing evidence B â€¢ (Deductive) Inference: deriving an unknown probability from known ones 56 Bayesâ€™s Rule in Practice 57 58 13 Summary of Important Rules â€¢ Conditional Probability: P(A|B) = P(A,B)/P(B) â€¢ Product rule: P(A,B) = P(A|B)P(B) â€¢ Chain rule: P(A,B,C,D) = P(A|B,C,D)P(B|C,D)P(C|D)P(D) â€¢ Conditionalized version of Chain rule: P(A,B|C) = P(A|B,C)P(B|C) â€¢ Bayesâ€™s rule: P(A|B) = P(B|A)P(A)/P(B) â€¢ Conditionalized version of Bayesâ€™s rule: P(A|B,C) = P(B|A,C)P(A|C)/P(B|C) â€¢ Addition / Conditioning rule: P(A) = P(A,B) + P(A,Â¬B) P(A) = P(A|B)P(B) + P(A|Â¬B)P(Â¬B) 59 Common Mistake â€¢ P(A) = 0.3 so P(Â¬A) = 1 â€“ P(A) = 0.7 â€¢ P(A|B) = 0.4 so P(Â¬A|B) = 1 â€“ P(A|B) = 0.6 because P(A|B) + P(Â¬A|B) = 1 but P(A|Â¬B) â‰  0.6 (in general) because P(A|B) + P(A|Â¬B) â‰  1 in general 60 Quiz â€¢ A doctor performs a test that has 99% reliability, i.e., 99% of people who are sick test positive, and 99% of people who are healthy test negative. The doctor estimates that 1% of the population is sick. â€¢ Question: A patient tests positive. What is the chance that the patient is sick? â€¢ 0-25%, 25-75%, 75-95%, or 95-100%? 61 Quiz â€¢ A doctor performs a test that has 99% reliability, i.e., 99% of people who are sick test positive, and 99% of people who are healthy test negative. The doctor estimates that 1% of the population is sick. â€¢ Question: A patient tests positive. What is the chance that the patient is sick? â€¢ 0-25%, 25-75%, 75-95%, or 95-100%? â€¢ Common answer: 99%; Correct answer: 50% 62 14 Given: P(TP | S) = 0.99 P(Â¬TP | Â¬S) = 0.99 P(S) = 0.01 Query: P(S | TP) = ? TP = â€œtests positiveâ€ S = â€œis sickâ€ 63 P(TP | S) = 0.99 P(Â¬TP | Â¬S) = 0.99 P(S) = 0.01 P(S | TP) = P(TP | S) P(S) / P(TP) = (0.99)(0.01) / P(TP) = 0.0099/P(TP) P(Â¬S | TP) = P(TP | Â¬S)P(Â¬S) / P(TP) = (1 â€“ 0.99)(1 â€“ 0.01) / P(TP) = 0.0099/P(TP) 0.0099/P(TP) + 0.0099/P(TP) = 1, so P(TP) = 0.0198 So, P(S | TP) = 0.0099 / 0.0198 = 0.5 64 Inference with Bayesâ€™s Rule â€¢ In a bag there are two envelopes â€“ one has a red ball (worth $100) and a black ball â€“ one has two black balls. Black balls are worth nothing â€¢ You randomly grab an envelope, and randomly take out one ball â€“ itâ€™s black â€¢ At this point youâ€™re given the option to switch envelopes. Should you switch or not? Similar to the â€œMonty Hall Problemâ€ 65 Inference with Bayesâ€™s Rule E: envelope, 1 = (R,B), 2 = (B,B) B: the event of drawing a black ball Given: P(B|E=1) = 0.5, P(B|E=2) = 1, P(E=1) = P(E=2) = 0.5 Query: Is P(E=1 | B) > P(E=2 | B)? Use Bayesâ€™s rule: P(E|B) = P(B|E)*P(E) / P(B) P(B) = P(B|E=1)P(E=1) + P(B|E=2)P(E=2) = (.5)(.5) + (1)(.5) = .75 P(E=1|B) = P(B|E=1)P(E=1)/P(B) = (.5)(.5)/(.75) = 0.33 P(E=2|B) = P(B|E=2)P(E=2)/P(B) = (1)(.5)/(.75) = 0.67 After seeing a black ball, the posterior probability of this envelope being #1 (thus worth $100) is smaller than it being #2 Thus you should switch! Conditioning rule 66 15 Another Example â€¢ 1% of women over 40 who are tested have breast cancer. 85% of women who really do have breast cancer have a positive mammography test (true positive rate). 8% who do not have cancer will have a positive mammography (false positive rate). â€¢ Question: A patient gets a positive mammography test. What is the chance she has breast cancer? 68 â€¢ Let Boolean random variable M mean â€œpositive mammography testâ€ â€¢ Let Boolean random variable C mean â€œhas breast cancerâ€ â€¢ Given: P(C) = 0.01 P(M|C) = 0.85 P(M|Â¬C) = 0.08 69 Compute the posterior probability: P(C|M) 70 â€¢ P(C|M) = P(M|C)P(C)/P(M) by Bayesâ€™s rule = (.85)(.01)/P(M) â€¢ P(M) = P(M|C)P(C) + P(M|Â¬C)P(Â¬C) by the Conditioning rule â€¢ So, P(C|M) = .0085/[(.85)(.01) + (.08)(1-.01)] = 0.097 â€¢ So, there is only a 9.7% chance that if you have a positive test you really have cancer! 71 16 Independence Two events A, B are independent if the following hold: â€¢ P(A, B) = P(A) * P(B) â€¢ P(A, Â¬B) = P(A) * P(Â¬B) â€¦ â€¢ P(A | B) = P(A) â€¢ P(B | A) = P(B) â€¢ P(A | Â¬B) = P(A) â€¦ 74 Independence â€¢ Independence is a kind of domain knowledge â€“ Needs an understanding of causation â€“ Very strong assumption â€¢ Example: P(burglary) = 0.001 and P(earthquake) = 0.002 â€“ Letâ€™s say they are independent â€“ The full joint probability table = ? 75 Independence â€¢ Given: P(B) = 0.001, P(E) = 0.002, P(B|E) = P(B) â€¢ The full joint probability distribution table (FJPD) is: â€¢ Need only 2 numbers to fill in entire table â€¢ Now we can do anything, since we have the FJPD Burglary Earthquake Prob. B E = P(B)P(E) B Â¬E Â¬B E Â¬B Â¬E 76 Independence â€¢ Given n independent, Boolean random variables, the FJPD has 2 n entries, but we only need n numbers (degrees of freedom) to fill in entire table â€¢ Given n independent random variables, where each can take k values, the FJPD table has: â€“ k n entries â€“ Only n(k-1) numbers needed (DOFs) 77 17 Conditional Independence â€¢ Random variables can be dependent, but conditionally independent â€¢ Example: Your house has an alarm â€“ Neighbor John calls when he hears the alarm â€“ Neighbor Mary calls when she hears the alarm â€“ Assume John and Mary donâ€™t talk to each other â€¢ Is JohnCall independent of MaryCall? â€“ No â€“ If John called, it is likely the alarm went off, which increases the probability of Mary calling â€“ P(MaryCall | JohnCall) â‰  P(MaryCall) 79 Conditional Independence â€¢ But, if we know the status of the Alarm, JohnCall will not affect whether or not Mary calls P(MaryCall | Alarm, JohnCall) = P(MaryCall | Alarm) â€¢ We say JohnCall and MaryCall are conditionally independent given Alarm â€¢ In general, â€œA and B are conditionally independent given Câ€ means: P(A | B, C) = P(A | C) P(B | A, C) = P(B | C) P(A, B | C) = P(A | C) P(B | C) 80 Independence vs. Conditional Independence â€¢ Say Alice and Bob each toss separate coins. A represents â€œAliceâ€™s coin toss is headsâ€ and B represents â€œBobâ€™s coin toss is headsâ€ â€¢ A and B are independent â€¢ Now suppose Alice and Bob toss the same coin. Are A and B independent? â€“ No. Say the coin may be biased towards heads. If A is heads, it will lead us to increase our belief in B beings heads. That is, P(B|A) > P(A) 81 â€¢ Say we add a new variable, C: â€œthe coin is biased towards headsâ€ â€¢ The values of A and B are dependent on C â€¢ But if we know for certain the value of C (true or false), then any evidence about A cannot change our belief about B â€¢ That is, P(B|C) = P(B|A, C) â€¢ A and B are conditionally independent given C 82 18 Revisiting Earlier Example â€¢ Let Boolean random variable M mean â€œpositive mammography testâ€ â€¢ Let Boolean random variable C mean â€œhas breast cancerâ€ â€¢ Given: P(C) = 0.01 P(M|C) = 0.85 P(M|Â¬C) = 0.08 85 Bayesâ€™s Rule with Multiple Evidence â€¢ Say the same patient goes back and gets a second mammography and it too is positive. Now, what is the chance she has Cancer? â€¢ Let M1, M2 be the 2 positive tests â€¢ M1 and M2 are not independent â€¢ Compute posterior: P(C|M1, M2) 86 Bayesâ€™s Rule with Multiple Evidence â€¢ P(C|M1, M2) = P(M1, M2|C)P(C)/P(M1, M2) by Bayesâ€™s rule = P(M1|M2, C)P(M2|C)P(C)/P(M1, M2) â€¢ P(M1, M2) = P(M1, M2|C)P(C) + P(M1, M2|Â¬C)P(Â¬C) by Conditioning rule = P(M1|M2, C)P(M2|C)P(C) + P(M1|M2, Â¬C)P(M2|Â¬C)P(Â¬C) by Conditionalized Chain rule Conditionalized Chain rule 87 Cancer â€œcausesâ€ a positive test, so M1 and M2 are conditionally independent given C, so â€¢ P(M1|M2, C) = P(M1 | C) = 0.85 â€¢ P(M1, M2) = P(M1|M2, C)P(M2|C)P(C) + P(M1|M2, Â¬C)P(M2|Â¬C)P(Â¬C) = P(M1|C)P(M2|C)P(C) + P(M1|Â¬C)P(M2|Â¬C)P(Â¬C) by cond. indep. = (.85)(.85)(.01) + (.08)(.08)(1-.01) = 0.01356 So, P(C|M1, M2) = (.85)(.85)(.01)/ .01356 = 0.533 or 53.3% 88 19 Example â€¢ Prior probability of having breast cancer: P(C) = 0.01 â€¢ Posterior probability of having breast cancer after 1 positive mammography: P(C|M1) = 0.097 â€¢ Posterior probability of having breast cancer after 2 positive mammographies (and cond. independence assumption): P(C|M1, M2) = 0.533 89 Bayesâ€™s Rule with Multiple Evidence â€¢ Say the same patient goes back and gets a second mammography and it is negative. Now, what is the chance she has cancer? â€¢ Let M1 be the positive test and Â¬M2 be the negative test â€¢ Compute posterior: P(C|M1, Â¬M2) 90 Bayesâ€™s Rule with Multiple Evidence â€¢ P(C|M1, Â¬M2) = P(M1, Â¬M2|C)P(C)/ P(M1, Â¬M2) by Bayesâ€™s rule = P(M1|C)P(Â¬M2|C)P(C)/P(M1, Â¬M2) = (.85)(1-.85)(.01)/P(M1, Â¬M2) â€¢ P(M1, Â¬M2) = P(M1, Â¬M2|C)P(C) + P(M1, Â¬M2|Â¬C)P(Â¬C) by Conditioning rule = P(M1|Â¬M2, C)P(Â¬M2|C)P(C) + P(M1|Â¬M2, Â¬C)P(Â¬M2|Â¬C)P(Â¬C) by Conditionalized Chain rule 91 Cancer â€œcausesâ€ a positive test, so M1 and Â¬M2 are conditionally independent given C, so P(M1|Â¬M2, C)P(Â¬M2|C)P(C) + P(M1|Â¬M2, Â¬C)P(Â¬M2|Â¬C)P(Â¬C) = P(M1|C)P(Â¬M2|C)P(C) + P(M1|Â¬C)P(Â¬M2|Â¬C)P(Â¬C) by cond. indep. = (.85)(1 - .85)(.01) + (1 - .08)(.08)(1 - .01) = 0.074139 (= P(M1, Â¬M2)) So, P(C|M1, Â¬M2) = (.85)(1 - .85)(.01)/ .074139 = 0.017 or 1.7% 92 20 Bayesâ€™s Rule with Multiple Evidence and Conditional Independence Chain rule Conditionalized Chain rule + conditional independence 93 Inference Ignorance â€¢ â€œInferences about Testosterone Abuse Among Athletes,â€ 2004 â€“ Mary Decker Slaney doping case â€¢ â€œJustice Flunks Math,â€ 2013 â€“ Amanda Knox trial in Italy 94 NaÃ¯ve Bayes Classifier â€¢ Classification problem: Find the value of class/decision/diagnosis variable Y that is most likely given evidence/measurements/attributes Xi = vi â€¢ Use Bayesâ€™s rule and conditional independence: â€¢ Try all possible values of Y and pick the value that gives the maximum probability â€¢ But denominator, P(X1=v1, â€¦, Xn=vn), is a constant for all values of Y, so it wonâ€™t affect which value of Y is best P(Y = c | X1 = v1, X2 = v2,..., Xn = vn ) = P(Y = c)P(X1 = v1 | Y = c)...P(Xn = vn | Y = c) / P(X1 = v1,..., Xn = vn ) 97 Naive Bayes Classifier Testing Phase â€¢ For a given test instance defined by X1=v1, â€¦, Xn=vn, compute â€¢ Assumes all evidence variables are conditionally independent of each other given the class variable â€¢ Robust because it gives the right answer as long as the correct class is more likely than all others Class variable Evidence variable ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥,P(Y=c) âˆ./0 1 ğ‘ƒ(ğ‘‹.=ğ‘£. | Y=c) 98 21 Compute from the Training set all the necessary Prior and Conditional probabilities NaÃ¯ve Bayes Classifier Training Phase J C HZ P(J) = P(C|J) = P(C|Â¬J) = P(Z|J) = P(Z|Â¬J) = P(H|J) = P(H|Â¬J) = J Person is Junior C Brought coat to class Z Lives in zipcode 53706 H Saw â€œHunger Games 1â€ more than once 101 NaÃ¯ve Bayes Classifier J C HZ P(J) = P(C|J) = P(C|Â¬J) = P(Z|J) = P(Z|Â¬J) = P(H|J) = P(H|Â¬J) = # Juniors ---------------------------- # people in database # Juniors who saw H>1 --------------------------------- # Juniors # non-juniors who saw H>1 --------------------------------------- # non-juniors J Person is Junior C Brought coat to class Z Lives in zipcode 53706 H Saw â€œHunger Games 1â€ more than once 102 NaÃ¯ve Bayes Classifier â€¢ Assume k classes and n evidence (i.e., attribute) variables, each with m possible values â€¢ k-1 values needed for computing P(Y=c) â€¢ (m-1)k values needed for computing P(Xi=vi |Y=c) for each evidence variable Xi â€¢ So, (k-1) + n(m-1)k values needed instead of exponential size FJPD table 103 NaÃ¯ve Bayes Classifier â€¢ Conditional probabilities can be very, very small, so instead use logarithms to avoid underflow: arg maxc log P(Y = c) + log P( i=1 n âˆ‘ Xi = vi | Y = c) 106 22 Add-1 Smoothing â€¢ Unseen event problem: Training data may not include some cases â€“ flip a coin 3 times, all heads Ã  one-sided coin? â€“ Conditional probability = 0 â€“ Just because a value doesnâ€™t occur in the training set doesnâ€™t mean it will never occur â€¢ â€œAdd-1 Smoothingâ€ ensures that every conditional probability > 0 by pretending that youâ€™ve seen each attributeâ€™s value 1 extra time 107 Add-1 Smoothing â€¢ Compute Conditional probabilities as â€¢ Note: 6 ./0 7 ğ‘ƒ ğ‘‹ = ğ‘£ğ‘– ğ‘Œ = ğ‘ = 1 ğ‘ƒ ğ‘‹ = ğ‘£ğ‘– ğ‘Œ = ğ‘) = ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘‹ = ğ‘£ğ‘–, ğ‘Œ = ğ‘) + 1 ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡(ğ‘Œ = ğ‘) + ğ‘š where m = number of possible values for attribute X number of times attribute X has value vi in all training instances with class c number of training instances with class c 108 Add-1 Smoothing â€¢ Compute Prior probabilities as where N = size of the training set, k = number of classes ğ‘ƒ ğ‘Œ = ğ‘ = ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡ ğ‘Œ = ğ‘ + 1 ğ‘ + ğ‘˜ 110 Laplace Smoothing â€¢ aka Add-Î´ Smoothing â€¢ Instead of adding 1, add Î´ (a positive real number) â€¢ Compute conditional probabilities as ğ‘ƒ(ğ‘‹=vi | Y=c) = ,FG1H I/J.,K/, L M ,FG1H K/, L M7 where m = number of possible values for attribute X 112","libVersion":"0.5.0","langs":""}