{"path":"sem5/AI/pdf/13_bayes-net.pdf","text":"1 Bayesian Networks (aka Bayes Nets, Belief Nets, Directed Graphical Models) [based on slides by Jerry Zhu and Andrew Moore] Chapter 14.1, 14.2, and 14.4 plus optional paper “Bayesian networks without tears” 1 • Probabilistic models allow us to use probabilistic inference (e.g., Bayes’s rule) to compute the probability distribution over a set of unobserved (“hypothesis”) variables given a set of observed variables • Full joint probability distribution table is great for inference in an uncertain world, but is terrible to obtain and store • Bayesian Networks allow us to represent joint distributions in manageable chunks using § Independence, conditional independence • Bayesian Networks can do any inference Introduction 2 Full Joint Probability Distribution Making a joint distribution of N variables: 1. List all combinations of values (if each variable has k values, there are kN combinations) 2. Assign each combination a probability 3. They should sum to 1 Weather Temperature Prob. Sunny Hot 150/365 Sunny Cold 50/365 Cloudy Hot 40/365 Cloudy Cold 60/365 Rainy Hot 5/365 Rainy Cold 60/365 3 Using the Full Joint Distribution • Once you have the joint distribution, you can do anything, e.g. marginalization: P(E) = ∑rows matching E P(row) • e.g., P(Sunny or Hot) = (150+50+40+5)/365 Convince yourself this is the same as P(sunny) + P(hot) - P(sunny and hot) Weather Temperature Prob. Sunny Hot 150/365 Sunny Cold 50/365 Cloudy Hot 40/365 Cloudy Cold 60/365 Rainy Hot 5/365 Rainy Cold 60/365 4 2 Using the Joint Distribution • You can also do inference: ∑rows matching Q AND E P(row) P(Q | E) = ∑rows matching E P(row) Weather Temperature Prob. Sunny Hot 150/365 Sunny Cold 50/365 Cloudy Hot 40/365 Cloudy Cold 60/365 Rainy Hot 5/365 Rainy Cold 60/365 P(Hot | Rainy) 5 The Bad News • Full Joint distribution requires a lot of storage space • For N variables, each taking k values, the joint distribution has kN numbers (and kN – 1 degrees of freedom) • It would be nice to use fewer numbers … • Bayesian Networks to the rescue! § Provides a decomposed / factorized representation of the FJPD § Encodes a collection of conditional independence relations 6 • Represent (direct) dependencies graphically • Directed, acylic graphs (DAGs) • Nodes = random variables § “CPT” stored at each node quantifies conditional probability of node’s r.v. given all its parents • Directed arc from A to B means A “has a direct influence on” or “causes” B § Evidence for A increases likelihood of B (deductive influence from causes to effects) § Evidence for B increases likelihood of A (abductive influence from effects to causes) • Encodes conditional independence assumptions Bayesian Networks 14 Example § A: your alarm sounds § J: your neighbor John calls you § M: your other neighbor Mary calls you § John and Mary do not communicate (they promised to call you whenever they hear the alarm) • What kind of independence do we have? • What does the Bayes Net look like? 15 3 Conditional Independence • Random variables can be dependent, but conditionally independent • Example: Your house has an alarm § Neighbor John will call when he hears the alarm § Neighbor Mary will call when she hears the alarm § Assume John and Mary don’t talk to each other • Is JohnCall independent of MaryCall? § No – If John called, it is likely the alarm went off, which increases the probability of Mary calling § P(MaryCall | JohnCall) ≠ P(MaryCall) 16 Conditional Independence • But, if we know the status of the alarm, JohnCall will not affect whether or not Mary calls P(MaryCall | Alarm, JohnCall) = P(MaryCall | Alarm) • We say JohnCall and MaryCall are conditionally independent given Alarm • In general, “A and B are conditionally independent given C” means: P(A | B, C) = P(A | C) P(B | A, C) = P(B | C) P(A, B | C) = P(A | C) P(B | C) 17 Example § A: your alarm sounds § J: your neighbor John calls you § M: your other neighbor Mary calls you § John and Mary do not communicate (they promised to call you whenever they hear the alarm) • What kind of independence do we have? • Conditional independence: P(J,M|A)=P(J|A)P(M|A) • What does the Bayes Net look like? A J M 18 Examples § A: your alarm sounds § J: your neighbor John calls you § M: your other neighbor Mary calls you § John and Mary do not communicate (they promised to call you whenever they hear the alarm) • What kind of independence do we have? • Conditional independence P(J,M|A)=P(J|A)P(M|A) • What does the Bayes Net look like? A J M Our BN: P(A,J,M) = P(A) P(J|A) P(M|A) Chain rule: P(A,J,M) = P(A) P(J|A) P(M|A,J) Our BN assumes conditional independence, so P(M|A,J) = P(M|A) 19 4 A Simple Bayesian Network CancerSmoking{ }heavylightnoS ,,Î { }malignantbenignnoneC ,,ÎP(S=no) 0.80 P(S=light) 0.15 P(S=heavy) 0.05 no light heavy P(C=none|S=) 0.96 0.88 0.60 P(C=benign|S=) 0.03 0.08 0.25 P(C=malig|S=) 0.01 0.04 0.15 Don’t need to store 21 A Bayesian Network Smoking GenderAge Cancer Lung Tumor Serum Calcium Exposure to Toxics 27 Applications • Medical diagnosis systems • Manufacturing system diagnosis • Computer systems diagnosis • Network systems diagnosis • Helpdesk troubleshooting • Information retrieval • Customer modeling 28 Pathfinder • Pathfinder was one of the first BN systems • It performed diagnosis of lymph-node diseases • It dealt with over 60 diseases and 100 symptoms and test results • 14,000 probabilities • Commercialized and applied to about 20 tissue types 32 5 Pathfinder Bayes Net 448 nodes, 906 arcs 33 35 Conditional Independence in Bayes Nets § A node is conditionally independent of its non-descendants, given its parents § A node is conditionally independent of all other nodes, given its “Markov blanket” (i.e., its parents, children, and children’s parents) 36 Conditional Independence Smoking GenderAge Cancer Cancer is conditionally independent of Age and Gender given Smoking 37 6 Conditional Independence Cancer Lung Tumor Serum Calcium Serum Calcium is conditionally independent of Lung Tumor, given Cancer P(L | SC, C) = P(L | C) 38 • 2 nodes are (unconditionally) independent if there’s no undirected path between them • If there is an undirected path between 2 nodes, then whether or not they are independent or dependent depends on what other evidence is known Interpreting Bayesian Nets A C B A and B are independent given nothing else, but are dependent given C 39 Example with 5 Variables § B: there’s burglary in your house § E: there’s an earthquake § A: your alarm sounds § J: your neighbor John calls you § M: your other neighbor Mary calls you • B, E are independent • J is directly influenced by only A (i.e., J is conditionally independent of B, E, M, given A) • M is directly influenced by only A (i.e., M is conditionally independent of B, E, J, given A) 40 Creating a Bayes Net • Step 1: Add variables. Choose the variables you want to include in the Bayes Net B: there’s burglary in your house E: there’s an earthquake A: your alarm sounds J: your neighbor John calls you M: your other neighbor Mary calls you B E A J M 41 7 Creating a Bayes Net • Step 2: Add directed edges • The graph must be acyclic • If node X is given parents Q1, …, Qm, you are saying that any variable that’s not a descendant of X is conditionally independent of X given Q1, …, Qm B: there’s burglary in your house E: there’s an earthquake A: your alarm sounds J: your neighbor John calls you M: your other neighbor Mary calls you B E A J M 42 Creating a Bayes Net • Step 3: Add CPTs (Conditional Probability Tables) • The table at node X must list P(X | Parent values) for all combinations of parent values B: there’s burglary in your house E: there’s an earthquake A: your alarm sounds J: your neighbor John calls you M: your other neighbor Mary calls you B E A J M e.g., you must specify P(J|A) AND P(J|¬A) since they don’t have to sum to 1! P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 43 Creating a Bayes Net 1. Choose a set of relevant variables 2. Choose an ordering of them, call them X1, …, XN 3. for i = 1 to N: 1. Add node Xi to the graph 2. Set parents(Xi) to be the minimal subset of {X1…Xi-1}, such that xi is conditionally independent of all other members of {X1…Xi-1} given parents(Xi) 3. Define the CPTs for P(Xi | assignments of parents(Xi)) • Different ordering leads to different graph, in general • Best ordering when each variable is considered after all variables that directly influence it 44 The Bayesian Network Created from a Different Variable Ordering 45 8 The Bayesian Network Created from a Different Variable Ordering 46 Compactness of Bayes Nets • A Bayesian Network is a graph structure for representing conditional independence relations in a compact way • A Bayes net encodes the full joint distribution (FJPD), often with far less parameters (i.e., numbers) • A full joint table needs kN parameters (N variables, k values per variable) § grows exponentially with N • If the Bayes net is sparse, e.g., each node has at most M parents (M << N), only needs O(NkM) parameters § grows linearly with N § can’t have too many parents, though 47 • Directed arc from one variable to another variable • Is A guaranteed to be independent of B? § No – Information can be transmitted over 1 arc (in either direction) • Example: My knowing the Alarm went off, increases my belief there has been a Burglary, and, similarly, my knowing there has been a Burglary increases my belief the Alarm went off Variable Dependencies A B 48 • This local configuration is called a “causal chain:” • Is A guaranteed to be independent of C? § No – Information can be transmitted between A and C through B if B is not observed • Example: B à A à M Not knowing Alarm means that my knowing that a Burglary has occurred increases my belief that Mary calls, and similarly, knowing that Mary Calls increases my belief that there has been a Burglary Causal Chain A CBA B C 49 9 • This local configuration is called a “causal chain:” • Is A independent of C given B? § Yes – Once B is observed, information cannot be transmitted between A and C through B; B “blocks” the information path; “C is conditionally independent of A given B” • Example: B à A à M Knowing that the Alarm went off means that also knowing that a Burglary has taken place will not increase my belief that Mary Calls Causal Chain A CBA B C 50 • This configuration is called “common cause:” • Is it guaranteed that B and C are independent? § No – Information can be transmitted through A to the children of A if A is not observed • Is it guaranteed that B and C are independent given A? § Yes – Observing the cause, A, blocks the influence between effects B and C; “B is conditionally independent of C given A” Common Cause A CB 51 • This configuration is called “common effect:” • Are A and B independent? § Yes • Example: B à A ß E Burglary and Earthquake cause the Alarm to go off, but they are not correlated § Proof: P(a,b) = Σc P(a,b,c) by marginalization = Σc P(a) P(b|a) P(c|a,b) by chain rule = Σc P(a) P(b) P(c|a,b) by cond. indep. = P(a) P(b) Σc P(c|a,b) = P(a) P(b) since last term = 1 Common Effect A B C 52 • This configuration is called “common effect:” • Are A and B independent given C? § No – Information can be transmitted through C among the parents of C if C is observed • Example: B à A ß E If I already know that the Alarm went off, my further knowing that there has been an Earthquake, decreases my belief that there has been a Burglary. Called “explaining away.” § Similarly, if C has descendant D and D is given, then A and B are not independent Common Effect A B C 53 10 Determining if two variables in a Bayesian Network are independent or conditionally independent given a set of observed evidence variables, is determined using “d-separation” D-separation is covered in CS 760 D-Separation 54 Computing a Joint Entry from a Bayes Net How to compute an entry in the joint distribution (FJPD)? E.g., what is P(S, ¬M, L, ¬R, T)? S M R L T P(S) = 0.3 P(M) = 0.6 P(R|M) = 0.3 P(R|¬M) = 0.6 P(T|L) = 0.3 P(T|¬L) = 0.8 P(L|M,S) = 0.05 P(L|M,¬S) = 0.1 P(L|¬M,S) = 0.1 P(L|¬M, ¬S) = 0.2 55 Computing with Bayes Net P(T, ¬R, L, ¬M, S) = P(T | ¬R, L, ¬M, S) * P(¬R, L, ¬M, S) = P(T | L) * P(¬R, L, ¬M, S) = P(T | L) * P(¬R | L, ¬M, S) * P(L, ¬M, S) = P(T | L) * P(¬R | ¬M) * P(L, ¬M, S) = P(T | L) * P(¬R | ¬M) * P(L | ¬M, S) * P(¬M, S) = P(T | L) * P(¬R | ¬M) * P(L | ¬M, S) * P(¬M | S) * P(S) = P(T | L) * P(¬R | ¬M) * P(L | ¬M, S) * P(¬M) * P(S) S M R L T P(S) = 0.3 P(M) = 0.6 P(R|M) = 0.3 P(R|¬M) = 0.6 P(T|L) = 0.3 P(T|¬L) = 0.8 P(L|M,S) = 0.05 P(L|M,¬S) = 0.1 P(L|¬M,S) = 0.1 P(L|¬M, ¬S) = 0.2 Apply the Chain Rule + conditional independence! 57 Before applying chain rule, best to reorder all of the variables, listing first the leaf nodes, then all the parents of the leaves, etc. Last variables listed are those that have no parents, i.e., the root nodes. So, for previous example, P(S,L,M,T,R) = P(T,R,L,S,M) Variable Ordering 58 11 The General Case P(X1=x1 , X2=x2 ,…., Xn-1=xn-1 , Xn=xn) = P(Xn=xn , Xn-1=xn-1 , …., X2=x2 , X1=x1) = P(Xn=xn | Xn-1=xn-1 , …., X2=x2 , X1=x1) * P(Xn-1=xn-1 , …, X2=x2 , X1=x1) = P(Xn=xn | Xn-1=xn-1 , …., X2=x2 , X1=x1) * P(Xn-1=xn-1 |… X2=x2 , X1=x1) * P(Xn-2=xn-2 ,…., X2=x2 , X1=x1) : = P Xi = xi( ) Xi−1 = xi−1( ), …, X1 = x1( )( )( ) i=1 n ∏ = P Xi = xi( ) Assignments of Parents Xi( )( ) i=1 n ∏ 59 Computing Joint Probabilities using a Bayesian Network How is any marginal probability computed? Sum the relevant joint probabilities: Compute: P(a,b) = P(a,b,c,d) + P(a,b,c,¬d) + P(a,b,¬c,d) + P(a,b,¬c,¬d) Compute: P(c) = P(a,b,c,d) + P(a,¬b,c,d) + P(¬a,b,c,d) + P(¬a,¬b,c,d) + P(a,b,c,¬d) + P(a,¬b,c,¬d) + P(¬a,b,c,¬d) + P(¬a,¬b,c,¬d) • A BN can answer any query (i.e., marginal probability) about the domain by marginalization (“summing out”) over the relevant joint probabilities A B C D 60 Where Are We Now? • We defined a Bayes net, using a small number of parameters, to describe the joint probability • Any joint probability can be computed as P(x1,…, xN) = ∏i P(xi | parents(xi)) • The above joint probability can be computed in time linear in the number of nodes, N • With this joint distribution, we can compute any conditional probability, P(Q | E); thus we can perform any inference • How? 61 ∑joint matching Q AND E P(joint) P(Q | E) = ∑joint matching E P(joint) Inference by Enumeration B E A J M P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 For example: P(B | J, ¬M) 1. Compute P(B,J, ¬M) 2. Compute P(J, ¬M) 3. Return P(B,J, ¬M)/P(J, ¬M) by def. of cond. prob. 62 12 Inference by enumeration ∑joint matching Q AND E P(joint) P(Q | E) = ∑joint matching E P(joint) B E A J M P(B)=0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E)=0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 For example: P(B | J,¬M) 1. Compute P(B,J,¬M) 2. Compute P(J,¬M) 3. Return P(B,J,¬M)/P(J,¬M) Compute the joints (4 of them) P(B, J, ¬M, A, E) P(B, J, ¬M, A, ¬E) P(B, J, ¬M, ¬A, E) P(B, J, ¬M, ¬A, ¬E) Each is O(N) for sparse graph P(x1,…xN) = ∏i P(xi | parents(xi)) Sum them up 63 Inference by enumeration ∑joint matching Q AND E P(joint) P(Q | E) = ∑joint matching E P(joint) B E A J M P(B)=0.001 P(A | B, E)=0.95 P(A | B, ~E)=0.94 P(A | ~B, E)=0.29 P(A | ~B, ~E)=0.001 P(E)=0.002 P(J|A)=0.9 P(J|~A)=0.05 P(M|A)=0.7 P(M|~A)=0.01 For example: P(B | J,¬M) 1. Compute P(B,J,¬M) 2. Compute P(J,¬M) 3. Return P(B,J,¬M)/P(J,~M) Compute the joints (8 of them) P(J,¬M,B,A,E) P(J,¬M,B,A,¬E) P(J,¬M,B,¬A,E) P(J,¬M,B,¬A,¬E) P(J,¬M,¬B,A,E) P(J,¬M,¬B,A,¬E) P(J,¬M,¬B,¬A,E) P(J,¬M,¬B,¬A,¬E) Each is O(N) for sparse graph P(x1,…xN) = ∏i P(xi | parents(xi)) Sum them up 64 Inference by Enumeration ∑joint matching Q AND E P(joint) P(Q | E) = ∑joint matching E P(joint) B E A J M P(B)=0.001 P(A | B, E)=0.95 P(A | B, ~E)=0.94 P(A | ¬B, E)=0.29 P(A | ¬B, ~E)=0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 For example: P(B | J,¬M) 1. Compute P(B,J,¬M) 2. Compute P(J,¬M) 3. Return P(B,J,¬M)/P(J,¬M) Sum up 4 joints Sum up 8 joints In general, if there are N Boolean variables, j query vars, and k evidence vars, how many joints to sum up? 65 Another Example Compute P(R | T, ¬S) from the following Bayes Net S M R L T P(S) = 0.3 P(M) = 0.6 P(R | M) = 0.3 P(R | ¬M) = 0.6 P(T | L) = 0.3 P(T | ¬L) = 0.8 P(L | M, S) = 0.05 P(L | M, ¬S) = 0.1 P(L | ¬M, S) = 0.1 P(L | ¬M, ¬S) = 0.2 66 13 Another Example Compute P(R | T, ¬S) S M R L T P(s)=0.3 P(M) = 0.6 P(R | M) = 0.3 P(R | ¬M) = 0.6 P(T | L)=0.3 P(T | ¬L)=0.8 P(L | M, S) = 0.05 P(L | M, ¬S) = 0.1 P(L | ¬M, S) = 0.1 P(L | ¬M, ¬S) = 0.2 Step 1: Compute P(R, T, ¬S) Step 2: Compute P(T, ¬S) Step 3: Return P(R, T, ¬S) ------------------ P(T, ¬S) 68 Another Example Compute P(R | T, ¬S) S M R L T P(s)=0.3 P(M) = 0.6 P(R | M) = 0.3 P(R | ¬M) = 0.6 P(T | L)=0.3 P(T | ¬L)=0.8 P(L | M, S) = 0.05 P(L | M, ¬S) = 0.1 P(L | ¬M, S) = 0.1 P(L | ¬M, ¬S) = 0.2 Step 1: Compute P(R, T, ¬S) Step 2: Compute P(T, ¬S) Step 3: Return P(R, T, ¬S) ------------------ P(T, ¬S) Sum of all the rows in the Joint that match R ∧ T ∧ ¬S Sum of all the rows in the Joint that match T ∧ ¬S 69 Another Example Compute P(R | T, ¬S) S M R L T P(s)=0.3 P(M)=0.6 P(R½M)=0.3 P(R | ¬M) = 0.6 P(T½L)=0.3 P(T | ~L)=0.8 P(L | M, S) = 0.05 P(L | M, ¬S) = 0.1 P(L| ¬M, S) = 0.1 P(L | ¬M, ¬S) = 0.2 Step 1: Compute P(R, T, ¬S) Step 2: Compute P(T, ¬S) Step 3: Return P(R, T, ¬S) ------------------------------------- P(R, T, ¬S) + P(¬R, T, ¬S) Sum of all the rows in the Joint that match R ∧ T ∧ ¬S Sum of all the rows in the Joint that match T ∧ ¬S Each of these obtained by the “computing a joint probability entry” method in the earlier slides 4 joint computes 8 joint computes = P(T, ¬S) 70 S M R L T Note: Inference through a Bayes Net can go both “forward” and “backward” across arcs • Causal (top-down) inference § Given a cause, infer its effects § E.g., P(T | S) • Diagnostic (bottom-up) inference § Given effects/symptoms, infer a cause § E.g., P(S | T) 71 14 The Good News We can do inference. That is, we can compute any conditional probability: P( Some variables | Some other variable values ) å å = Ù = 2 2 1 matching entriesjoint and matching entriesjoint 2 21 21 )entryjoint ( )entryjoint ( )( )( )|( E EE P P EP EEP EEP “Inference by Enumeration” Algorithm 72 The Bad News • In general if there are N variables and evidence contains j variables, and each variable has k values, how many joints to sum up? k(N-j) • It is this summation that makes inference by enumeration inefficient • Computing conditional probabilities by enumerating all matching entries in the joint is expensive: Exponential in the number of variables • Some computation can be saved by carefully ordering the terms and re-using intermediate results (variable elimination algorithm) • A more complex algorithm called a join tree (junction tree) can save even more computation • But, even so, exact inference with an arbitrary Bayes Net is NP-Complete 77 Variable Elimination Algorithm General idea: • Write query in the form • Iteratively § Move all irrelevant terms outside of innermost sum § Perform innermost sum, getting a new term § Insert the new term into the product P(xn,e) = ! P(xi | pa i ) i∏ x2 ∑ x3 ∑ xk ∑ 79 V S LT A B X D P(v)P(s)P(t | v)P(l | s)P(b | s)P(a | t,l)P(x | a)P(d | a,b) Compute P(d) Need to eliminate: v, s, x, t, l, a, b Initial factors: P(v, s, t, l, a, b, x, d) = Inference by Enumeration (i.e., brute force) approach: P(d) = P(v, s,t,l,a,b, x,d) v ∑ s ∑ t ∑ l ∑ a ∑ b ∑ x ∑ 80 15 V S LT A B X D P(v)P(s)P(t | v)P(l | s)P(b | s)P(a | t,l)P(x | a)P(d | a,b) • We want to compute P(d) • Need to eliminate: v, s, x, t, l, a, b Initial factors Eliminate: v Note: fv(t) = P(t) Compute: fv (t) = P(v)P(t | v) v ∑ ⇒ fv (t)P(s)P(l | s)P(b | s)P(a | t,l)P(x | a)P(d | a,b) Idea behind Variable Elimination Algorithm 81 Parameter (CPT) Learning for BN • Where do you get these CPT numbers? § Ask domain experts, or § Learn from data B E A J M P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 93 Parameter (CPT) Learning for BN § Learn from a data set like this: B E A J M P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 How to learn this CPT? (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (B, ¬E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (~B, ~E, ¬A, ¬J, ¬M) (B, E, A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) … 94 Parameter (CPT) Learning for BN § Learn from a data set like this: B E A J M P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 Count #(B) and #(¬B) in dataset. P(B) = #(B) / [#(B) + #(¬B)] (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (B, ¬E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (~B, ~E, ¬A, ¬J, ¬M) (B, E, A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) … 95 16 Parameter (CPT) Learning for BN § Learn from a data set like this: B E A J M P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 Count #(E) and #(¬E) in dataset. P(E) = #(E) / [#(E) + #(¬E)] (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (B, ¬E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (~B, ~E, ¬A, ¬J, ¬M) (B, E, A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) … 96 (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (B, ¬E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (~B, ~E, ¬A, ¬J, ¬M) (B, E, A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) … Parameter (CPT) Learning for BN § Learn from a data set like this: B E A J M P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 Count #(A) and #(¬A) in dataset where B=true and E=true. P(A|B,E) = #(A) / [#(A) + #(¬A)] 97 P(B) = 0.001 Parameter (CPT) Learning for BN § Learn from a data set like this: B E A J M P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 Count #(A) and #(¬A) in dataset where B=true and E=false. P(A|B,¬E) = #(A) / [#(A) + #(¬A)] (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (B, ¬E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (~B, ~E, ¬A, ¬J, ¬M) (B, E, A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) … 98 Parameter (CPT) Learning for BN § Learn from a data set like this: B E A J M P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 Count #(A) and #(¬A) in dataset where B=false and E=true. P(A|¬B,E) = #(A) / [#(A) + #(¬A)] (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (B, ¬E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (~B, ~E, ¬A, ¬J, ¬M) (B, E, A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) … 99 17 Parameter (CPT) Learning for BN § Learn from a data set like this: B E A J M P(B) = 0.001 P(A | B, E) = 0.95 P(A | B, ¬E) = 0.94 P(A | ¬B, E) = 0.29 P(A | ¬B, ¬E) = 0.001 P(E) = 0.002 P(J|A) = 0.9 P(J|¬A) = 0.05 P(M|A) = 0.7 P(M|¬A) = 0.01 Count #(A) and #(¬A) in dataset where B=false and E=false. P(A|¬B, ¬E) = #(A) / [#(A) + #(¬A)] p p p p (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (B, ¬E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (~B, ~E, ¬A, ¬J, ¬M) (B, E, A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) … 100 (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (B, ¬E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, J, ¬M) (¬B, E, A, J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (¬B, ¬E, ¬A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) (~B, ~E, ¬A, ¬J, ¬M) (B, E, A, ¬J, M) (¬B, ¬E, ¬A, ¬J, ¬M) … Parameter (CPT) Learning for BN § ‘Unseen event’ problem Count #(A) and #(¬A) in dataset where B=true and E=true. P(A|B,E) = #(A) / [#(A) + #(¬A)] What if there’s no row with (B, E, ¬A, *, *) in the dataset? Do you want to set P(A|B,E) = 1 P(¬A|B,E) = 0? Why or why not? 101 Smoothing CPTs § “Add-1” smoothing: add 1 to all counts § In the previous example, count #(A) and #(¬A) in dataset where B=true and E=true § P(A|B,E) = [#(A)+1] / [#(A)+1 + #(¬A)+1] § If #(A)=1, #(¬A)=0: § without smoothing P(A|B,E) = 1, P(¬A|B,E) = 0 § with smoothing P(A|B,E) = 0.67, P(¬A|B,E) = 0.33 § If #(A)=100, #(¬A)=0: § without smoothing P(A|B,E) = 1, P(¬A|B,E) = 0 § with smoothing P(A|B,E) = 0.99, P(¬A|B,E) = 0.01 § Smoothing saves you when you don’t have enough data, and hides away when you do § It’s a form of Maximum a posteriori (MAP) estimation 103 Naive Bayes Classifier Testing Phase • For a given test instance defined by X1=v1, …, Xn=vn, compute • Assumes all evidence variables are conditionally independent of each other given the class variable • Robust because it gives the right answer as long as the correct class is more likely than all others Class variable Evidence variable 𝑎𝑟𝑔𝑚𝑎𝑥(P(Y=c) ∏*+, - 𝑃(𝑋*=𝑣* | Y=c) 104 18 • A special Bayes Net structure: § a ‘class’ variable Y at root, compute P(Y | X1, …, XN) § evidence nodes Xi (observed features) are all leaves § conditional independence between all evidence assumed. Usually not valid, but often empirically OK BN Special Case: Naïve Bayes Y X1 XNX2 … 105 • What’s stored in the CPTs? A Special BN: Naïve Bayes Classifiers J C HZ J Person is Junior C Brought coat to class Z Lives in zipcode 53706 H Saw “Hunger Games 1” more than once 106 A Special BN: Naïve Bayes Classifiers J C HZ P(J) = P(C|J) = P(C|¬J) = P(Z|J) = P(Z|¬J) = P(H|J) = P(H|¬J) = J Person is Junior C Brought coat to class Z Lives in zipcode 53706 H Saw “Hunger Games 1” more than once 107 Bayesian Network Properties • Bayesian Networks compactly encode joint distributions • Topology of a Bayesian Network is only guaranteed to encode conditional independencies § Arcs do not necessarily represent causal relations 129 19 What You Should Know • Inference with full joint distribution • Problems of full joint distribution • Bayesian Networks: representation (nodes, arcs, CPT) and meaning • Compute joint probabilities from Bayes net • Inference by enumeration • Naïve Bayes classifier • You are NOT responsible for the Variable Elimination Algorithm 130","libVersion":"0.5.0","langs":""}