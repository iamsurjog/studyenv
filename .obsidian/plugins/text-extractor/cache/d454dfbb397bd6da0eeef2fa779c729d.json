{"path":"sem5/CAD/pdfs/_OceanofPDF.com_Solution_Architects_Handbook_-_Saurabh_Shrivastava.pdf","text":"Solutions Ar chitect's Handbook             Kick-start your solutions architect career by learning architecture designprinciples and strategies                  Saurabh ShrivastavaNeelanjali Srivastav                         BIRMINGHAM - MUMBAI Solutions Ar chitect's Handbook C o p y r i g h t © 2 0 2 0 P a c k t P u b l i s h i n g A l l r i g h t s r e s e r v e d . N o p a r t o f t h i s b o o k m a y b e r e p r o d u c e d , s t o r e d i n a r e t r i e v a l s y s t e m , o r t r a n s m i t t e d i n a n y f o r m o r b y a n y m e a n s , w i t h o u t t h e p r i o r w r i t t e n p e r m i s s i o n o f t h e p u b l i s h e r , e x c e p t i n t h e c a s e o f b r i e f q u o t a t i o n s e m b e d d e d i n c r i t i c a l a r t i c l e s o r r e v i e w s . E v e r y e f f o r t h a s b e e n m a d e i n t h e p r e p a r a t i o n o f t h i s b o o k t o e n s u r e t h e a c c u r a c y o f t h e i n f o r m a t i o n p r e s e n t e d . H o w e v e r , t h e i n f o r m a t i o n c o n t a i n e d i n t h i s b o o k i s s o l d w i t h o u t w a r r a n t y , e i t h e r e x p r e s s o r i m p l i e d . N e i t h e r t h e a u t h o r s , n o r P a c k t P u b l i s h i n g o r i t s d e a l e r s a n d d i s t r i b u t o r s , w i l l b e h e l d l i a b l e f o r a n y d a m a g e s c a u s e d o r a l l e g e d t o h a v e b e e n c a u s e d d i r e c t l y o r i n d i r e c t l y b y t h i s b o o k . P a c k t P u b l i s h i n g h a s e n d e a v o r e d t o p r o v i d e t r a d e m a r k i n f o r m a t i o n a b o u t a l l o f t h e c o m p a n i e s a n d p r o d u c t s m e n t i o n e d i n t h i s b o o k b y t h e a p p r o p r i a t e u s e o f c a p i t a l s . H o w e v e r , P a c k t P u b l i s h i n g c a n n o t g u a r a n t e e t h e a c c u r a c y o f t h i s i n f o r m a t i o n .   C o m m i s s i o n i n g E d i t o r :   K u n a l C h a u d h a r i A c q u i s i t i o n E d i t o r :   K a r a n G u p t a C o n t e n t D e v e l o p m e n t E d i t o r :   R u v i k a R a o S e n i o r E d i t o r :   A f s h a a n K h a n T e c h n i c a l E d i t o r :   G a u r a v G a l a C o p y E d i t o r :   S a f i s E d i t i n g P r o j e c t C o o r d i n a t o r :   F r a n c y P u t h i r y P r o o f r e a d e r :   S a f i s E d i t i n g I n d e x e r : M a n j u A r a s a n P r o d u c t i o n D e s i g n e r : J o s h u a M i s q u i t t a F i r s t p u b l i s h e d : M a r c h 2 0 2 0 P r o d u c t i o n r e f e r e n c e : 1 2 0 0 3 2 0 P u b l i s h e d b y P a c k t P u b l i s h i n g L t d . L i v e r y P l a c e 3 5 L i v e r y S t r e e t B i r m i n g h a m B 3 2 P B , U K . I S B N 9 7 8 - 1 - 8 3 8 6 4 - 5 6 4 - 9 www.packt.com                    T o our loving daughter Sanvi, who fills our lives with happiness and joy . – Saurabh and  Neelanjali    Packt.com Subscribe to our online digital library for full access to over 7,000 booksand videos, as well as industry leading tools to help you plan your personaldevelopment and advance your career . For more information, please visit our website. Why subscribe? Spend less time learning and more time coding with practical eBooksand V ideos from over 4,000 industry professionals Improve your learning with Skill Plans built especially for you Get a free eBook or video every month Fully searchable for easy access to vital information Copy and paste, print, and bookmark content Did you know that Packt of fers eBook versions of every book published, with PDF and ePub files available? Y ou can upgrade to the eBook version at www.packt.com and as a print book customer , you are entitled to a discount on the eBook copy . Get in touch with us at customercare@packtpub.com  for more details. At www.packt.com , you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts andof fers on Packt books and eBooks.  For eword The technology realm has always been fast-moving, and in order to keepgrowing in their careers, IT professionals need to incrementally acquire newskills over time. However , in the last decade, this trend of fast change has become dominant with cloud computing, becoming the 'new normal.' Now , almost every day , there are some new announcements, features, and service updates from cloud providers, which necessitates a focus on a culture ofcontinuous learning for everyone involved. Along with this, now the typicalboundaries between the usual roles of a developer , database administrator , security professional, build/release engineer , and so on have started to blur , and this has also resulted in new roles being created that focus on big-picture and end-to-end ownership. One such role is that of a 'SolutionsArchitect,' which started to evolve from existing roles in industry like'Application Architect' and 'IT Architect' and has now become mainstream.There are also variations of this role. However , the most common variant is that of 'Cloud Solutions Architect,' which is a pretty dynamic role in itself. Oftentimes, IT professionals want to switch roles, but they lack direction onhow to be successful on that path. This book focuses on this very aspectaround an ef fective transition from an existing IT role to that of a Solutions Architect and explains in a very logical manner the steps to embark on thatjourney . It starts of f with a simple, very relatable explanation of what this role entails and how it dif fers from a few of the other similar types of profiles. Post that, it goes into the technical skills and aspects of  knowledge that are needed to be a successful Solutions Architect. This begins withbasic design pillars and architectural principles (including high availability , reliability , performance, security , and cost optimizations), followed by a deep dive into each one of those. The book also covers some key conceptsaround cloud-native architectures, DevOps, and the data engineering andmachine learning domains, which are the cornerstone of any modern-dayarchitecture. I have personally been through this journey of being a Solutions Architectfrom a development team lead, and so has Saurabh, and we always wishedfor a handbook that could have helped us back then. So, to fill that majorgap in the industry , Saurabh has created this very detailed book, which is based on personal experiences and learnings that makes it a very relatableread for anyone from a variety of backgrounds. I highly recommend youread this book and keep it as a handy reference always, as in it you will findvery important nuggets of knowledge that will help you be a successfulSolutions Architect, and open up a new world of infinite possibilities! Kamal Arora Senior Manager , Solutions Ar chitectur e, A WS https://www.amazon.com/Kamal-Arora/e/B07HLTSNRJ/ A WS Solution Architect Leader and author of two Packt books, Cloud- Native Ar chitectur es and Ar chitecting Cloud-Native Applications ContributorsAbout the authors Saurabh Shrivastava is a technology leader , author , inventor , and public speaker with over 16 years of experience in the IT industry . He currently works at Amazon W eb Services as a Solutions Architect Leader and enables global consulting partners and enterprise customers on their journey to thecloud. Saurabh has also led global technical partnerships and also holds apatent in the area of cloud platform automation. Saurabh has written various blogs and white papers in a diverse set oftechnologies, such as big data, IoT , machine learning, and cloud computing. Prior to A WS, Saurabh worked as an enterprise solution architect and software architect in Fortune 50 enterprises, startups, and global productand consulting or ganizations.           Neelanjali Srivastav is a technology leader , agile coach, and cloud practitioner with over 14 years of experience in the software industry . She holds B.Sc. and M.Sc. degrees in bioinformatics and informationtechnology from Punjab University , Chandigarh. She currently leads teams of software engineers, solution architects, and systems analysts tomodernize IT systems and develop innovative software solutions for lar ge enterprises. She has worked in dif ferent roles in the IT services industry and R&D space. Neelanjali is a result-driven and top-performing leader who excels in project management and Agile Scrum methodologies for lar ge-scale enterprises on a global scale.About the r eviewer Kamesh Ganesan is a cloud evangelist and seasoned technology professional with nearly 23 years of IT experience in all major cloudtechnologies, including Azure, A WS, Google Cloud Platform ( GCP ), and Alibaba Cloud. W ith over 45 IT certifications, including 5 A WS, 3 Azure, and 3 GCP certifications, he's played many roles, including a certifiedmulti-cloud architect, cloud-native application architect, lead databaseadministrator , and programmer analyst. He architected, built, automated, and delivered high-quality , mission-critical, and innovative technology solutions to help his enterprise, along with commercial and governmentalclients, to be very successful and vastly improve their business value as aresult of using multi-cloud strategies.           Packt is sear ching for authors like you If you're interested in becoming an author for Packt, please visit authors.packt pub.com and apply today . W e have worked with thousands of developers and tech professionals, just like you, to help them share their insight with theglobal tech community . Y ou can make a general application, apply for a specific hot topic that we are recruiting an author for , or submit your own idea. T able of Contents Title Page Copyright and Credits Solutions Architect's Handbook Dedication About Packt Why subscribe? Foreword Contributors About the authors About the reviewer Packt is searching for authors like you Preface Who this book is for What this book covers To get the most out of this book Download the color images Conventions used Get in touch Reviews 1 . The Meaning of Solution Architecture What is solution architecture? Evolution of solution architecture Why is solution architecture important? The benefits of solution architecture Addressing the business needs and quality of delivery Selecting the best technology platform Addressing solution constraints and issues Helping in resource and cost management Managing solution delivery and project life cycle Addressing non-functional requirements Solution architecture in the public cloud What is the public cloud? Public clouds, private clouds, and hybrid clouds The public cloud architecture Public cloud providers and cloud service offering Summary 2 . Solution Architects in an Organization Types of solution architect role Enterprise solution architect Solution architect Technical architect&#xA0; Cloud architect&#xA0; Architect evangelist Infrastructure architect Network architect Data architect Security architect DevOps architect Understanding a solution architect's responsibilities Analyzing user requirements Defining non-functional requirements Engaging and working with stakeholders Handling various architecture constraints Making technology selections Developing a proof of concept and a prototype Designing solutions and staying through delivery Ensuring post-launch operability and maintenance Working as a technology evangelist Solution architects in an agile organization Why Agile methodology? Agile manifesto Agile process and terminology Sprint ceremonies Agile tools and terms Agile versus waterfall Agile architecture Summary 3 . Attributes of the Solution Architecture Scalability and elasticity The capacity dilemma in scaling Scaling your architecture Static content scaling Server fleet elasticity Database scaling High availability and resiliency Fault-tolerance and redundancy Disaster recovery and business continuity Extensibility and reusability Usability and accessibility Portability and interoperability Operational excellence and maintainability Security and compliance Authentication and authorization Web security Network security Infrastructure security Data security Cost optimization and budget Summary 4 . Principles of Solution Architecture Design Scaling workload Predictive scaling Reactive scaling Building resilient architecture Design for performance Using replaceable resources Creating immutable infrastructure Canary testing Think loose coupling Think service not server Using the right storage for the right need Think data-driven design Overcoming constraints Adding security everywhere Automating everything Summary 5 . Cloud Migration and Hybrid Cloud Architecture Design Benefits of cloud-native architecture Creating a cloud migration strategy Lift and Shift migration&#xA0; Rehost Replatform Relocate Cloud-native approach Refactor Repurchase Retain or retire Retain Retire Steps for cloud migration&#xA0; Discovering your workload Analyzing the information Creating migration plan Designing the application Performing application migration to the cloud Data migration Server migration Integration, validation, and cutover Live migration cutover Operating cloud application Application optimization in the cloud Creating a hybrid cloud architecture Designing a cloud-native architecture Popular public cloud choices Summary Further reading 6 . Solution Architecture Design Patterns Building an n-tier layered architecture The web layer The application layer The database layer Creating multi-tenant SaaS-based architecture Building stateless and stateful architecture designs Understanding SOA SOAP web service architecture RESTful web service architecture Building an SOA-based e-commerce website architecture Building serverless architecture Creating microservice architecture Real-time voting application reference architecture Building queue-based architecture Queuing chain pattern Job observer pattern Creating event-driven architecture Publisher/subscriber model Event stream model Building cache-based architecture Cache distribution pattern in a three-tier web architecture Rename distribution pattern Cache proxy pattern Rewrite proxy pattern App caching pattern Memcached versus Redis Understanding the circuit breaker pattern Implementing the bulkheads pattern Creating a floating IP pattern Deploying an application with a container The benefit of containers Container deployment Database handling in application architecture High-availability database pattern Avoiding anti-patterns in solution architecture Summary 7 . Performance Considerations Design principles for architecture performance Reducing latency Improving throughput Handling concurrency Apply caching Technology selection for performance optimization Making a computational choice Selecting the server instance Working with containers Docker Kubernetes Going serverless Choosing a storage Working with block storage and storage area network (SAN) Working with file storage and network area storage (NAS) Working with object storage and the cloud data storage Choosing the database Online transactional processing (OLTP) Nonrelational databases (NoSQL) Online analytical processing (OLAP) Building a data search Making the networking choice Defining a DNS routing strategy Implementing a load balancer Applying autoscaling Managing performance monitoring Summary 8 . Security Considerations Designing principles for architectural security Implementing authentication and authorization control Applying security everywhere Reducing blast radius Monitoring and auditing everything all the time Automating everything Protecting data Preparing a response Selecting technology for architectural security User identity and access management FIM&#xA0;and SSO Kerberos AD Amazon Web Services (AWS) Directory Service Security Assertion Markup Language (SAML) OAuth and OpenID Connect (OIDC) Handling web security Web app security vulnerabilities Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks SQLi attacks XSS attacks Cross-Site Request Forgery (CSRF) attacks Buffer overflow and memory corruption attacks Web security mitigation Web Application Firewall (WAF) DDoS mitigation Securing an application and its infrastructure Application and operating system hardening Software vulnerabilities and secure code Network, firewall, and trusted boundary IDS/IPS Host-based IDS Network-based IDS Data security Data classification Data encryption Encryption key management Envelope encryption AWS Key Management Service (KMS) Hardware security module (HSM) Data encryption at rest and in transit Security and compliance certifications The cloud's shared security responsibility model Summary 9 . Architectural Reliability Considerations Design principles for architectural reliability Making systems self-healing Applying automation Creating a distributed system Monitoring capacity Performing recovery validation Technology selection for architecture reliability Planning the RTO and RPO Replicating data Synchronous versus asynchronous replication Replication methods Planning disaster recovery Backup and restore Pilot light Warm standby Multi-site Applying best practices for disaster recovery Improving reliability with the cloud Summary 10 . Operational Excellence Considerations Designing principles for operational excellence Automating the operation Making incremental and reversible changes Predicting failures and responding Learning from the mistake and refining Keeping operation's runbook updated Selecting technologies for operational excellence Planning for operational excellence IT Asset Management (ITAM) Configuration management The functioning of operational excellence Monitoring system health Infrastructure monitoring Application monitoring Platform monitoring Log monitoring Security monitoring Handling alerts and incident response Improving operational excellence ITOA RCA Auditing and reporting Achieving operational excellence in the public cloud Summary 1 1 . Cost Considerations Design principles for cost optimization Calculating the total cost of ownership Planning the budget and forecast Managing demand and service catalogs Keeping track of expenditure Continuous cost optimization Techniques for cost optimization Reducing architectural complexity Increasing IT efficiency Applying standardization and governance Monitoring cost usage and report Cost optimization in the public cloud Summary 12 . DevOps and Solution Architecture Framework Introducing DevOps Understanding the benefits of DevOps Understanding the components of DevOps CI/CD Continuous monitoring and improvement IaC Configuration management (CM) Introducing DevSecOps Combining DevSecOps and CI/CD Implementing a CD strategy In-place deployment Rolling deployment Blue-green deployment Red-black deployment Immutable deployment Implementing continuous testing in the CI/CD pipeline A/B testing Using DevOps tools for CI/CD Code editor Source code management CI server Code deployment Code pipeline Implementing DevOps best practices Summary 13 . Data Engineering and Machine Learning What is big data architecture? Designing big data processing pipelines Data ingestion Technology choices for data ingestion Ingesting&#xA0;data to the cloud Storing data Technology choices for data storage Structured data stores Relational databases Data warehousing NoSQL databases SQL versus NoSQL databases Types of NoSQL data store Search data stores Unstructured data stores Data lakes Processing data and performing analytics Technology choices for data processing and analysis Visualizing&#xA0;data Technology choices for data visualization Understanding IoT What is ML? Working with data science and ML Evaluating ML models &#x2013; overfitting versus underfitting Understanding supervised and unsupervised ML Summary 14 . Architecting Legacy Systems Learning the challenges of legacy systems Difficulty in keeping up with user demand Higher cost of maintenance and update Shortage of skills and documentation Vulnerable to corporate security issues Incompatibility with other systems Defining a strategy for system modernization Benefits of system modernization Assessment of a legacy application Defining the modernization approach Documentation and support Looking at legacy system modernization techniques Encapsulation, rehosting, and re-platforming Refactoring and rearchitecting Redesigning and replacing Defining a cloud migration strategy for legacy systems Summary 15 . Solution Architecture Document Purpose of the SAD Views of the SAD Structure of the SAD Solution overview Business context Conceptual solution overview Solution architecture&#xA0; Solution delivery Solution management Appendix section of SAD IT procurement documentation for a solution architecture&#xA0; Summary 16 . Learning Soft Skills to Become a Better Solution Architect Acquiring pre-sales skills Presenting to C-level executives Taking ownership and accountability Defining strategy execution and&#xA0;OKRs Thinking big Being flexible and adaptable Design thinking Being a builder by engaging in coding hands-on Becoming better with continuous learning Being a mentor to others Becoming a technology evangelist and thought leader Summary Other Books You May Enjoy Leave a review - let other readers know what you think Pr eface This book guides readers to create a robust, scaled, highly available, andfault-tolerant solution by learning a dif ferent aspect of solution architecture and next-generation architecture design in a cloud environment. This bookwill start by providing an understanding of solution architecture and how itfits into an agile enterprise environment. It will take the reader through thejourney of solution architecture design by providing detailed knowledge ofdesign pillars, advanced design patterns, anti-patterns, and the cloud-nativeaspects of modern software design. The reader will dive deep into performance optimization, security , compliance, reliability , cost optimization, and operational excellence in solution design. This book provides an in-depth understanding of theautomation of security , infrastructure, DevOps, disaster recovery , and the documentation of solution architecture. In addition, readers will learnvarious architecture designs in cloud platform like Amazon W eb Services ( A WS ) and how to best utilize cloud platform for your solution design, modernization and migration need. This book also provides a good understanding of future-proof architecturedesign with data engineering, machine learning, and the Internet of Things ( IoT ). As a bonus, this book will also of fer a soft skill aspect to enhance your solution architecture skills and continuous learning techniques. Thisbook will you to start your career journey and improve your skills in thesolution architect role. By end of this book, you will have gained therequired skills to advance your career as a solution architect.  Who this book is for This book is for software developers, system engineers, DevOps engineers,architects, and team leaders working in the IT industry , who aspire to become solution architects and are keen on designing secure, reliable, high-performance, and cost-ef fective architectures. What this book covers Chapter 1 ,  The Meaning of Solution Ar chitectur e , is used as a base to define what solution architecture is and explain its importance. It explains variousbenefits of having a solution architecture in place and talks aboutarchitecting on the public cloud. Chapter 2 , Solution Ar chitects in an Or ganization , talks about the dif ferent types of solution architect roles and how they fit into the or ganizational structure. It explores the various responsibilities of the solution architect indetail. It further explains the solution architect's role fit in an agileor ganization along with agile processes.  Chapter 3 ,  Attributes of the Solution Ar chitectur e , throws light on various attributes of solution architecture, such as scalability , resiliency , disaster recovery , accessibility , usability , security , and cost. It explains the coexistence and utilization of these architectural attributes to create anef ficient solution design.  Chapter 4 ,  Principles of Solution Ar chitectur e Design , talks about architectural principles for creating scalable, resilient, and high-performance architecture. It explains ef ficient architecture design by applying security , overcoming constraints, and applying changes, along with test and automation approaches. It explores architecture principals touse design thinking ef fectively by exploring service-oriented architecture and taking a data-driven approach. Chapter 5 ,  Cloud Migration and Hybrid Cloud Ar chitectur e Design , explains the benefits of the cloud and approaches to design cloud-native architecture.It gives an understanding of dif ferent cloud migration strategies and migration steps. It talks about hybrid cloud design and explores popularpublic cloud providers. Chapter 6 ,  Solution Ar chitectur e Design Patterns,  explores various architecture design patterns, such as layered, microservice, event-driven,queue-based, serverless, cache-based, and service-oriented with examples.It demonstrates the applicability of solution architecture attributes andprinciples to design the best architecture as per business requirements. Itexplains about the various reference architectures in A WS cloud platform. Chapter 7 ,  Performance Considerations , provides an understanding of essential attributes of application performance improvement such aslatency , throughput, and concurrency . It explains various technology choices to improve performance at multiple layers of architecture, includingcompute, storage, database, and networking, along with performancemonitoring. Chapter 8 ,  Security Considerations , talks about the various design principles applicable to securing your workload. Security needs to apply at every layerand to every component of the architecture, and this chapter helps you toget an understanding of the right selection of technology to ensure yourarchitecture is secure at every layer . It explores the industry compliance guidelines applicable to architecture design and explains security in thecloud with a shared responsibility model. Chapter 9 , Ar chitectural Reliability Considerations , talks about design principles to make your architecture reliable. It explores various disasterrecovery techniques to ensure high availability of your application, and datareplication methods for business process continuations. It explains bestpractices and the role of the cloud in the application to achieve reliability . Chapter 10 ,  Operational Excellence Considerations , talks about various processes and methods for achieving operational excellence in applications.It explains best practices and technology selections that apply throughoutthe phases of application design, implementation, and post-production toimprove application operability . It also explores operational excellence for cloud workloads. Chapter 11 ,  Cost Considerations , talks about the various techniques for optimizing cost without risking business agility . It explains multiple methods used to monitor cost and apply governance for cost control. Ithelps you to understand cost optimization using the cloud. Chapter 12 ,  DevOps and the Solution Ar chitectur e Framework , explains the importance of DevOps in application deployment, testing, and security . It explores DevSecOps and its role in the application's continuous deploymentand delivery pipeline. It talks about DevOps best practices and the tools andtechniques to implement them. Chapter 13 ,  Data Engineering and Machine Learning , talks about how to design big data and analytics architectures. It outlines the steps to create abig data pipeline, including data ingestion, storage, processing, andvisualization. It helps you to understand the concepts and technologyinvolved in IoT . It explores details about machine learning, model evaluation techniques, and gives an overview of various machine learningalgorithms. Chapter 14 ,  Ar chitecting Legacy Systems , talks about the various challenges and modernization drivers for legacy systems. It explains strategies andtechniques to modernize legacy systems. Use of the public cloud isbecoming a go-to strategy for many or ganizations, so this chapter also explores the cloud migration of legacy systems.  Chapter 15 , Solution Ar chitectur e Document , talks about the solution architecture document along with its structure and the various detailsrequired to accommodate the documentation. It explores a variety of ITprocurement documentation, where the solution architect participates inproviding feedback. Chapter 16 ,  Learning Soft Skills to Become a Better Solution Ar chitect , talks about various soft skills required for a solution architect to be successful inthe role. It helps you to understand methods to acquire strategic skills suchas pre-sales and executive communication, develop design thinking, andpersonal leadership skills such as thinking big and ownership . It explores techniques to establish yourself as a leader and continue improving yourskillset. T o get the most out of this book Prior experience of software architecture design will help you to followalong with this book. However , no specific prerequisites are required to understand this book. All the examples and relevant instructions areprovided throughout the various chapters. This book takes you into the deepconcepts underlying solution architecture design and does not requireknowledge of any particular programming language, framework, or tool. Download the color images W e also provide a PDF file that has color images of the screenshots/diagrams used in this book. Y ou can download it here: https://st atic.packt-cdn.com/downloads/9781838645649_ColorImages.pdf . Conventions used There are a number of text conventions used throughout this book. CodeInText : Indicates c ode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, andT witter handles. Here is an example: \" Y ou can apply data encryption in transit and at rest. When you push into a code repository ( git push ), it encrypts the data and then stores it. When you pull from a code repository( git pull ), it decrypts the data and then sends the data back to the caller . \" Bold : Indicates a new term, an important word, or w ords that you see onscreen. For example, words in menus or dialog boxes appear in the textlike this. Here is an example: \"The  Jenkin Master  of fload builds to the slave node instance in case of overload. \" W arnings or important notes appear like this. T ips and tricks appear like this. Get in touch Feedback from our readers is always welcome. General feedback : If you have questions about any aspect of this book, mention the book title in the subject of your message and  email us at customercare@packtpub.com . Errata : Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, wewould be grateful if you would report this to us. Please visit www.packtpub.com/ support/errata , selecting your book, clicking on the Errata Submission Form link, and entering the details. Piracy : If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the locationaddress or website name. Please contact us at copyright@packt.com with a link to the material. If you ar e inter ested in becoming an author : If there is a topic that you have expertise in and you are interested in either writing or contributing to abook, please visit authors.packtpub.com . Reviews Please leave a review . Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers canthen see and use your unbiased opinion to make purchase decisions, we atPackt can understand what you think about our products, and our authorscan see your feedback on their book. Thank you! For more information about Packt, please visit  packt.com . The Meaning of SolutionAr chitectur e This book is your first step in the solution architecture world. Throughoutthis book, you will learn all about solution architecture, and how to becomea solution architect. In this first chapter , let's understand the meaning of solution architecture. Solution architecture is the building blocks of solutiondevelopment in the or ganization. It helps to create a successful solution in a complex or ganization, where product development has dependencies on multiple groups. For the success of application development, defining solution architectureshould be the first step, which then lays out the foundations and the robustbuilding block of implementation. Solution architecture not only considersthe requirements of the business, but also handles critical, non-functionalrequirements such as scalability , high availability , maintainability , performance, security , and so on. A solution architect is a person who is responsible for designing solutionarchitecture by collaborating across stakeholders. The solution architectanalyzes the functional requirement and defines a non-functionalrequirement in order to cover all aspects of the solution and avoid anysurprises. Each solution has multiple constraints such as cost, budget,timeline, regulatory , and so on, and the solution architect considers them, while also creating the design and making technology selection. The solution architect develops a proof of concept and prototype in order toevaluate various technology platforms, and then chooses the best strategyfor solution implementation. They mentor the team throughout solutiondevelopment and provide post-launch guidance to maintain and scale thefinal product. In this chapter , you will learn about the following topics: What is solution architecture?Evolution for solution architectureUnderstanding the importance of solution architectureThe benefits of solution architectureThe workings of solution architecture in the public cloud What is solution ar chitectur e? If you ask around, there may be 10 dif ferent answers for the definition of solution architecture, and they may all be correct, as per their or ganization's structure. Each or ganization may see solution architecture from a dif ferent perspective, based on their business needs, or ganizational hierarchy , and solution complexity . In a nutshell, solution architecture defines and foresees multiple aspects of abusiness solution, at both the strategic and tactical perspectives. Solutionarchitecture is not just about a software solution. It covers all aspects of asystem, which will be included but not limited to, system infrastructure,networking, security , compliance requirement, system operation, cost, and reliability . As you can see, the following diagram provides a dif ferent aspect that a solution architect can address: C i r c l e o f s o l u t i o n a r c h i t e c t u r e As shown in the preceding diagram, a good solution architect addresses themost common aspects of the solution in an or ganization: Globally distributed teams : In this age of globalization, almost every product has users distributed across the globe, and stakeholder's groupsto take care of customer needs. Often, the software development teamhas an onshore – of fshore model, where a team works across dif ferent time zones to increase productivity and optimize project cost. Solutiondesign needs to consider a globally distributed team structure.Global compliance r equir ement : When you are deploying your solution globally , each country and region has its laws and compliance regime, which your solution needs to adhere. Some examples are asfollows: The Federal Risk and Authorization Management Pr ogram ( FedRAMP ) and Department of Defense   Cloud Computing Security Requir ements Guide ( DoD SRG ) for the USA The General Data Pr otection Regulation ( GDPR ) for Europe The Information Security Register ed Assessors Pr ogram ( IRAP ) for Australia The Center for Financial Industry Information Systems  ( FISC ) for Japan The Multi-T ier Cloud Security ( MTCS ) standard for Singapore The G-Cloud for the UK  The IT -Grundschutz for Germany The Multi-Level Pr otection Scheme ( MLPS ) Level 3 for China Also, compliance requirements are dif ferent from industry to the industry , for example, the International Organization for Standardization ( ISO ) 9001 (which is primarily for healthcare, life sciences, medical devices,and the automotive and aerospace industries), thePayment Card Industry Data Security Standard( PCI-DSS ) for finance, the Health Insurance Portability and Accountability Act  ( HIPP A ) for healthcare, and so on. Solution architecture needs toconsider that compliance in the design phase. Y ou will learn more about compliance in Chapter 8 , Security Considerations . Cost and budget : Solution architecture gives a good estimation of the overall cost of the project, which helps to define a budget. Thisincludes capital expenditur e ( CapEx ), which is the upfront cost and, operational expenditur e ( OpEx ), which is an ongoing cost. It helps management to create an overall budget for human resources,infrastructure resources, and other licensing-related costs.Solution implementation component : Solution architecture provides a high-level overview of dif ferent implementation components of the product beforehand, which helps to plan execution.Business r equir ements : Solution architecture considers all business requirements, which includes both functional and non-functionalrequirements. It makes sure that business requirements are compatible,therefore allowing them to be converted into the technicalimplementation stage and strike a balance between stakeholders. IT infrastructur e r equir ements : Solution architecture determines what kind of IT infrastructure is required to execute the project, whichincludes computing, storage, network, and so on. This helps to plan theIT resources more ef fectively . T echnology selection : During solution design, a solution architect creates a proof of concept and prototype, which considers thecorporate requirements, and then recommends the right technology andtools for implementation. Solution architecture aims to build in-houseversus third-party tool sourcing and define software standards acrossthe or ganization. End user r equir ements : Solution architecture pays special attention to the requirements of the end user , who will be the actual consumer of the product. It helps to discover the hidden requirements that a productowner may not be able to capture, due to a lack of technicality . During implementation and launch, solution architect provides a standarddocument and typical language structure in order to make sure that allof the requirements have been met to satisfy the user's needs.Solution maintenance : Solution architecture is not just about solution design and implementation, but it also takes care of post-launchactivities, such as solution scalability , disaster recovery , operational excellence, and so on.Pr oject timeline : Solution architecture designs the layout details of each component with their complexity , which further helps to define the project milestones and timeline by providing resource estimationand associated risks. An industry-standard and well-defined solution architecture address allbusiness requirements in a technical solution, and they make sure to deliverthe desired result in order to satisfy the stakeholders, as per theirexpectations in terms of the quality , availability , maintainability , and scalability of the solution. The initial design of a solution architecture may be conceived at a veryearly stage during the pre-sales cycle, such as the r equest for pr oposal  ( RFP ) or the r equest for information  ( RFI ) and is followed by the creation of a prototype or proof of concept, in order to discover anysolution risk. Solution architect also identifies whether to build a solution or to source it. It helps to identify technology selection, while also keeping anor ganization's critical security and compliance requirements in mind. There could be two primary situations for creating a solution architecture: First, enhancing technology for an existing application, which mayinclude hardware refresh or software re-architectingSecond, to create a new solution from scratch, where you get moreflexibility to choose the best fit of technology to address a businessrequirement However , while re-architecting the existing solution, you need to consider the minimal impact and create a solution that can best fit the currentenvironment. Solution architects may decide to re-build if re-architectingthe existing solution is not worth it, and a better solution can be provided bya re-build approach. In a more simplified language, solution architecture is about looking at allthe aspects of the system in order to generate a technical vision, whichprovides steps to implement the business requirements. Solutionarchitecture can define an implementation for a project or a group ofprojects in a complex environment, by putting together all of the dif ferent pieces that are related to data, infrastructure, networking, and softwareapplication. A good solution architecture not only satisfies the functionaland non-functional requirements, but also addresses system scalabilities andmaintenance in the long run. W e have just learned about an overview of solution architecture and its dif ferent aspects. In the next section, we will look at the evolution of solution architecture. Evolution of solution ar chitectur e Solution architecture has evolved with technological modernization. T oday , solution architecture design has changed drastically compared to a coupleof decades ago, due to the increasing use of the internet, the availability ofhigh-bandwidth network, the low cost of storage, and computer availability . Back in the days before the era of the internet, most solution designsfocused on providing a thick desktop client, which was capable of operatingwith low bandwidth and working of fline when a system could not connect to the internet. This technology started evolving in the last decade. Service-oriented ar chitectur e ( SOA ) started taking shape for distributed design, and applications started moving from monolithic to modern n-tier ar chitectur e , where the frontend server , application server , and database were live in their computer and the storage layer . These SOAs are mostly achieved by an XML-based messaging protocol, called Simple Object Access Pr otocol ( SOAP ). This majorly follows a client – server model in order to create services. In this age of digitization, you will see microservice-based solution designbecoming increasingly popular , which is based on JavaScript Object Notation  ( JSON )-based messaging and the Repr esentational State T ransfer ( REST ) service. These are W eb APIs, which do not require XML-based web service protocols (SOAPs) to support their interfaces.They rely on web-based HTTP protocols such as POST , GET , UPDATE , DELETE , and so on. Y ou will learn more about dif ferent architecture patterns in great detail in Chapter 6 , Solution Ar chitectur e Design Patterns . The microservice architecture addresses the need for changing requirementsin an agile environment, where any solution changes need to beaccommodated and deployed rapidly . Or ganizations have to be agile to stay ahead of the competition. This forces solution architecture to be flexible, compared to the waterfall model, where you have a long cycle of projectrelease. The web-based microservice architecture is fueled by an almost unlimitedresource capability , which is available from cloud providers  and which can scale in minutes or seconds. It's becoming easier to innovate, experiment,and change as solution architects and developers can risk failing withoutharming anything. Why is solution ar chitectur e important? Solution architecture is the building block for an overall enterprise softwaresolution that addresses specific problems and requirements. As the projectsize increases, the team becomes distributed globally . It is required to have a solution architecture in place for long-term sustainability and a solidfoundation. Solution architecture addresses various solution needs, keeping the businesscontext intact. It specifies and documents technology platforms, applicationcomponents, data requirements, resource requirements, and many importantnon-functional requirements such as scalability , reliability , performance, throughput, availability , security , and maintainability . Solution architecture is vital for any industry and its solution. In the absenceof solution architecture, there is a chance that software development couldfail; projects can get delayed, get over budget, and not deliver enoughfunctionalities. This scenario can be drastically improved by creating a solution architecture and applying experience and knowledge; all of whichare provided by a solution architect. It helps to keep stakeholders from allareas  – from non-technical business functions to technical development – on the same page, which avoids confusion, keeps the project on track withinschedule and time, and helps to derive maximum r eturn on investment ( ROI) . Often, the solution architect requires customer collaboration in order tounderstand specifications. In a solution architect's role, the architect needsmultiple skillsets from technical leaders and experts, to business analystsand project management. W e will learn more about the solution architect's role in Chapter 2 , Solution Ar chitects in an Or ganization . A good solution architecture puts specifications in place with a well-definedsolution, which helps us to deliver and accomplish the final product, alongwith smooth product operability after launch. A single problem can havemultiple solutions, and each solution has its constraints. Solutionarchitecture considers all the solutions, and finds the best way , by creating a hands-on proof of concept that accommodates all of the business andtechnical limitations. The benefits of solution ar chitectur e The previous section sheds light on the importance of solution architecture.This section will further explore and provide more details on the benefits ofsolution architecture in various aspects of an or ganization, as shown in the following solution architecture benefits diagram: A s o l u t i o n a r c h i t e c t u r e ' s b e n e f i c i a l a t t r i b u t e s The preceding diagram highlights the following attributes of a good solutionarchitecture: T echnology value and r equir ement : Solution architecture determines the return on investment, which solution can be obtained by a particulartechnology selection, and the market trend. The solution architect evaluates which technology an or ganization or project should adopt in order to achieve long-term sustainability , maintainability , and team comfort.Business goal : The primary responsibility of a solution architecture design is to accommodate the needs of the stakeholders and adaptchange in their requirements. Solution architecture converts businessgoals into a technical vision by analyzing market trends andimplementing the best practice. Solution architecture needs to beflexible enough to meet new , challenging, demanding, and rapidly changing business requirements. T arget date : A solution architect continuously works with all stakeholders, including the business team, customer , and development team. A solution architect defines the process standard and providesguidelines for solution development. He/she makes sure that the overallsolution is in alignment with the business objective and launch timeline,to ensure minimal chances of tar get date slippage. Incr eased Return on Investment (ROI) : Solution architecture determines the ROI  and help to measure the success of the project. Solution architecture forces businesses to think about how to reducecosts and remove waste in the process, in order to improve the overallROI.Market opportunity : Solution architecture involves the process of analyzing and continuously evaluating the latest trends in the market. It also helps with backing up and promoting new products.Budget and r esour cing : For a better budget, it is always recommended to invest well in estimation. A well-defined solution architecture helpsto understand the number of resources that are required for projectcompletion. This helps with a better budget forecast and resourceplanning.Pr oject timeline : Defining an accurate project timeline is very critical for solution implementation. A solution architect determines theresources and ef fort that are required during the design phase, which should help define the schedule. Now , you have had a high-level overview of solution architecture and its benefits. Let's dive deep into the everyday aspects of solution architecture. Addr essing the business needs and quality of delivery In the life cycle of product development, the most challenging phase is toestablish the nature of the requirements, especially when all the elementsneed to be addressed as high priority , and they keep changing rapidly . This challenge is even worse when there are dif ferent views of the same requirement from various stakeholders. For example, a business useranalyzes the page design from a user point of view , while a developer is looking at it from implementation feasibility and load latency perspectives.This can cause conflicts and misunderstandings of requirements betweenfunctional and technical members. In such cases, solution architecture helpsto bridge the gap, and define a standard that all members can understand. Solution architecture defines standard documentation, which can explainthe technical aspects to non-technical stakeholders and update themregularly . As a solution architecture's design spans across the or ganization and dif ferent teams, it can help to discover hidden requirements. The solution architect makes sure that the development team knows about therequirements, and also maintains the cycle of progress. A good solution architecture defines not only the solution design, but alsothe success criteria in the form of qualitative and quantitative output, inorder to ensure the quality of delivery . The qualitative output can be collected from user feedback, such as their sentiment analysis, whilequantitative output may include latency , performance, load time at the technical side, and sales numbers at the business side. T aking continuous feedback and adapting to it is the key to high-quality delivery , which should adhere to all the phases of solution design and development. Selecting the best technologyplatform In a rapid and competitive market, the biggest challenge is maintaining theuse of the best technologies. T oday , when you have multiple resources all around the world, then you have to choose a technology very carefully . The solution architecture design process can ef fectively tackle this problem. The selection of the technology stack plays a significant role in ef ficient solution implementation by the team. In solution architecture, we shoulduse dif ferent strategies to adopt various platforms, technologies, and tools. A solution architect should validate all of the needs carefully , and then evaluate and investigate the result with multiple parameters in order to findthe best-fit solution for the product development, by creating a workingmodel of the product as a prototype. A good solution architecture addresses the depth of dif ferent tools and technologies by investigating all possible architectural strategies, based onthe mixed-use case, techniques, tools, and code reuse, which comes fromyears of experience. The best platform simplifies the implementationprocess; however , the right technology selection is very critical. This can be achieved by building a prototype according to the business requirementassessment, and the agility , speed, and security of the application. Addr essing solution constraints and issues Any solution can be limited by various constraints and may encounterissues due to complexities or unforeseen risks. Solution architecture needsto balance multiple constraints, such as resources, technologies, cost,quality , time to market, frequently changing requirements, and so on. Each project has its own specific goal, requirement, budget, and timeline.Solution architecture evaluates all of the possible critical paths and sharesbest practices to achieve a project goal in a given timeframe and budget.This is a systematic approach, where all tasks are interdependent of its priortask, and in order to achieve success in the project, all tasks need to executein sequence. A delay in one task can impact the project timeline and canresult in the or ganization losing the market window to launch the product. If there is an issue in the project development process, the probability of aproject getting delayed is higher . Sometimes, you encounter problems that are limitations of technology or of the solution environment. If you have awell-thought-out solution architecture, the most common issues are relatedto the non-functional requirements; resources and budgeting can mitigatefor the product development life cycle.  A solution architect helps to drive the project by diving deep into eachcomponent of the project. They think of an out-of-the-box idea to save theproject from the issues, and they will prepare a backup plan in the event thatthings do not work out as in the main plan. They evaluate the best possibleway to execute the project by choosing the best practice and balancingconstraints. Helping in r esour ce and cost management There are always risks and uncertainties involved during solutionimplementation. It's very tedious to understand how much time a developer will spend on fixing a bug. A good solution architecture controls the cost and budget and reduces uncertainty by providing developers with the required guidance in terms of priority , dif ferent communication services, and details of each component. Solution architecture also creates documentation for keeping the system upto date, a deployment diagram, software patches, version; and enforces therunbook to solve frequent issues and business continuation processes. Italso addresses the indirect impacts of the cost of building a solution byconsidering extensibility , scalability , and other external factors that matter to the development environment. Managing solution delivery andpr oject life cycle Lots of planning is involved in the inception stage of solution architecture.Solution architecture starts with a strategic view and provides moretechnical implementation input as you move forward with the solutionimplementation. Solution architecture ensures an end-to-end solution delivery and impactsthe overall project life cycle. It defines a process standard for dif ferent phases of the project life cycle and makes sure that it is applied across theor ganization so that other dependencies can be addressed as the implementation moves forward. Solution architecture considers a holistic view of the project. It keepssyncing other dependent groups such as security , compliance, infrastructure, project management, and support, in order to keep them engaged indif ferent phases of the project life cycle as required. Addr essing non-functional r equir ements Often, you have to deal with the non-functional r equir ements ( NFRs ) in your application. For project success, it is highly essential to address them,as they have a broader impact on the overall project and solution. TheseNFRs can make or break your user base, and address very critical aspects ofa solution such as security , availability , latency concerns, maintenance, logging, masking confidential information, performance concerns,reliability , maintainability , scalability , usability , and so on. If these are not considered on time, it can impact your project delivery . The following diagram shows some of the most common NFRs: N o n - f u n c t i o n a l a t t r i b u t e s o f s o l u t i o n a r c h i t e c t u r e As shown, NFRs include the following attributes of solution architecture.However , there can be more NFRs, depending upon the project: Disaster r ecovery : T o make sure the solution is up and running in case of any unforeseen events.Security and compliance : Put a safety net in place for a solution to save it from an external attack, such as a virus, malware, and so on.Also make sure that the solution complies with local and industrylaws, by meeting compliance requirements.High availability : T o make sure the solution is always up and running. Scalability : T o make sure the solution can handle the additional load in case of increasing demands.Application performance : T o make sure the application is loading as per user expectation, and without much delay . Network r equest and r esponse latency : Any activity performing on the application should be completed within the appropriate time andshould not time out. Y ou will learn more details about the preceding attributes in Chapter 3 , Attributes of the Solution Ar chitectur e . Solution architecture defines an initial framework for product development and the building blocks of thesolution. While establishing a solution, architecture, quality , and customer satisfaction are always the main focus. Solution architecture needs to buildcontinuously by working on a proof of concept and keep exploring andtesting until the desired quality is reached. Solution ar chitectur e in the public cloud Solution architecture in the cloud has become increasingly important thesedays, and it is the future of application solutions. The great thing aboutcloud computing architecture is that you have an end-to-end view of allarchitecture components, which include the frontend platforms, theapplication development platform, servers, storage, database, automation,delivery , and the networks that are required to manage the entire solution landscape. Before jumping into solution architecture in the cloud, let's understand moreabout the public cloud, and how it is becoming a more essential and drivingtechnology platform. What is the public cloud? A public cloud is a base on the standard computing model in which aservice provider makes resources such as virtual machines, applications,storage, and so on, available to their customers over the internet. Publiccloud services of fer a pay-as-you-go  model. In the cloud computing model, a public cloud vendor provides on-demandavailability of IT resources such as the server , database, network, storage, and so on, which or ganizations can use with secure web-based interfaces, or through application programs over the internet. In most of the cases, thecustomer only pays for the services that they are applying for the durationof utilization, which saves costs for them by optimizing IT resources toreduce idle time. Y ou can annotate the public cloud with an electric power supply model, where you switch on the light and pay only for the amount of electricity youuse in units. As soon as you switch of f, you are not paying for it. It abstracts you from the complexity of power generation using turbines, resources tomaintain the facility , a lar ge infrastructure setup, and you use the entire service in a simplified way . Public clouds, private clouds, andhybrid clouds Here, you will learn about a high-level overview of the dif ferent types of cloud computing deployment models. Y ou will learn more details in Chapter 5 , Cloud Migration and Hybrid Cloud Ar chitectur e Design . A p rivate cloud , or on-pr emises , is registered to a single or ganization that owns and accesses it. Private clouds act as a replicate or extension of thecompany's existing data center . Often, a public cloud has shared tenancy , which means virtual servers from multiple customers share the samephysical server; however , they of fer dedicated physical servers to customers if the customer wants it for a license or compliance need. A public cloudsuch as Amazon W eb Services ( A WS ) , Microsoft Azure, or Google Cloud Platform  ( GCP ) creates a massive IT infrastructure, which can be utilized over the internet with a pay-as-you-go model. A third model is a hybrid cloud , used by lar ge enterprises who are moving their workload from on-premises to a cloud, where they still have a legacyapplication that cannot move to the cloud directly , or maybe they have a licensed application that needs to stay on-premises, or sometimes due tocompliance reasons, they need to secure data on-premises. In such asituation, the hybrid model helps, where the enterprise has to maintain apartial environment on-premises and move other applications to the publiccloud. Sometimes an or ganization moves to test and develop the environment to the public cloud and keep production environments on-premises. A hybrid model can vary depending upon the or ganization's cloud strategy . The public cloud ar chitectur e In a typical definition, the public cloud is a fully virtualized environment,which is accessible both over the internet or private network lines. However , in recent times, public cloud vendors also started of fering an on-premises physical infrastructure for better hybrid cloud adoption.  The public cloud provides a multi-tenancy model, where IT infrastructure such as storage andcompute are shared between multiple customers; however , they are isolated at the software and logical network levels, and do not interfere with eachother's workload. In the public cloud, by creating network-level isolation,or ganizations can have their virtual private cloud, which is equivalent to the logical data center . Public cloud storage achieves high durability and availability , by creating a redundancy model using multiple data centers, and robust data replication.This makes them achieve architecture resiliency and easy scalability . There are three major types of cloud computing model, as shown here: T y p e o f c l o u d c o m p u t i n g m o d e l As shown in the diagram, you can see a comparison between customerresponsibilities in the on-premises environment with the cloud computingservice model. In on-premises, the customer has to manage everything,while in the cloud computing model, customers can of fload responsibilities to the vendor and focus on their business needs. The following points arehigh-level details of services that are provided under dif ferent cloud computing models: Infrastructur e as a Service (IaaS) : In the IaaS model, a vendor provides infrastructure resources such as a compute server , networking components, and data storage space as managed services. It helpscustomers to use IT resources without worrying about handling datacenter overheads such as heating and cooling, racking and stacking,physical security , and so on.  Platform as a Service (PaaS) : The PaaS model adds a layer of service, where the vendor takes care of the resources that are required for yourdevelopment platform, such as the operating system, softwaremaintenance, patching, and so on, along with infrastructure resources.The PaaS model facilitates your team's focus on writing business logicand handle data, by taking care of all of the platform maintenanceburdens for you.Softwar e as a Service (SaaS) : The SaaS model adds one more layer of abstraction on top of the PaaS and IaaS models, where the vendorprovides ready-to-use software, and you pay for the service. Forexample, you use email services such as Gmail, Y ahoo! Mail, AOL, and so on, where you get your own space of email as a service, and youdon't have to worry about underlying applications or infrastructures. The fourth emer ging model is the Function as a Service  ( FaaS ) model, which is becoming popular in the building of serverless architecture usingservices including A WS Lambda. Y ou will learn more details about serverless architecture in Chapter 6 , Solution Ar chitectur e Design Patterns . Public cloud pr oviders and cloud service offering There are several public cloud providers in the IT industries; among them,key players are A WS, GCP , Microsoft Azure, and Alibaba Cloud. These providers of fer an array of services, from computing, storage, networking, databases, and application development, to big data analytics and AI/ML,and so on. In the following screenshot from the A WS console, you can see 100+ services that are of fered in multiple areas. The highlighted EC2 service, known as Amazon Elastic Cloud Compute, allows you to spin-up the virtualmachine in minutes in the A WS cloud: A W S c o n s o l e a n d s e r v i c e o f f e r i n g s Public cloud vendors not only provide infrastructure, but also facilita te an array of services in various areas such as analytics, big data, artificial intelligence, application development, email, security , monitoring, alert, and so on. W ith the public cloud, dif ferent technical capabilities become more accessible to the development team, which helps to drive innovation andreduce the time to market for the product launch.Summary In this chapter , you learned about the definition of solution architecture from both the industry standards and in a more simplified way . Y ou learned the importance of solution architecture, and how it can help the or ganization to achieve a more significant result and maximize the return on itsinvestments. This chapter helped you to understand the benefits of having asolution architecture, and how it helps in dif ferent aspects of solution design and implementation. In summary , solution architecture is a building block in a complex or ganization and is used to address all stakeholder's needs and establish a standard in order to fill the gap between business requirements andtechnical solutions. A good solution architecture not only addressesfunctional requirements but also puts long-term thought into, and takes careof, non-functional requirements such as scalability , performance, resiliency , high availability , disaster recovery , and so on. Solution architecture finds an optimal solution to accommodate the constraints of cost, resources,timelines, security , and compliance. Later in the chapter , you learned about the basics of cloud computing , solution architecture in the cloud environment, and about significant publiccloud providers and their service of ferings. This also helped you to gain a high-level overview of dif ferent cloud computing models, such as IaaS, PaaS, SaaS, and the cloud computing deployment models in the publiccloud, private cloud, and hybrid cloud. Finally , this chapter sheld some light on the evolution of solution architecture design. In the next chapter , you will learn all about the solution architecture role – the dif ferent types of solution architect, the role's responsibilities as regards solution architecture, and how they fit into an or ganizational structure and an agile environment. Solution Ar chitects in an Organization A solution architect understands the needs and goals of an or ganization. Often, solution architects work for the or ganization as part of a team. All stakeholders, processes, teams, and or ganization management af fect solution architect roles and their work. In this chapter , you will learn and understand the solution architect role and how solution architects fit withinthe or ganization. Following that, you will learn about the various types of solution architect and how they coexist in an or ganization. Y ou may need a generalist solution architect and other specialist solution architects,depending on your project's complexity . This chapter will provide details on the solution architect's responsibilityand how it can impact an or ganization's success. A solution architect wears multiple hats and business executives heavily depend on their experienceand decision-making to understand their technical vision. The solution-and-software development methodology has evolved over thelast few decades, from waterfall to agile environment, which a solutionarchitect needs to adopt. This chapter will provide details about the Agilemethodology and the iterative approach that a solution architect should takefor the continuous improvement of solution delivery . Overall, agile thinking is very important for a solution architect. In addition to solution design, solution architects need to handle variousconstraints to evaluate risk and plan risk mitigation. Also, qualitymanagement plays a significant role, which shouldn't be overlooked. Thesolution architect plays an essential role throughout the solution's life cycle(from requirement collection, solution design, and solution implementationto testing and launch). A solution architect needs to engage regularly post-launch to ensure thescalability , high availability , and maintainability of the solution. For broader consumer products, the solution architect also needs to work with the salesteam as a technology evangelist of the product through content publishingand public speaking in various forums.   In this chapter , you will learn about the following topics: T ypes of solution architect role Solution architect responsibilitiesSolution architects in an agile or ganization T ypes of solution ar chitect r ole In the previous chapter , you learned about solution architecture and how various stakeholders impact solution strategies. Now , let's understand the solution architect's role. The software solution can develop without asolution architect, depending on the project's size, but for a lar ge project, it is a requirement to have a dedicated solution architect. The success andfailure of the plan depends on the solution architect. There is always a need for someone who can make architectural decisionsfor the team and drive team collaboration with stakeholders. Sometimes, itis required to have multiple solution architects in the team, depending onthe size of the project. The dif ferent types of solution architects are depicted in the following diagram, showing how they have dif ferent self-dif ferentiating responsibilities in the or ganization: T y p e s o f s o l u t i o n a r c h i t e c t As shown in the preceding diagram, an or ganization can have multiple types of solution architects. Solution architect can be categorizedas  generalists or specialists . Generalist solution architects have the breadth that comes from multiple technical domains. Specialist solution architectshave very in-depth knowledge in their area of expertise, such as big data,security , and networking. A generalist solution architect needs to collaborate with a specialist solution architect, to align with project'srequirements and complexity . The role of a solution architect varies from or ganization to or ganization and you may encounter a variety of job titles related to solution architect, themost common being generalist solution architect roles. Their focuses are asfollows: Enterprise solution architect: Or ganization strategy Business architecture Solution architect: Solution designSolution integration T echnical architect: Software designSoftware development Cloud architect: Cloud strategyCloud migration Architect evangelist: Platform adoptionT echnical content There may be other titles (such as application architect and softwarearchitect); however , this depends on the or ganization's structure. Specialist solution architect roles are as follows: Infrastructure architect: IT infrastructure designSoftware standardization and patching Network architect: Network designIT network strategy and latency Data architect: Data engineering and analysisData science and data intelligence Security architect: Cyber securityIT compliance DevOps architect: IT automationContinuous integration and continuous deployment (CI/CD) There may be other types of specialist solution architect, such as migrationarchitect and storage architect. This, again, depends on the or ganization's structure. As per the project and or ganizational complexity , a solution architect can take on multiple roles or dif ferent solution architects can have overlapping responsibilities. Y ou will learn more about each architect role in subsequent sections. Enterprise solution ar chitect Do you ever think about how products launch in the information technologyindustry? This is where the enterprise solution role comes into the picture –they define best practices, culture, and suitable technologies. An enterprisearchitect works closely with stakeholders, subject matter experts, andmanagement to identify or ganizational strategies for information technology and make sure that their knowledge aligns with companybusiness rules. Enterprise architects handle solution design across the or ganization; they create long-term plans and solutions with stockholders and leadership. Oneof the most important aspects is to finalize which technologies should beused by the company and making sure the company is using thesetechnologies with consistency and integrity . Another important aspect of the enterprise architect is defining the businessarchitecture. In some or ganizations, you may see a business architect as the job title. Business architecture fills the gap between or ganizational strategy and its successful execution. It helps convert a map strategy into executableaction items and takes this to a tactical level for implementation. Overall, enterprise architects are more aligned with company visions andresponsibilities when it comes to defining or ganization-wide standards for the successful implementation of the business' vision. Solution ar chitect In general, this book explores the role of a solution architect in a moregeneric way . Still, you often see solution architects with dif ferent titles, as per the or ganization's structure, for example, enterprise solution architect, software architect, or technical architect. In this section, you will find somedistinct attributes related to the various titles. However , the responsibilities of the solution architect overlap, depending on an or ganization's structure. If you wish to know how a solution should be or ganized and delivered, then a solution architect plays an essential role in this context. A solutionarchitect designs the overall system and how dif ferent systems integrate across dif ferent groups. A solution architect defines the expected outcome by working with business stakeholders and providing a clear understandingof the delivery objective on the part of the technical team. The solution architect connects the dots across the or ganization and ensures consistency within dif ferent teams to avoid any last-minute surprises. The solution architect engages throughout the project life cycle and also definesmonitoring and alerting mechanisms to ensure smooth operations after aproduct's launch. The solution architect also plays an essential role inproject management by providing advice regarding resource, cost, andtimeline estimation. Overall, the solution architect gets to engage on a more tactical levelcompared to the enterprise architect. Sometimes, the solution architect takeson the role of an enterprise architect if a more strategic involvement isrequired. T echnical ar chitect  A technical architect can also be called application architect or softwarearchitect. A technical architect is responsible for software design anddevelopment. The technical architect works with the or ganization in terms of engineering and is more focused on defining technical details forsoftware development by a team. They also work across the or ganization to understand how integration will work alongside other parts of the softwaremodule, which may be managed by other groups. A technical architect can manage the details of API design and define APIperformance and scaling aspects. They make sure that the software is beingdeveloped consistently with the or ganization's standards and that they can easily integrate with another component. The technical architect is a point of contact for any technical questionrelated to the engineering team and will have the ability to troubleshoot thesystem as required. For a small software development project, you may notsee a technical architect role, as a senior engineer may take up that role andwork on software architecture design. The technical architect mentors and supports the software engineering teamby working closely with them and resolving any hindrance that arises fromcross-team integration or business requirements. Cloud ar chitect  The cloud architect role may not have been in existence within the lastdecade, but as cloud adoption is increasing among enterprises this is onerole that is in high demand in the current scenario. The cloud architect plansand designs the cloud environment and is responsible for deploying andmanaging the company's cloud computing strategies. Cloud architectsprovide breadth and depth for cloud services and can define the cloud-native design. As you learned in the Solution ar chitectur e in the public cloud section in  Cha pter 1 , The Meaning of Solution Ar chitectur e , using the cloud is now the current trend and it has become the norm for or ganizations to move onto a public cloud. Major cloud providers such as Amazon W eb Service, Microsoft Azure, and Google Cloud Platform are helping customers toadopt cloud platforms at exponential speed with Softwar e as a Service  ( SaaS ), Platform as a Service  ( PaaS ), and Infrastructur e as a Service  ( IaaS ) of ferings. Y ou will learn more about cloud architectures in  C hapter 5 , Cloud Migration and Hybrid Cloud Ar chitectur e Design . There are a lar ge number of enterprises that have an existing workload that they want to migrate into the cloud to utilize scalability , ease of business, and price benefits. A cloud architect can prepare a cloud migration strategyand develop a hybrid cloud architecture. A cloud architect can advise howon-premise applications will connect to the cloud and how dif ferent traditional of ferings fit into a cloud environment. For startup businesses and enterprises starting in the cloud, a cloud architectcan help to design a cloud-native architecture, which is more optimized forthe cloud and uses the full capabilities it provides. The cloud-nativearchitecture tends to be built on pay-as-you-go models to optimize cost and leverage automation available in the cloud. Cloud is now an essential part of enterprise strategy , and a cloud architect is a must-have role of companies want to succeed in the modern era byincreasing the pace of innovation and automation.Ar chitect evangelist An architect evangelist is also known as a technology evangelist , and this is a comparatively new role. This is a new paradigm in marketing,especially when you want to increase the adoption of complex solutionplatforms. People will always want to hear from an expert who has in-depthknowledge and the ability to answer their queries so that they can make aninformed decision. Here, architect evangelists come into the picture as theywork as a subject matter expert in a competitive environment. An architect evangelist can design the architecture based on customerrequirements, which resolves the customer's pain points, and results in thecustomer wins. The evangelist can be a trusted advisor for customers andpartners. The architect evangelist has a deep understanding of architecturalissues, concepts, and market trends to help secure platform adoption andshow revenue growth through market capture. T o increase platform adoption for the overall tar get audience, the architect evangelist writes public content such as blogs, whitepapers, and articles.They speak on public platforms such as industry summits, technical talks,and conferences. They conduct technical workshops and publish tutorials tospread the word about the platform. This makes it very important for asolution architect to have excellent written and verbal communicationskills, as you will often see solution architects taking technologyevangelism as an additional responsibility . Infrastructur e ar chitect An infrastructure architect is a specialist architect role heavily focused onenterprise IT infrastructure design, security , and data center operation. They work closely with solution architects to make sure that the or ganization's infrastructure strategy is aligned with its overall business requirements, andthey plan an appropriate resource capacity to fulfill this need by analyzingsystem requirements and the existing environment. They help reduce capitalexpenditure that could be utilized for operational spending to increaseor ganizational ef ficiency and ROI. The infrastructure architect is the backbone of the or ganization since they define and plan overall IT resources, from storage servers to individualworkspaces. The infrastructure architect creates detailed plans for procuringand setting up IT infrastructures. They define software standards, softwarepatching, and software update plan systems across an or ganization. The infrastructure architect handles infrastructure security and makes sure allenvironments are protected from unwanted virus attacks. They also plan fordisaster recovery and system backups to make sure business operations arealways running. In most e-commerce businesses, infrastructure architect jobs becomechallenging as they need to plan for the peak season such as Thanksgivingin USA, Boxing Day in Canada and UK, Diwali in India and so on, whenmost consumers start shopping. They need to prepare enough server andstorage capacity to accommodate the peak season, whose workload may be10 times higher than normal, thus increasing the cost. A system will besitting idle for most of the year , outside of the peak season. They need to plan for cost optimization and better user experience at the same time,which is another reason they may use the cloud to fulfill additional capacityand scale on-demand to reduce the cost. They need to ensure that systemsare occupied while supporting the growth of the new expertise. Overall, an infrastructure architect needs to have a good understanding ofdata center operation and the components involved, such as heating,cooling, security , racking and stacking, server , storage, backup, software installation and patching, load balancers, and virtualization.Network ar chitect Have you ever wondered how giant enterprises with multiple locations forof fices or stores are connected? Here, the network architect comes into the picture, as they orchestrate an or ganization's network communication strategy and establish communication between IT resources, giving life tothe IT infrastructure. A network architect is responsible for designing the computer network,Local Ar ea Network ( LAN ), W ide Ar ea Network ( W AN ), internet, intranet, and other communication systems. They manage or ganizational information and network systems. They ensure low network latency andhigh network performance is available for users to increase theirproductivity . They establish secure connectivity between user workspaces and the internal network using  V irtual Private Network ( VPN ) connectivity . The network architect works closely with the infrastructure architect andsometimes you see this as an overlapping role to ensure all ITinfrastructures are connected. They work with the security team and designthe or ganization's firewall to protect against unethical attacks. They are responsible for monitoring and protecting the network via packetmonitoring, port scanning, and putting  Intrusion Detection System ( IDS ) and Intrusion Pr evention System ( IPS ) into place. Y ou will learn more about IDS/IPS systems in Chapter 8 ,  Security Considerations . Overall, a network architect needs to have a good understanding of networkstrategies, network operations, secure connections using VPN, firewallconfiguration, network topology , load balance configuration, DNS routing, IT infrastructure connectivity , and so on. Data ar chitect Any solution design revolves around data, and it is mostly about storing,updating, and accessing it regardless of whether it is about customers orproducts. As the adoption of the internet is increasing, so are data and theneed for data architects. In the last decade, data growth has risenexponentially – not long ago, gigabytes of data were considered to be bigdata, but now even 100 terabytes of data are deemed to be normal. Y ou can even get a 1-terabyte computer hard disk. T raditionally , data used to be stored in a structured relational way . Now , most data are in an unstructured format generated from resources such associal media, Internet of Things ( IoT ), and application logs. There is a need to store, process, and analyze data to get useful insights, where thedata architect role comes into the picture. The data architect defines a set of rules, policies, standards, and models thatgovern the type of data that's used and collected in the or ganization database. They design, create, and manage the data architecture in anor ganization. A data architect develops data models and data lake designs to capture business's key performance indicators ( KPIs ) and enable data transformation. They ensure consistent data performance and data qualityacross the or ganization. The primary customers for a data architect are as follows: Business executives using Business Intelligence ( BI ) tools for data visualizationBusiness analysts using a data warehouse to get more data insightData engineers performing data wrangling using Extract, T ransform, and Load ( ETL ) jobs Data scientists for machine learningDevelopment teams for application data management T o fulfill or ganizational needs, the data architect is responsible for the following: Selection of database technologyA relational database schema for application developmentData warehousing for data analysis and BI toolsData lake as the centralized datastoreDatamart designMachine learning toolsData security and encryptionData compliance Y ou will learn more about data architectures in Chapter 13 ,  Data Engineering and Machine Learning . Overall, the data architect needs to be aware of dif ferent database technologies, BI tools, data security , and encryption to make the right selection. Security ar chitect Security should be the top priority for any or ganization. There are multiple instances when lar ge and well-established or ganizations went out of business due to a security breach. Or ganizations not only lose customer trust but also experience legal complications due to security incidents.There are various industry compliance certifications, suchas  Organizational Security ( SOC2 ), Finance Data  ( PCI ), and  HealthCar e data ( HIPP A ), that are in place to ensure or ganization and customer data security , which a company needs to adhere to as per the nature of their application. Looking at the critical nature of security , or ganizations need to research and design the most robust security architecture for their projects, and that'swhere a security architect is necessary . A security architect works closely with all groups and solution architects to make sure security is a highpriority . A security architect's responsibilities include the following: Designing and deploying the implementation of the network andcomputer security in the or ganization. Understanding the company's technology , and information systems, and safeguarding the security of the computers in the or ganization. W orking with a variety of settings, such as securing company networks and websites.Planning vulnerability testing, risk analysis, and security audits.Reviewing and approving the installation of a firewall, VPN, androuter , and scanning the server . T esting final security processes and making sure they work as expected.Providing technical guidance to the security teams.Making sure applications comply with industry standards as required.Making sure data is secure with the required accessibility andencryption. Security architects are expected to understand, design, and guide all aspectsof security related to data, network, infrastructure, and applications with avariety of tools and techniques. Y ou will learn more about security and compliance in Chapter 8 ,  Security Considerations . DevOps ar chitect As a system gets complex, there are more chances of human error , which can lead to additional ef fort being needed, increased cost, and reduced quality . Automation is the best way to avoid failure and improve overall system ef ficiency . Now automation is not an optional choice—if you want to be agile and move faster , automation is a must. Automation can be applied anywhere, whether it is testing and deployingapplications, spinning up infrastructure, and even ensuring security . Automation plays a critical role, and a DevOps architect automateseverything everywhere. DevOps is a combination of practices and tools thatassist in delivering an application at a faster pace. It allows the or ganization to serve its customers better and stay ahead of the competition. In DevOps, model- development team and operationalteamwork together in sync. For a software application, a DevOps architectdefines continuous integration and continuous delivery (CI/CD). In CI,automated builds and test runs happen before the development team mer ges its code changes into a central repository . CD expands upon continuous integration by deploying all code changes to a production environment afterthe build and test stage. The DevOps architect automates infrastructure deployment, known asInfrastructure as a code, which is highly prevalent in the cloud environment.DevOps can utilize tools such as Chef and Puppet for instructed automation or use cloud-native tools if the workload is in a cloudenvironment. Infrastructure automation provides excellent flexibility for thedevelopment team for experimentation and enables the operations team tocreate replica environments. For smooth operation, a DevOps architect plans monitoring and alertingwith automated communication in the event of issues or any significantchanges. Any security incidents, deployment failures, or infrastructure failures can be monitored automatically , and alerts can be sent via mobile or email to the respective team when required. The DevOps architect also plans for disaster recovery dif ferent deployment methods. Or ganizational  Recovery Point Objective ( RPO ) is the volume of data loss that an or ganization can tolerate. Recovery T ime Object ( R T O ) suggests how much time the application can take to recover and start functioning again. Y ou will learn more about DevOps in Chapter 12 , DevOps and Solution Ar chitectur e Framework . Understanding a solutionar chitect's r esponsibilities In the previous section, you learned about the role of a solution architect,the dif ferent types of architect in an or ganization, and how they coexist. In this section, you will learn more details about the responsibilities of asolution architect. A solution architect is a technical leader , and a customer - facing role, which comes with many responsibilities. The primaryresponsibility of a solution architect is to convert or ganization business visions into a technical solution and work as a liaison between businessesand technical stakeholders. A solution architect uses broad technologyexpertise and business experience to ensure the success of the solution'sdelivery . A solution architect's responsibilities may dif fer slightly based on the nature of the or ganization. Often, in a consulting or ganization, a solution architect may be dedicated to a particular project and customer , while in a product- based or ganization, a solution architect may be working with multiple customers to educate them on a product and review their solution design.Overall, a solution architect holds the following primary responsibilities: S o l u t i o n a r c h i t e c t ' s r e s p o n s i b i l i t y m o d e l As shown in the preceding diagram, there are various significantresponsibilities for a solution architect. In upcoming sections, you will learnabout the various aspects of the solution architect's responsibilities. Analyzing user r equir ements Business requirements are at the center of any solution design and they aredefined in raw terms when a project starts. It is necessary to engage adiverse set of groups from the beginning, which includes the technicalcapability to identify requirements. The business stakeholder definesrequirements, and it warrants multiple adjustments when it comes totechnological evolution. T o save ef fort, it is necessary  to engage solution architects while defining the user requirement document. The solution architect designs the application, which may impact the overallbusiness outcome. This makes requirement analysis a critical skill that asolution architect should possess. A good solution architect needs to havethe skills of a business analyst and the ability to work with a diverse set ofstakeholders. Solution architect bring a broad range of business experience with them.They are not only technical experts but also have a good knowledge of thebusiness domain. They work closely with the product manager and otherbusiness stakeholders to understand all the aspects of requirements. A goodsolution architect helps the product team uncover hidden requirements,which a non-technical stakeholder may not have thought about from anoverall solution perspective. Defining non-functionalr equir ements Non-functional r equir ements  ( NFR ) may not be visible to users and customers directly , but their absence may impact the overall user experience in a negative way and hamper the business. NFR includes critical aspects ofthe system, such as performance, latency , scalability , high availability , and disaster recovery . The most common non-functional requirements are shown in the following diagram: N F R s i n a s o l u t i o n d e s i g n The preceding diagram shows the following NFRs for consideration: Performance : What will be the application load time for users?How can we handle network latency? Security and compliance : How can we secure an application from unauthorized access?How can we protect an application from malicious attacks?How can we meet local laws and audit requirements? Recoverability : How can we recover an application from an outage?How can we minimize recovery time in the event of an outage?How can we recover data loss? Maintainability : How can we ensure application monitoring and alerts?How can we ensure application support? Reliability : How can we make sure the application performs consistently?How can we inspect and correct glitches? A vailability : How can we ensure the high availability of an application?How can we make an application fault-tolerant? Scalability : How can we meet the increasing demand for resources?How can we achieve a good scale for a sudden spike in utilization? Usability : How can we simplify an application's use?How can we achieve a seamless user experience? However , depending on the nature of the project, there may be an NFR that is suitable for that project only (for example, voice clarity for a call centersolution). Y ou will learn more about these attributes in Chapter 3 , Attributes of the Solution Ar chitectur e . The solution architect becomes engaged in a project from a very early stage,which means they need to design a solution by gauging requirements acrossgroups in an or ganization. The solution architect needs to ensure consistency in solution design across system components and requirements. The solutionarchitect is responsible for defining NFR across groups and dif ferent components since they make sure that the desired usability of a solution isachieved across the board. NFRs are an integral and essential aspect of solution design, which tends toslip when teams are too focused on business requirements, and this canimpact the user experience. A good solution architect has the primaryresponsibility of conveying the importance of NFR and making sure they areimplemented as part of solution delivery . Engaging and working withstakeholders Stakeholders could be anyone who has a direct or indirect interest in theproject. As well as the customer and user , it may also be the development team, sales, marketing, infrastructure, network, support team, or the  project funding group . Stakeholders could be internal or external to the project. Internalstakeholders include the project team, sponsors, employees, and seniormanagement. External stakeholders include customers, suppliers, vendors,partners, shareholders, auditors, and the government. Often, stakeholders have a dif ferent understanding of the same business problem as per their context; for example, a developer may look at abusiness requirement from a coding perspective, while an auditor may lookat it from a compliance and security perspective. A solution architect needsto work with all technical and non-technical stakeholders. They possess excellent communication skills and negotiation techniqueswhich help to find out the optimal path for a solution while keepingeveryone on board. A solution architect works as a liaison betweentechnical and non-technical resources and fills the communication gap.Often, those communication gaps between a businessperson and thetechnical team become a reason for failure. The businessperson tries to lookat things from more of a feature and functionality perspective, while thedevelopment team strives to build a more technically compatible solution,which may sometimes lean toward the non-functional side of the project. The solution architect needs to make sure both teams are on the same pageand that the suggested features are also technically compatible. Theymentor and guide the technical team as required and put their perspectiveinto a simple language that everyone can understand. Handling various ar chitectur e constraints Architecture constraints are one of the most challenging attributes ofsolution design. A solution architect needs to manage architecturalconstraints carefully and be able to negotiate between them to find anoptimal solution. Often, these constraints depend on each other , and emphasizing on one limitation can inflate others. The most common constraints are as following:   A r c h i t e c t u r a l c o n s t r a i n t s i n a s o l u t i o n d e s i g n As shown in the preceding diagram, solution design helps us understand thefollowing attributes of an application: Cost :How much funding is available for solution implementation?Expected Return on Investment ( ROI )? Quality : How closely should  outcomes match functional and non- functional requirements?How can we ensure and track the quality of the solution? T ime : When should the output be delivered?Is there any flexibility regarding time? Scope : What is the exact expectation?How does the requirement gap need to be handled andaccommodated? T echnology : What technology can be utilized?What flexibility does using legacy versus new technologiesprovide?Should we build in-house or source from a vendor? Risk :What can go wrong and how can we mitigate that?What is the risk tolerance of stakeholders? Resour ce : What is required to complete solution delivery?Who will work on the solution's implementation? Compliance : What are the local law requirements that can impact the solution? What are the audit and certification requirements? There could be more specific constraints related to a project, such as storingdata in a country due to government regulation and opting for in-housedevelopment due to security concerns. Handling constraints could be verytricky . Saving costs by reducing resources may impact the delivery timeline. Achieving a schedule with limited resources may affect quality , which in turn increases cost due to unwanted bug fixes. So, finding the balancebetween cost, quality , time, and scope is significant. Scope cr eep is one of the most challenging situations as it can negatively impact all otherconstraints and increase risks of solution delivery . It is essential for a solution architect to understand all the aspects of everyconstraint and to be able to identify any resulting risk. They must put riskmitigation plans into place and find a balance between them. Handling anyscope creep can help a lot in delivering the project on time. Making technology selections T echnology selection is the key aspect and complexity of the solution architect's role. There is a broad range of technologies available, and asolution architect is required to identify the right ones for the solution. Thesolution architect needs to have a breadth and depth of technologies tomake the right decision since the chosen technology stack can impact theoverall delivery of the product. Each problem can have multiple solutions and an available range oftechnologies. T o make the right selection, a solution architect needs to keep functional requirements and NFRs in mind and define selection criteriawhile creating a technology decision. The selected technology needs toconsider dif ferent perspectives, whether the goal is the ability to integrate with other frameworks and APIs or to meeting performance requirementsand security needs. A solution architect should be able to choose the technology that not onlysatisfies current requirements but also scaling for future needs. Developing a pr oof of concept and a pr ototype Creating a prototype is probably the most fun part of being a solutionarchitect. T o choose a proven technology , a solution architect needs to develop  a pr oof of concept  ( POC ) in various technology stacks to analyze their fit for functional and non-functional requirements for the solution. The idea of developing POC is to evaluate technology with a subset ofcritical functional implementations, which can help us to decide ontechnology stack based on their capabilities. It has a short life cycle and islimited to being reviewed by experts within a team or or ganization. The solution design POC is when a solution architect is trying to figure out thebuilding blocks of the solution. After evaluating multiple platforms using POC, the solution architect mayproceed with prototyping to a technology stack. A prototype is developedfor demonstration purposes and given to the customer so that it can be usedto secure funding. POCs and prototyping are by no means production-ready; solution architect builds have limited functionality , which can prove a challenging aspect of solution development. Designing solutions and stayingthr ough delivery Solution architects work on solution design after understanding dif ferent aspects of functional requirements, NFRs, solution constraints, andtechnology selection. In an agile environment, this is an iterative approachwhere the requirements may change over time and need to accommodate thesolution design. The solution architect needs to design a future-proof solution, which shouldhave strong building blocks and be flexible enough to adjust to changes.However , the solution architect needs to be careful about drastic changes to the requirements and apply a risk mitigation plan. For future-proof design, you can take the example of a loosely coupledmicroservice architecture based on RESTful APIs. These architectures canbe extendable to new requirements and have the ability to integrate easily . Y ou will learn more about dif ferent architecture designs in Chapter 6 ,  Solution Ar chitectur e Design Patterns . The following flow-chart shows the solution delivery life cycle. The solutionarchitect is involved in all the phases of solution design and delivery: S o l u t i o n d e l i v e r y l i f e c y c l e As shown in the preceding diagram, the solution delivery life cycle includesthe following: Business Requir ement and V isions : A solution architect works with business stakeholders to understand their vision.Requir ement Analysis and T echnical V ision : A solution architect analyzes the requirements and defines a technical vision in order toexecute the business strategy . Pr ototyping and Recommendation : A solution architect makes a technology selection by developing POC and showcase prototypes.Solution Design : A solution architect develops solution designs in line with an or ganization's standards and in collaboration with other impacted groups.Development : A solution architect works with the development team on solution development and works as a bridge between the businessand technical team.Integration and T esting : A solution architect makes sure that the final solution is working as expected with all functional and non-functionalrequirements.Implementation : A solution architect works with the development and deployment team for smooth implementation and guides them through any hindrances.Operation and Maintenance : A solution architect makes sure logging and monitoring are in place and guides the team on scaling and disasterrecovery as required. However , the overall life cycle is an iterative process. Once the application goes into production and customers start using it, you may discover morerequirements from customer feedback, which will drive the product visionfor future enhancements. The solution architect has major ownership during solution design in whichthey do the following: Document solution standardsDefine high-level designDefine cross-system integrationDefine dif ferent solution phases Define an implementation approachDefine a monitoring and alert approachDocument the pros and cons of design choicesDocument audit and compliance requirement Solution architects are not only responsible for solution design. They alsohelp project managers with resource and cost estimation, defining theproject's timeline and milestones, the project's release, and its support plan.The solution architect works through dif ferent phases of the solution life cycle, from design to delivery and launch. The solution architect helps thedevelopment team overcome obstacles and hurdles by providing expertiseand a broad understanding. Ensuring post-launch operabilityand maintenance The solution architect plays an integral role after the solution's launch inrespect of product operability . T o handle the increasing user base and product utilization, a solution architect should know how to scale theproduct to meet demands and ensure high availability without impacting theuser experience. In unforeseen events such as outages, a solution architecture guides you toexecute a disaster recovery plan for business process continuation. Thesolution architect satisfies or ganization R ecovery Point Objectives  ( RPO ) and R ecovery Point Objectives  ( R T O ). RPO is how much data loss an or ganization can tolerate in terms of the volume of data which is lost during the outage interval— for example, a loss of 15 minutes of data. R T O is how much time the system takes to get back up and running again. Y ou will learn more about R T O and RPO in Chapter 12 ,  DevOps and Solution Ar chitectur e Framework . In the event of performance issues due to an increase in demand, thesolution architect helps scale the system horizontally to mitigate applicationbottlenecks or vertically to alleviate database bottlenecks. Y ou will learn more about dif ferent scaling mechanisms and self-healing in Chapter 9 , Ar chitectural Reliability Considerations . The solution architect plans to accommodate any new requirements in anexisting product that arise from usage patterns, or due to any other reason.They can make changes to non-functional requirements based onmonitoring user behavior; for example, users bounce of f the page if it takes more than 3 seconds to load. The solution architect works through this andguides the team in handling issues that may occur post-release. W orking as a technology evangelist An evangelist is the most exciting part of the solution architect role wherethey work as a technology evangelist. The solution architect increasesproduct and platform adoption by spreading the word through publicforums. They write blogs about solution implementation and conductworkshops to showcase potential benefits and the use of technologyplatforms. They build mass support for technologies and help establish a standard. Asolution architect should be passionate about technology . They should be an excellent public speaker and possess excellent writing skills to perform thetechnology evangelist role. Solution ar chitects in an agile organization In the last half decade, you may have seen the rapid adoption of the Agilemethodology . In this competitive market, an or ganization needs to be proactive toward rapid changes and bring output to the customer a very fast.Fast innovation and release can only be possible if or ganizations are adapting quickly and respond to change faster , which means there must be flexibility built into every part of the or ganization and solution architecture. T o be successful in an agile environment, a solution architect needs an agile mindset and must adopt the rapid delivery method by continuously workingwith stakeholders to fulfill their needs. First, let's understand a little bitmore about the Agile methodology . This is a vast topic, and in this section, we will take a high-level overview of it. Why Agile methodology? Agile can create and respond to changes to make a profit in a fast-movingbusiness environment. Its agility comes from balancing flexibility andstability . In today's competitive environment, where technology is moving fast (which results in a high probability of changes and customer demand),agile is the answer to coping with the situation and gaining a competitiveedge. Nowadays, all successful or ganizations are customer driven. They take frequent feedback from end users on their products and use that feedback toexpand their user base. Agile helps gather results from the developmentteam to continuously adapt feedback into software releases, and most of thetime everything has a high priority . T o deal with this situation, you need agile. Executive management provides funding and looks for transparency . They demand productive output to increase ROI, and you want to win theirconfidence by showing incremental development of the product. T o create transparency for a project and keep track of its budget and deliverytimeline, you need agile. When you continuously want to engage your stakeholders by showing thema demonstration of the product, and when development and testing are partof the same cycle, you need Agile methodology . In the preceding scenarios, you looked at various situations where the Agilemethodology is required to keep the or ganization ahead with robust delivery and customer feedback. Agile is able to quickly move  in a time box manner , which means you time box activities in a short cycle and take an iterative approach for productdevelopment instead of working on the entire product to develop anddeliver it at once. The agile method advocates seeking continuous feedback by keeping customers and stakeholders engaged closely , involving them in every phase of product development, adapting feedback into requirements,evaluating market trends, and working with them to prioritize thestakeholders. Then, the development team take up the prioritizedrequirements, conduct technical analyses, design, develop, test, and deliver . Everyone works as one team toward one goal and breaks the silo mindset.Agile thinking helps the technical team understand the requirements fromthe customer's perspective and respond to changes quickly and ef ficiently . This is the reason why most companies want to go agile. The Agilemethodology is fast and easy to adopt using many tools that are available onthe market, such as  JIRA , V ersionOne , and  Rally . Y ou may face some initial challenges while developing agile thinking, but the benefits are veryhigh compared to the challenges that every or ganization faces when moving toward adopting the Agile methodology . Agile manifesto Applying any agile method requires a clear understanding of the four valuesstated in the Agile manifesto. Let's understand these manifestos: Individuals and interactions  over pr ocesses and tools : Processes and tools always help complete the project. Project stakeholders, whoare a part of the project, know how to implement the plan and how todeliver the successful result with the help of tools for project delivery . But the primary responsibility for project delivery is the people  and their collaboration.W orking softwar e  over compr ehensive documentation : Documentation is always an essential process for any product'sdevelopment. In the past, many teams only worked to collect andcreate a repository for documents such as high-level design, low-leveldesign, and design change, which later help achieve qualitative andquantitative descriptions of the product. W ith the Agile methodology , you focus on the deliverable. Therefore, according to this manifesto, you need documentation.However , you need to define how much documentation is vital to the continuous delivery of the product. Primarily , the team should focus on delivering software incrementally throughout the product'slife cycle. Customer collaboration  over contract negotiation : Earlier , when or ganizations worked on a fixed bid or time & material projects, the customer always came in the first and the last stages of the softwarelife cycle. They were outsiders who were not involved in productdevelopment; by the time they finally got a chance to see the productafter launch, the market trends had changed, and they lost the market. Agile believes that customers share equal responsibility for theproduct's launch and that they should be involved in every step ofdevelopment. They are part of demonstrating giving feedback based on new market trends or consumer demand. Since the business isnow part of the development cycle, these changes can be attainableby being agile and having continuous customer collaboration. Responding to change  when following a plan : In the current fast- paced market in which customers demand change with new markettrends, businesses keep on changing. It is vital to make sure there's abalance between frequently changing the requirements and agilelywelcoming the changes since sprint cycles vary from 1 to 3 weeks.Responding to change means that if, anything changes in thespecification, the development team will accept the change and showthe deliverable in sprint demonstrations to keep winning theconfidence of the customers. This manifesto helps the team understandthe value of welcoming changes .  The Agile Manifesto is a tool that's used to establish basicguidelines for adopting an agile methodology .  These manifestos are the core of all agile techniques. Let's understand the agile process inmore detail. Agile pr ocess and terminology Let's get familiar with the most common agile terminologies and how theybind together . Here, you will learn about the agile scrum process, which is widely adopted. The agile scrum process has a small sprint cycle of  1 to 3 weeks , depending on the project's stability , but the most common is a 2- week sprint cycle, which you can call a development cycle. These sprints are development cycles where the team will analyze, develop,test, and deliver a working feature. The team takes an iterative approachand creates a working building block of the product as the projectprogresses with each sprint. Each requirement is written as a user story thatkeeps a customer persona in mind, and makes the requirement clearlyvisible. The agile scrum team has varied roles. Let's understand the most commonroles and how the solution architect collaborates with them: Scrum T eam : This consists of the Product Owner , Scrum Master , and development team. Analysts, technical architects, software engineers,software testers, and deployment engineers are part of the developmentteam.Scrum Master : This facilitates all scrum ceremonies (which you will learn in next section), keeps the team motivated, and removesimpediments for the team. The Scrum Master works with the solutionarchitect to remove any technical blockers and get technicalclarification for business requirements.Pr oduct owner : This is a businessperson who is a customer advocate. The product owner understands market trends and can define prioritieswithin the business. The solution architect works with the productowner to understand the business' vision and keep it aligned with thetechnical view . Development team : They do product implementation and are responsible for the project's delivery . They are a cross-functional team that is committed to continuous and incremental delivery . The solution architect needs to work closely with the development team for smoothproduct implementation and delivery .Sprint cer emonies The sprint cycle includes multiple activities that are performed to managedevelopment, which are often called scrum ceremonies. Those scrumceremonies are as follows: Backlog gr ooming : Grooming is a time-box meeting in which the product owner , solution architect, and business connect to discuss backlog stories, prioritize them, and create a consensus for sprintdeliverables.Sprint planning : In sprint planning, the Scrum Master facilitates groomed stories being assigned to the scrum team based on the team'scapacity . Sprint Daily Standup : Daily Standup is a very ef ficient way of collaboration, where all team members meet in one place, and alldiscuss their last day workload, what plans they have for today , and whether they are facing any problems. This meeting is meant to beshort and straightforward and around 15 minutes in length. Standup isthe platform that the solution architect uses to collaborate with thedevelopment team. Sprint demonstration : During demonstrations, all stakeholders gather and review the team's work of what they had done in a sprint. Based onthis, the stakeholder accepts and rejects the stories. The solutionarchitect makes sure that the functional and non-functionalrequirements have been met. During this meeting, teams collectfeedback from the product owners and solution architect and look atwhat changes were made.Sprint r etr ospect : Retrospect is conducted at the end of each sprint cycle and is where the team inspects and adopts best practices. Theteam identifies things that went well and what they should continue topractice, as well as things that they can do better in the next sprint.Sprint Retrospect helps the or ganization apply continuous improvement while working on their delivery . Agile tools and terms   Let's learn about some agile tools that help drive team metrics and project progress: Planning poker : Planning poker is one of the most popular estimation techniques in agile methodology , where the Scrum Master plays poker games to estimate user stories when a sprint starts. During this activity , each user story will be evaluated based on its complexity . T eam members use comparative analysis to give story points for each userstory , which helps the team understand how much ef fort is required to complete the user stories.Burndown chart : A burndown chart is used to monitor sprint progress and help the team understand how much work is pending. The ScrumMaster and the team always follow the burndown chart to make surethere is no risk in the sprint and reuse that information to improve theestimation next time.Pr oduct backlog : The product backlog contains a collection of requirements in the form of user stories and epics. The product ownercontinuously updates the backlog and prioritizes requirements duringsprint grooming. Epic is a high-level requirement, and product ownerswrite a user story to refine them. The development team breaks downthese user stories into a task, which is an executable action item.Sprint board : The sprint board contains a collection of user stories listed for the active sprint. The sprint board provides transparency asanyone can look at the project's progress for that particular sprintcycle. The team refers to the board on a daily standup to determineoverall work progress and remove any obstructions. Definition of Done : This means all user stories should pass the Done criteria that have been set up by the solution architect and productowner in collaboration with stakeholders. Some of these criteria are asfollows: The code must be peer reviewedThe code should be unit tested Enough documentationCode qualityCode writing standardAgile versus waterfall W aterfall is one of the oldest and most traditional software development methodologies that or ganizations used to follow . In this section, you will learn about the dif ference between waterfall and agile and why or ganizations need to move over to agile. W e are not going to look at the details of the waterfall process; instead, we will point out the keydif ferences: Agile methodologies help change the mindset from the traditionalmethod to an agile mindset. The motivation for this is to move from awaterfall method to agile methods in order to achieve maximumbusiness values and win customer confidence. This makes agile anadvocate for customer collaboration at each step and providestransparency . The waterfall method tends to be more project-and document-centric, where customers were involved at the end phase.The waterfall method is more helpful for the project where allrequirements are unambiguous, and the sequence of their deliverablesalso known, which helps remove any unpredictability as requirementsare very straightforward. The Agile methodology is helpful forcompanies that want to keep up with the market trend and haveincreased pressure from the customer . They need early releases for their products and have to be adaptive to changes in the requirements.Agile projects are delivered in a small iterative manner with thehighest quality and to achieve business value. Many agile teams workin parallel across the sprint to provide a shippable solution for theproduct at every end of the sprint cycle. As every sprint has a smalldeliverable and keeps building on top of that, the customercontinuously gets to see the working model of the product. W aterfall has a long cycle and stakeholders get to see the final product at theend, which means there isn't much scope left to accommodate changes. The agile process makes sure the team is progressing toward the goaland that the project will be completed on time by putting checkpointsat every sprint cycle. In traditional waterfall methods, there is no frequent checkpoint that can ensure that the team is on the right pathand verify whether the project will be completed on time, which maycause ambiguity . In the Agile methodology , the customer always collaborates with the product owner and the team. This collaboration makes sure theyobserve and review the small, shippable product. Agile also ensuresthat work is being done and shows progress to the stakeholder . However , in the waterfall method, there is no such customer interaction until the project ends. Agile is the most adaptive methodology since fast-moving technologies andbusinesses are becoming so unpredictable and need high velocity . Agile supports inspecting and adapting cycles, which creates a balance betweendemand and control. Agile ar chitectur e What comes into your mind when you think about the solution architect inan agile model? there are many myths , such as thinking that the solutionarchitecture is a very complex activity , and with agile you will be asked to submit your design right away or in the next sprint cycle. Another myth isthat the agile architecture will not be robust to such architecture design anddevelopment, that testing cannot be possible, and so on. Agile architecture is about designing decoupled and extendable interfaces.A solution architect in an agile environment needs to follow an iterative re-architect concept by inspecting and adapting the approach. It's aboutchoosing the right solution for enterprises, communicating well, takingcontinuous feedback, and modeling in an agile way . The development team needs a solid foundation and the ability to adapt to a changing requirement;they need guidance and mentoring from a solution architecture. The foundation of the agile architecture should be reducing the cost ofchanges, reducing unnecessary requirements by challenging them, and creating a framework to reverse incorrect requirements rapidly . The Agile architect builds prototypes to minimize risk and plans for change byunderstanding them. They design the prototype while balancing the needsof all stakeholders and creating a loosely coupled architecture that caneasily integrate with other modules. Y ou will learn more about various loosely coupled architecture patterns in Chapter 6 , Solution Ar chitectur e Design Patterns . Summary In this chapter , you learned how the solution architect fits into the or ganization and how dif ferent kinds of solution architect roles coexist. There is generalist solution architect role such as enterprise solutionarchitect, solution architect, technical architect, cloud architect, andarchitect evangelist. The generalist solution architect has a broad knowledge of technology andmay develop in-depth expertise in a particular area. The specialist solutionarchitect dives deep in other required areas of the project. The specialistsolution architect possesses in-depth knowledge of their area of expertise,with some of the most common specialist solution architect roles beingnetwork architect, data architect, security architect, infrastructure architect,and DevOps architect. Y ou learned about solution architect responsibilities in great detail. Solution architects wear multiple hats; they work with stakeholders across theor ganization and analyze functional requirements and define non-functional requirements. The solution architect ensures consistency and standardsacross the or ganization, and they provide technology recommendations and solution prototypes. The solution architect handles various projectconstraints such as cost, quality , scope, and resources, and finds a balance between them. The solution architect helps the project manager estimate cost andresources, and define a timeline, and stays throughout the project fromdesign to launch. During the project's implementation, the solution architectmakes sure that the stakeholder's expectations have been met and works asa liaison between the technical and business teams. The solution architectengages in post-launch application monitoring, alerts, security , disaster recovery , and scaling. At the end of this chapter , you learned about the benefits of an agile process. W e took a brief overview of the Agile methodology , roles, tools, terminology , and how agile dif fer from the traditional waterfall method. Y ou learned about the traits of the agile architecture and how solution architects should make their architecture more flexible and agile. In the next chapter , you will learn about the dif ferent attributes of the solution architecture that you should consider while designing a solution.These attributes include architecture security , scalability , availability , reliability , fault tolerance, extensibility , portability , interoperability , operational excellence, performance ef ficiency , cost optimization, and self- healing. Attributes of the SolutionAr chitectur e The solution architecture needs to consider multiple attributes and designapplications. Solution design may have a broad impact across numerousprojects in an or ganization and that demands a careful evaluation of the various properties of the architecture and striking a balance between them. This chapter will provide an overall understanding of each attribute andhow they are related to each other and coexist in solution design. There may be more attributes, depending on the solution's complexity , but in this chapter , you will learn about the common characteristics that can be applied to most aspects of solution design. Y ou can also view them as NFRs (which fulfills an essential aspect of design). It is the responsibility of asolution architect to look at all the attributes and make sure they satisfy thedesired requirements and fulfill customer expectations. In this chapter , we will cover the following topics: Scalability and elasticityHigh availability and resiliencyFault tolerance and redundancyDisaster recovery and business continuityExtensibility and reusabilityUsability and accessibilityPortability and interoperabilityOperational excellence and maintainabilitySecurity and complianceCost optimization and budgets Scalability and elasticity Scalability has always been a primary factor while designing a solution. Ifyou ask any enterprise about their existing and new solutions, most of thetime they like to plan ahead for scalability . Scalability means giving your system the ability to handle growing workloads, and it can apply to multiplelayers, such as the application server , web app, and database. As most applications nowadays are web-based, let's talk about elasticity . This is not only about growing out your system by adding more capabilitiesbut also shrinking it to save cost. Especially with the adoption of the publiccloud, it becomes easy to grow and shrink your workload quickly , and elasticity is replacing the term scalability . T raditionally , there are two modes of scaling: Horizontal scaling:  It is becoming increasingly popular as compute commodity has become exponentially cheaper in the last decade. Inhorizontal scaling, the team adds more instances to handle increasingworkloads: H o r i z o n t a l s c a l i n g For example, as shown in the preceding diagram, let's say yourapplication is capable of handling a thousand r equests per second with two instances . As your user base grows, the application starts receiving 2,000 r equests per second , which means you may want to double your application instance to four to handle the increasedload. V ertical scaling:  This has been around for a long time. It's where the team adds additional compute storage and memory power to the sameinstance to handle increasing workloads. As shown in the followingdiagram, during vertical scaling, you will get a lar ger instance rather than adding more new instances to handle increased workload: V e r t i c a l s c a l i n g The vertical scaling model may not be cost-ef fective. When you purchase hardware with more compute and memory capacity , the cost increases exponentially . Y ou want to avoid vertical scaling after a certain threshold unless it is essential. V ertical scaling is most commonly used to scale relational database servers. However , you need to think about database sharding here. If your server hits thelimits of vertical scaling, a single server cannot grow beyond certainmemory and compute capacity . The capacity dilemma in scaling Most businesses have a peak season when the user is most active, and theapplication has to handle an additional load to meet demands. T ake the classic example of an e-commerce website, selling all kinds of productssuch as cloth, groceries, electronic items, merchandise, and many more.These e-commerce sites have regular traf fic throughout the year but get 10 times more traf fic in the shopping season, for example, Black Friday and Cyber Monday in the US, or Boxing Day in the UK. This pattern creates aninteresting problem for capacity planning, where your workload is going toincrease drastically for just 1 month in the entire year . In the traditional on-premise data center , ordering additional hardware can take between 4 to 6 months before it becomes application-ready , which means a solution architect has to plan for capacity . Excess capacity planning means your IT infrastructure resources will be sitting idle for most of theyear , and less capacity means you are going to compromise user experience during significant sales events, thus impacting the overall businesssignificantly . This means a solution architect needs to plan elastic workloads, which can grow and shrink on demand. The public cloud madethis very easy . Scaling your ar chitectur e Let's continue with the e-commerce website example by considering amodern three-tier architecture. Let's see how we can achieve elasticity at adif ferent layer of the application. Here, we are only tar geting the elasticity and scalability aspects of architecture design. Y ou will learn more about this in Chapter 6 , Solution Ar chitectur e Design Patterns . The following diagram shows a three-tier architecture diagram of the A WS Cloud tech stack: S c a l i n g t h r e e - t i e r a r c h i t e c t u r e  In the preceding architecture diagram, you can see a lot of components,such as the following: V irtual server (Amazon EC2) Database (Amazon RDS)Load balancer (Amazon Elastic Load Balancer)DNS Server (Amazon Route53)CDN Service (Amazon CloudFront)Network boundary (VPC) and Object Store (Amazon S3) As shown in the preceding diagram, there is a fleet of web servers andapplication servers behind the load balancer . In this architecture, the user sends an application request to the load balancer , which routes traf fic to the web server . As user traf fic increases, auto scaling adds more servers in the web and application fleet.  When there is low demand, it removes additional servers gracefully . Here, auto scaling can add or remove servers based on the chosen matrix-like CPU utilization and memory utilization; for example, youcan configure whether CPU utilization goes beyond 60% and add three newservers; if this goes below 30%, you can then remove two existing servers. W e will dive deep into each component throughout this book. In the next section, you will learn about static content scaling. Static content scaling The web layer of the architecture is mostly concerned about displaying andcollecting data and passing it to the application layer for further processing.In the case of an e-commerce website, each product will have multipleimages and maybe videos to show a product's texture and demos, whichmeans the website will have lots of static content with read-heavy workloadsince, most of the time, users will be browsing products. Storing static content in the web server means consuming lots of storagespace, and as product listings grow you have to worry about scalability . The other problem is that static content (such as high-resolution images andvideos) is heavy in size, which may cause significant load latency on theuser's end . The web tier needs to utilize the  Content Distribution Network  ( CDN ) to solve this issue. CDN providers (such as Akamai, Amazon CloudFront, Microsoft AzureCDN, and Google CDN) provide edge locations across the globe wherestatic content can be cached from the web server to available videos andimages near the user's location and reduce latency . T o scale the origin storage, it's recommended to use object storage such as Amazon S3 or an on-premise custom origin, which can grow independentof memory and compute capabilities. These storage solutions can hold staticHTML pages to reduce the load of web servers and enhance userexperience by reducing latency through the CDN network. Server fleet elasticity The application tier collects user requests from the web tier and performsthe heavy lifting of calculating business logic and talking to the database.When user requests increase, the application tier needs to scale to handlethem and shrink back as demands decrease. In such scenarios, users are tiedto the session, where they may be browsing from their mobile andpurchasing from their desktop. Performing horizontal scaling withouthandling user sessions may cause a bad user experience as it will reset theirshopping progress. Here, the first step is to take care of user sessions by decoupling them fromthe application server instance, which means you should considermaintaining the user session in an independent layer such as a NoSQLdatabase. NoSQL databases are key-value pair stor es , where you can store semi-structured data. Once you start storing your user session in NoSQL databases such asAmazon DynamoDB or MongoDB, your instance can scale horizontallywithout impacting the user experience. Y ou can add a load balancer in front of a fleet of application servers, which can distribute the load amonginstances; with the help of auto scaling, you can automate adding orremoving instances on demand. Database scaling Most applications use relational databases to store their transactional data.The main problem with relational databases is that they can scalehorizontally until you plan for other techniques such as sharding andmodifying your application accordingly . This sounds like a lot of work. When it comes to databases, it is better to take preventive care and reducetheir load. Using a mix of storage, such as storing user sessions in separateNoSQL databases and storing static content in an object store, helps toof fload the master database. It's better to keep the master database node only for writing and updating data and use an additional read replica for allread requests. Amazon RDS engine provides up to six read replicas for relationaldatabases, and Oracle plugins can live-sync data between two nodes. Readreplicas may have milliseconds of delay while syncing with the masternode, and you need to plan for that while designing your application. It isrecommended to use a caching engine such as Memcached or Redis tocache frequent queries and thus reduce the load to the master node. If your database starts growing beyond capacity , then you need to redesign and divide the database in the shard by applying partitions. Here, each shardcan grow independently , and the application needs to be determined based on a partition key that the shard request will go to. So, as you can see, scalability is a significant factor while designing asolution architecture, and it can impact the overall project budget and userexperience significantly if it's not planned properly . A solution architect always needs to think in terms of elasticity while designing applications andoptimizing workloads for the best performance and cost. Statistical Analysis System (SAS ) need to evaluate dif ferent options such as CDNs for static content scaling, load balancing, and autoscaling options for server scaling and various data storage options for caching, objectstores, NoSQL stores, read replicas, and sharding. In this section, you learned about various methods of scaling and how toinject elasticity into the dif ferent layers of your architecture. Scalability is an essential factor to ensure you have high application availability andmake your application resilient. W e'll learn more about high availability and resiliency in the next section. High availability and r esiliency The one thing an or ganization doesn't want to see is downtime . Application downtime can cause a loss of business and user trust, which makes highavailability one of the primary factors while designing the solutionarchitecture. The requirement of application uptime varies from applicationto application. If you have an external-facing application with a lar ge user base such as an e-commerce website or social media, then 100% uptime becomes critical. Inthe case of an internal application (accessed by an employee such as an HRsystem or internal company), a blog can tolerate some downtime. Achievinghigh availability is directly associated with cost, so a solution architectalways needs to plan for high availability , as per the application requirements, to avoid over -architecting. T o achieve a  high availability ( HA ) architecture, it's better to plan workloads in the isolated physical location of the data center so that if anoutage happens in one place, then your application replica can operate fromanother location. As shown in the following architecture diagram, you have a web andapplication server fleet available in two separate availability zones (which isthe dif ferent physical location of the data center). The load balancer helps distribute the workload between two availability zones in case A vailability Zone 1 goes down due to power or network outage. A vailability Zone 2 can handle user traf fic, and your application will be up and running. In the case of the database, you have a standby instance in A vailability Zone 2 , which will failover and become the primary instance in the event of an issue in A vailability Zone 1 . Both the master and standby instances continuously sync data: H i g h a v a i l a b i l i t y a n d r e s i l i e n c e a r c h i t e c t u r e The other factor is the architecture's resiliency . When your application is in trouble and you are facing an intermittent issue then apply the principle ofself-healing, this means your application should be able to recover itselfwithout human intervention. For your architecture, resiliency can be achieved by monitoring theworkload and taking proactive action. As shown in the precedingarchitecture, the load balancer will be monitoring the health of instances. Ifany instance stops receiving the request, the load balancer can take out thebad instances from the server fleet and tell autoscaling to spin up a newserver as a replacement. The other proactive approach to monitor the healthof all instances (such as CPU and memory utilization and spinning up newinstances as soon as a working instance starts to reach a threshold limit) suchas CPU utilization is higher than 70% or that memory utilization is morethan 80%. The attributes of high availability and resiliency can help in terms of cost byachieving elasticity , for example, if server utilization is low , you can take out some servers and save costs. The HA architecture goes hand in hand withself-healing, where you can make sure that your application is up andrunning, but; you also need to have a quick recovery to maintain the desireduser experience. Fault-tolerance and r edundancy In the previous section, you learned that fault tolerance and high availabilityhave a close relationship with each other . High availability means your application is available for the user , but maybe with degraded performance. Suppose you need four servers to handle a user's traf fic. For this, you put two servers in two dif ferent physically isolated data centers. If there is an outage in one data center , then user traf fic can be served from another data center . But now , you have only two servers, which means you're left with 50% of the original capacity , and users may experience performance issue. In this scenario, your application has 100% high availability but is only50% fault tolerant. Fault tolerance is about handling workload capacity if an outage occurswithout compromising system performance. A full fault-tolerantarchitecture involves high costs due to increased redundancy . It depends on your application's criticality as whether your user base can live withdegraded performance for a period of application recovery: F a u l t - t o l e r a n c e a r c h i t e c t u r e As shown in the preceding diagram, your application needs four servers tohandle the full workload by distributing them into two dif ferent zones. In both scenarios, you are maintaining 100% high availability . T o achieve 100% fault tolerance, you need full redundancy and have to maintain the double count of the server so that the user doesn't encounterany performance issues during the outage of one zone. By keeping the samenumber of servers, we will achieve only 50% fault tolerance. While designing the application architecture, a solution architect needs todetermine the nature of the application's user base, only design for 100%fault tolerance (as required) and of fset any redundancy costs involved. Disaster r ecovery and business continuity In the previous section, you learned about using high availability and faulttolerance to handle application uptime. There may be a situation when theentire region where your data center is located goes down due to massivepower grid outages, earthquakes, or floods, but your global business shouldcontinue running. In such situations, you must have a disaster recovery planwhere you will plan your business continuity by preparing suf ficient IT resources in an entirely dif ferent region, maybe in dif ferent continents or countries. When planning disaster recovery , a solution architect must understand an or ganization's Recovery T ime Objective ( R T O ) and Recovery Point Objective ( RPO ). R T O means how much downtime a business can sustain without any significant impact. RPO indicates how much data loss abusiness can resist. A reduced R T O and RPO means more cost, so it is essential to understand whether the business is mission-critical and needsminimal R T O and RPO. The following architecture diagram shows a multi-site disaster recoveryarchitecture where the primary data center location is in Ireland, Europe, andthe disaster recovery site is in V ir ginia, USA, hosted on A WS public cloud. In this case, your business will continue operating, even if somethinghappens to the entire European region or to the public cloud. The fact thatthe disaster recovery plan is multi-site to achieve minimal R T O and RPO means minimal to no outage and no data loss: H y b r i d m u l t i - s i t e d i s a s t e r r e c o v e r y a r c h i t e c t u r e The following are the most common disaster recovery plans, all of whichyou will learn about in Chapter 12 , DevOps and Solution Ar chitectur e Framework : Backup and Stor e : This plan is the least costly and has maximum R T O and RPO. In this plan, all the server's machine images, and databasesnapshots should be stored in the disaster recovery site. In the event of adisaster , the team will try to restore the disaster site from a backup. Pilot Lite : In this plan, all the server's machine images are stored as a backup, and a small database server is maintained in the disasterrecovery site with continual data sync from the main site. Other criticalservices, such as Active Directory , may be running in small instances. In the event of a disaster , the team will try to bring up the server from the machine image and scale up a database. Pilot Lite is a bit morecostly than the backup and recovery option but has less R T O and RPO than Backup and Store. W arm Standby : In this plan, all the application servers and the database server (running on lower capacity) instances in the disaster recovery site and continue to sync up with the leading site. In the eventof a disaster , the team will try to scale up all the servers and databases. W arm Standby is costlier than the  Pilot Lite  option but has less R T O and RPO.Multi-site : This plan is the most expensive and has a near -zero R T O and RPO. In this plan, a replica of the leading site maintains in adisaster recovery site with equal capacity and actively serves usertraf fic. In the event of a disaster , all traf fic will be routed to an alternate location. Often, or ganizations choose a less costly option for disaster recovery , but it is essential to do regular testing and make sure the failover is working. Theteam should put a routine checkpoint in operational excellence to make surethere's business continuity in the event of disaster recovery . Extensibility and r eusability Businesses evolve as they grow , where applications not only scale to handle an increased user base but also keep adding more features to stay ahead andget a competitive edge. A solution design needs to be extendable andflexible enough to modify an existing feature or add new functionality . T o modularize their application, often or ganizations want to build a platform with a group of features and launch them as separate applications. This isonly possible with reusable design. T o achieve solution extensibility , a solution architect needs to use a loosely coupled architecture wherever possible. At a high level, creating a RESTful-or queue-based architecture can help develop loosely coupledcommunication between dif ferent modules or across applications. Y ou will learn more about the other kinds of architecture in Chapter 6 ,  Solution Ar chitectur e Design Patterns . In this section, we will take a simple example to explain the concept of architecture flexibility . The following architecture diagram shows an API-based architecture in an e-commerce application. Here, you have independent services such as productcatalog, order , payment, and shipping being utilized by an end user application in a pick-and-choose manner . Mobile and browser applications are used by the customer to place an online order . These applications need a product catalog service to browse the product on the web, an order service toplace an order , and a payment service to make a payment. The product catalog and order service, in turn, communicate with theshipping service to send ordered items to the customer's doorstep. On theother hand, brick-and-mortar stores use Point of Sales systems, where acustomer representative scans barcodes, places orders on behalf of thecustomer , and takes payment. Here, no shipping service is required as the customer picks the item up in-store: E x t e n s i b l e A P I - b a s e d a r c h i t e c t u r e In the preceding diagram, you can see the Reward API, which is used forthird-party API integration. This architecture allows you to extend thecurrent design to integrate the Reward API for customer retention and toattract new customers by providing benefits when they purchase an item. Here, you can see how payment services are reutilized by both online andstore ordering. Another service can reuse this if the or ganization wants to take payments for gift card service, food services, and so on. Extensibility and reusability are not limited to the service design level. Itgoes deep into the actual API framework level, where software architectsshould use  object-oriented analysis and design  ( OOAD ) concepts such as inheritance and containership to create an API framework. This can beextended and reutilized to add more features to the same service. Usability and accessibility Y ou want your users to have a seamless experience when browsing through the application. It should be so smooth that even the users don't notice thatthey can find things without any dif ficulties. Y ou can do this by making your application highly usable. User research and testing are an essentialaspect when it comes to defining usability that can satisfy user experience. Usability is how quickly the user can learn navigation logic when usingyour application for the first time. It's about how quickly they can bounceback if they make a mistake and are able to perform the task ef ficiently . Complex and feature-rich applications have no meaning if they can't beused ef fectively . Often, when you are designing your application, you want to tar get a global audience or significant geographic region. Y our user base should be diverse in terms of technical amenities and physical abilities. Y ou want your application to be accessible to everyone, regardless of whether a user has aslow internet-connection or old devices, or they have physical limitations. Accessibility is about inclusion and making your application usable byeveryone. While designing an application, a solution architect needs tomake sure it can be accessed over a slow internet and is compatible with adiverse set of devices. Sometimes, they may have to create a dif ferent version of the application altogether to achieve that. Accessibility design should include design components such as voicerecognition and voice-based navigation, screen magnifiers, and readingcontent aloud. Localization helps the application become available in alanguage that's specific to a region, for example, Spanish, Mandarin,German, Hindi, or Japanese. As shown in the following diagram, customer satisfaction is a component inboth usability and accessibility: C u s t o m e r s a t i s f a c t i o n w i t h u s a b i l i t y a n d a c c e s s i b i l i t y Y ou must know your users to achieve usability and accessibility , where accessibility is a component of usability , and they both go hand in hand. Before starting the solution design process, a solution architect should workalongside a product owner to research users by conducting interviews, andsurveys, and gathering feedback on the mock frontend design. Y ou need to understand the user's limitations and empower them with supportingfeatures during application development.When the product is launched, the team should plan for A/B testing byrouting a small portion of user traf fic to new features and understanding user reactions. After launch, the application must have a mechanism tocollect continuous feedback (by providing a feedback form or by launchingcustomer support) to make the design better . Portability and inter operability Interoperability is about the ability of one application to work with othersthrough a standard format or protocol. Often, an application needs tocommunicate with the various upstream systems to consume data anddownstream systems to supply data, so it is essential to establish thatcommunication seamlessly . For example, an e-commerce application needs to work with otherapplications in the supply chain management ecosystem. This includesenterprise resource planning applications to keep a record of alltransactions, transportation life cycle management, shipping companies,order management, warehouse management, and labor management, and soon. All applications should be able to exchange data seamlessly to achieve anend-to-end feature from customer order to delivery . Y ou will encounter similar use cases everywhere, whether it is a healthcare application,manufacturing application, or telecom application. A solution architect needs to consider application interoperability duringdesign by identifying and working with various system dependencies. Aninteroperable application saves lots of costs as it depends on systems thatcan communicate in the same format without any data messaging ef fort. Each industry has its standard size for data exchange that it  needs to understand and adhere to. In general, for software design, the architect may choose a popular formatsuch as JSON or XML between dif ferent applications so that they can communicate with each other . In modern RESTful API design and microservice architecture, both formats are supported out of the box. System portability allows your application to work across dif ferent environments without the need for any changes or with minimal changes. Any software application must work across various operating systems andhardware to achieve higher usability . Since technology changes rapidly , you will often see that a new version of a software language, developmentplatform, or operating system is released. T oday , mobile applications are an integral part of any system design, and your mobile apps need to becompatible with major mobile operating systems platforms such as iOS,Android, and W indows. During design, the solution architect needs to choose a technology that canachieve the desired portability of the application; for example, if you areaiming to deploy your application across dif ferent operating systems. Programming languages such as Java may be the better choice as it is oftensupported by all operating systems, and your application will work on adif ferent platform without needing to be ported across. For mobile applications, an architect may choose a JavaScript-based language suchas  React  Native , which can provide cross-platform mobile app development. Interoperability enriches system extensibility , and portability increases the usability of an application. Both are critical attributes of an architecturedesign and may add additional exponential costs if they're not addressedduring solution design. A solution architect needs to carefully consider bothaspects as per industry requirements and system dependencies. Operational excellence andmaintainability Operational excellence can be a great dif ferentiator for your application by providing an on-par service to customers with minimal outage and highquality . It also helps the support and engineering team to increase productivity by applying proactive operational excellence. Maintainabilitygoes hand in hand with operational excellence. Easily maintainableapplications help reduce costs, avoid errors, and let you gain a competitiveedge. A solution architect needs to design for operation, which means the designshould include how the workload will be deployed, updated, and operatedin the long run. It is essential to plan for logging, monitoring, and alerting tocapture all incident and take quick actions for the best user experience.Apply automation wherever possible, whether deploying infrastructures orchanging the application code to avoid human error . Including deployment methods and automation strategy in your design isvery important as this can accelerate the time to market for any newchanges without impacting existing operations. Operation excellenceplanning should consider security and compliance elements as regulatoryrequirements may change over time and your application has to adhere tothem to operate. Maintenance can be proactive or reactive. For example, once a new versionof an operating system becomes available in the market, you can modernizeyour application to switch platforms immediately or monitor system healthand wait until the end of the life of the software before making any changes.In any case, changes should be in small increments with a rollback strategy . T o apply these changes, you can automate the entire process by setting up the  continuous integration and continuous deployment ( CI / CD ) pipeline. For the launch, you can plan for A/B deployment or blue-green deployment. For operational readiness, architecture design should include appropriatedocuments and knowledge-sharing mechanisms— for example, creatingand maintaining a runbook to document routine activity and creating aplaybook that can guide your system process through issues. This allowsyou to act quickly in the event of an incident. Y ou should use  r oot cause analysis for post incidence to determine why the issue occurred and make sure it doesn't happen again. Operational excellence and maintenance are an ongoing ef fort; every operational event and failure is an opportunity to learn and help youimprove your operation by learning from previous mistakes. Y ou must analyze the operation's activities and failures, do more experimenting, andmake improvements. Y ou will learn more about performing excellent consideration in solution design in Chapter 10 , Operational Excellence Considerations . Security and compliance Security is one of the most essential attributes of solution design. Manyor ganizations fail due to security breaches, which results in a loss in customer trust and an unrecoverable business loss. Industry-standardregulations such as PCI for finance, HIPP A for health care, GDPR for European Union, and SOC compliance enforce security to protect consumerdata and provide standard guidance to the or ganization. Depending on your industry and region, you must comply with local legislation by adhering tocompliance needs. Primarily , application security needs to be applied in the following aspects of solution design: Authentication and AuthorizationW eb security Network securityInfrastructure securityData security These can be seen in the following diagram: S e c u r i t y a s p e c t s i n s o l u t i o n d e s i g n Let's take a look at the dif ferent security aspects. Y ou will dive deep into each component in Chapter 8 , Security Considerations . Authentication and authorization Authentication means specifying who can access the system and authorization  is applied to activities that a user can perform after getting inside the system or application. Solution architects must consider theappropriate authentication and authorization system while creating asolution design. Always start with the least privileged and provide furtheraccess as required by the user role. If your application is for corporate internal use, you may want to allowaccess through a federated or ganizational system such as Active Directory , SAML 2.0, or LDAP . If your application is tar geting mass user bases such as social media websites or gaming apps, you can allow them toauthenticate through OAuth 2.0 and OpenID access, where users can utilizetheir other IDs such as Facebook, Google, Amazon, and T witter . It is important to identify any unauthorized access and takeimmediate action to mitigate security threats, which warrants continuouslymonitoring and auditing the access management system. Y ou will learn about application security in  Chapter 8 , Security Considerations .  W eb security A web application is often exposed to the internet and is more vulnerable toexternal attacks. Solution design must consider preventing attacks suchas  cr oss-site scripting ( XSS ) and SQL injection . These days, the  Distributed Denial of Service ( DDoS ) attack is causing trouble for or ganizations. T o prevent this, the appropriate tools are required, and an incident response plan needs to be put in place. Solution architects should plan to use a W eb Application Fir ewall ( W AF ) to block malware and SQL injection attacks. W AF can be used to prevent traf fic from a country where you don't have a user base or to block malicious IP addresses. W AF , in combination with a  Content Distribution Network ( CDN ), can help to prevent and handle DDoS attacks. Network security Network security helps prevent overall IT resources inside an or ganization and application being open to external users. Solution design must plan tosecure the network, which can help prevent unauthorized system access,host vulnerabilities, and port scanning. Solution architects should plan for minimal system exposure by keepingeverything behind a corporate firewall and avoiding internet accesswherever possible. For example, the web server shouldn't be exposed to theinternet instead; only the load balancer should be able to talk to the internet.For network security , plan to utilize an  Intrusion Detection System ( IDS ) and an  Intrusion Pr evention System ( IPS ) and put them in front of network traf fic. Infrastructur e security If you are maintaining your own data center , then the physical security of the infrastructure is very important if you wish to block physical access toyour server on the part of any unauthorized user . However , if you are leasing the data center or using a private cloud, then this can be handled bya third-party vendor . Logical access to the server must be secured by network security , which is done by configuring the appropriate firewall. Data security This is one of the most critical components that need to be secured. Afterall, you are putting layers of security at the access, web, application, andnetwork layers to secure your data. Data can be exchanged between twosystems, so it need to be secure in transit, or it may be sitting in a databaseor some storage where data needs to be secure at rest. Solution design needs to plan data-in-transit security with Secur e Socket Layer/T ransport Layer Security ( SSL/TLS ) and security certification. Data at rest should be secured using various encryption mechanisms, whichmay be symmetric or asymmetric. The design should also plan to secure theencryption key with the right key management approach, as per applicationrequirements. Key management can be achieved using a hardware securitymodule or services provided by cloud vendors. While ensuring security , it is essential to have a mechanism to identify any security breach as soon as it occurs and respond to it. Adding automation toevery layer to monitor , and get an immediate alert for , any violation must be part of the solution design. DevSecOps is becoming a trend in mostor ganizations since it applies best practices to automating security needs and security responses during the software development life cycle. Y ou will learn more about DevSecOps in  Chapter 12 , DevOps and Solution Ar chitectur e Framework . T o adhere to compliance with the local legislation, solution design needs to include an audit mechanism. For finance, regulatory compliance such asPayment Card Industry Data Security Standard ( PCI DSS ) is strictly required to gain the log trails of every transaction in the system, whichmeans all activity needs to be logged and sent to the auditor when required.Any Personal Identifiable Information ( PII ) data, such as customer email IDs, phone numbers, and credit card numbers needs to be secured byapplying encryption and limited access for any application storing PII data. In on-premise environments, it is the customer's responsibility to secure theinfrastructure and application and also to get certification for compliance.However , in the public cloud, environments such as A WS ease this burden since infrastructure security and compliance are taken care by a cloudvendor . The customer shares  responsibility for  the security of the application and make sure it's compliant by completing the required audit. Cost optimization and budget Every solution is limited by budget and investors look for maximal ROI.The solution architect needs to consider cost-saving during architecturedesign. Cost should be optimized from pilot creation to solutionimplementation and launch. Cost optimization is a continuous ef fort and should be continuous process. Like any other constraint, cost-saving comeswith a trade-of f; it should make a point of determining whether other components such as the speed of delivery and performance are morecritical. Often, cost increases due to over -provision resources and overlooks the cost of procurement. The solution architect needs to plan optimal resources toavoid excessive underutilization. At the or ganization level, there should be an automated mechanism to detect ghost resources, which team membersmay create dev and test environments, and it may no longer be in use aftercompletion of the implementation task. Often, those ghost resources gounnoticed and cause costs to overrun. Or ganizations need to keep a record of their inventory by applying automated discovery . During technology selection, it's essential to evaluate build versus sour ce cost . Sometimes, it's better to use a third-party tool when your or ganization doesn't have the expertise and the cost of the build will be high, forexample, by sourcing log analysis and business intelligence tools. Also, weneed to determine the ease of learning and the complexity ofimplementation when selecting a technology for solution implementation.From an IT infrastructure perspective, we need to evaluate capitalexpenditure versus operation expenditures as maintaining a data centerrequires high capital investment upfront to meet unforeseen scalingdemands. Since multiple choices are available, solution architects can selectoptions from the following— public cloud, private cloud, and multi-cloud.Alternatively they can take a hybrid approach.  Like all components, cost needs to be automated and alerts need to be set upagainst budget consumption. Cost needs to be planned and divided betweenthe or ganizational unit and the workload so that responsibilities can be shared with all groups. The team needs to continuously look at costoptimization by optimizing operation support and workload as morehistorical data is collected. Summary In this chapter , you learned about various solution architecture attributes that need to be considered while creating a solution design. Y ou learned about two modes of scalability , vertical and horizontal, and how to scale various layers of the architecture, including the web layer , application servers, and databases. Y ou also learned how to apply elasticity in your workload using autoscaling so that it can grow and shrink on demand. This chapter provided insightsinto designing a resilient architecture and the methods to achieve highavailability . Furthermore, this helped you understand fault tolerance and redundancy so that you can make your application performant, as per youruser's expectations, and plan for disaster recovery for the continuation ofyour business in the case of any unforeseen events. Then, you learned about the importance of making your architectureextendable and accessible and how architecture portability andinteroperability help reduce costs and increase the adoption of yourapplication. This chapter ended with how to apply operational excellence,security , and cost to every layer of your architecture and how those attributes should be considered right from the beginning of the solutiondesign process.  Y ou will look at each component in more detail later in this book. In the next chapter , you will learn about the principle of solution architecture design, which will focus on how to design the solutionarchitecture while bearing in mind various attributes that were explained inthis chapter . Principles of Solution Ar chitectur e Design In the previous chapter , you learned about the attributes of solution architecture. Those attributes are essential properties, which a solutionarchitect needs to keep in mind while creating a solution design. In thischapter , you will learn about the principles of solution architecture design, which incorporate various attributes during solution design. This chapter throws light on the most important and common designprinciples. However , there could be more design aspects based on the nature of the product complexity and industry domain. As you move forward toyour learning path of becoming a solution architect in this book, thesedesign principles and attributes will be further applied in order to createvarious design patterns in Chapter 6 , Solution Ar chitectur e Design Patterns . Y ou will learn about the following topics in this chapter: Scaling workloadBuilding resilient architectureDesign for performanceUsing replaceable resourcesThink loose couplingThink service not serverUsing the right storage for the right needThink data-driven designOvercoming constraintsAdding security everywhereAutomating everything In this chapter , you will not only learn about designing scalable, resilient, and performance architecture, but you will also learn how to take care ofyour architecture by applying security , overcoming constraints, and applying changes along with the test and automation approach. Theseprinciples will help you to apply thinking in the right way , by using service- oriented ar chitectur e  ( SOA ) and a data-driven approach.Scaling workload In the Scalability and elasticity section of Chapter 3 , Attributes of the Solution Ar chitectur e , you learned about dif ferent modes of scaling and how to scale static content, a server fleet, and a database at a high level.Now , let's look at various types of scaling that can be used to handle workload spikes. Scaling could be predictive if you are aware of your workload, which isoften the case; or it could be reactive if you get a sudden spike or if youhave never handled that kind of load before. Regardless of scaling beingreactive or predictive, you need to monitor the application and collect thedata in order to plan your scaling needs accordingly . Let's dive deep into these patterns with examples. Pr edictive scaling Pr edictive scaling is the best-case approach that any or ganization wants to take. Often, you can collect historical data of application workload, forexample, an e-commerce website such as Amazon may have a sudden traf fic spike, and you need predictive scaling to avoid any latency issues. T raf fic patterns may include the following: W eekends have three times more traf fic than a weekday . Daytime has five times more traf fic than at night. Shopping seasons, such as Thanksgiving or Boxing Day , have 20 times more traf fic than regular days. Overall, the holiday season in November and December has 8 to 10times more traf fic than during other months. Y ou may have collected the previous data based on monitoring tools that are in place to intercept the user's traf fic, and based on this, you can make a prediction for scaling. Scaling may include planning to add more serverswhen workload increases, or to add additional caching. This example of ane-commerce workload is one that tends toward higher complexity andprovides lots of data points to help us to understand overall design issues.For such complex workloads, predictive scaling becomes more relevant. Predictive auto-scaling is becoming very popular , where historical data and trends can be fed to prediction algorithms, and you can predict in advancehow much workload is expected at a given time. Using this expected data,you can set up the configuration to scale your application. T o better understand predictive auto-scaling, look at the following metrics dashboard from the A WS predictive auto-scaling feature. This graph has captured historical CPU utilization data of the server , and based on that, has provided the forecasted CPU utilization: P r e d i c t i v e s c a l i n g f o r e c a s t In the following screenshot , an algorithm is suggesting how much minimum capacity you should plan in order to handle the traf fic, based on the forecast: P r e d i c t i v e s c a l i n g c a p a c i t y p l a n Y ou can see that there is a variation in the minimum capacity at dif ferent times of the day . Predictive scaling helps you to best optimize your workload based on predictions.  Predictive auto-scaling helps to reduce latency and avoid an outage, as adding new resources may take some time. If there isa delay in adding additional resources to handle website traf fic spikes, it may cause a request flood and false high traf fic, as users tend to send a repeated request when they encounter slowness or outages. In this section, you learned about predictive auto-scaling, but sometimes,due to a sudden spike in the workload, you need reactive scaling. W e will learn about this in the next section. Reactive scaling W ith the use of a machine learning algorithm, predictive scaling is getting more accurate, but you have to deal with sudden traf fic spikes, and depend upon reactive scaling. This unexpected traf fic that may come could be even 10 times the regular traf fic; this usually happens due to a sudden demand or , for example, due to a first attempt to run sales events, where we're not sureabout the amount of incoming traf fic. Let's take an example where you are launching a flash deal on your e-commerce website. Y ou will have a lar ge amount of traf fic on your home page, and from there, the user will go to the flash deal product-specificpage. Some users may want to buy the product; therefore, they will go tothe add to cart page. In this scenario, each page will have a dif ferent traf fic pattern, and you will need to understand your existing architecture and traf fic patterns, along with an estimate of the desired traf fic. Y ou also need to understand the navigation path of the website. For example, the user has to log in to buy aproduct, which can lead to more traf fic on the login page. In order to plan for the scaling of your server resources for traf fic handling, you need to determine the following patterns: Determine web pages, which are read-only and can be cached.Which user queries need just to read that data, rather than write orupdate anything in the database?Does a user query frequently , requesting the same or repeated data, such as their own user profile? Once you understand these patterns, you can plan to of fload your architecture in order to handle excessive traf fic. T o of fload your web-layer traf fic, you can move static content, such as images and videos, to content distribution networks from your web server . Y ou will learn more about the Cache Distribution pattern in Chapter 6 , Solution Ar chitectur e Design Patterns . At the server fleet level, you need to use a load balancer in order todistribute traf fic, and you need to use auto-scaling to increase or shrink several servers in order to apply horizontal scaling. T o reduce the database load, use the right database for the right need—a NoSQL database to storeuser sessions and review comments, a relational database for thetransaction, and apply caching to store frequent queries. In this section, you learned about the scaling patterns and methods that areused to handle the scaling needs of your application in the form ofpr edictive scaling and r eactive scaling . In Chapter 6 , Solution Ar chitectur e Design Patterns , you will learn about the details of the dif ferent types of design patterns, and how to apply them in order to be able to scale yourarchitecture. Building r esilient ar chitectur e Design for failure, and nothing will fail. Having a resilient architecturemeans that your application should be available for customers while alsorecovering from failure. Making your architecture resilient includes applyingbest practices in every aspect, in order to make your application recoverable.Resiliency needs to be used in all layers of architecture, includinginfrastructure, application, database, security , and networking. From the security perspective, the Distributed Denial of Service ( DDoS ) attack has the potential to impact the availability of services andapplications. The DDoS attack usually puts fake traf fic in your server and makes it busy , therefore, legitimate users are unable to access your application. This can happen at the network layer or the application layer . Y ou will learn more about DDoS attack and mitigation in Chapter 8 , Security Considerations . It's essential to take a proactive approach to prevent DDoS attacks. The firstrule is to keep as much as application workload possible in the privatenetworks and not expose your application endpoints to the internet whereverpossible. T o take early action, it is essential to know your regular traf fic and have a mechanism in place to determine substantial suspicious traf fic at the application and network packet levels. Exposing your application through the content distribution network ( CDN ) will provide the inbuilt capability and adding the W eb Application Fir ewall ( W AF ) rule can help to prevent unwanted traf fic. Scaling should be your last resort but be ready with an auto-scaling mechanism to enable youto scale your server in the case of such an event. T o achie ve resiliency at the application level, the first thing that comes to mind is redundancy , which leads to making your applicatio n highly available by spreading the workload across the geographic location. T o achieve redundancy , you can have a duplicate server fleet at a dif ferent rack in the same data center and in a dif ferent region. If servers are spread across dif ferent physical locations, the first level of traf fic routing can be handled using the Domain Name System   ( DNS ) before it reaches the load balancer: A p p l i c a t i o n a r c h i t e c t u r e r e s i l i e n c y As you can see in the preceding architecture, resiliency needs to be appliedin all the critical layers that af fect the application's availability to implement the design of failure. T o achieve resiliency , the following best practices need to be applied in order to create a redundant environment:  Use the DNS server to route traf fic between dif ferent physical locations so that your application will still be able to run in the case of entire regionfailure. Use the CDN to distribute and cache static content such as videos,images, and static web pages near the user location, so that yourapplication will still be available in case of a DDoS attack or localpoint of pr esence ( PoP ) location failure. Once traf fic reaches a region, use a load balancer to route traf fic to a fleet of servers so that your application should still be able to run evenif one location fails within your region.Use auto-scaling to add or remove servers based on user demand. As aresult, your application should not get impacted by individual serverfailure.Create a standby database to endure the high availability of thedatabase, meaning that your application should be available in theinstance of a database failure. In the preceding architecture, if any components fail, you should have abackup to recover it and achieve architecture resiliency . The load balancer and routers at the DNS server perform a health check to make sure that thetraf fic is routed to only healthy application instances. Y ou can configure this to perform a shallow health check, which monitors local host failures; ordeep health checks, which can also take care of dependency failure.However , a deep health check takes more time and is more resource- intensive than to the shallow health check. At the app lication level, it is essential to avoid   cascading failur e ,  where the failure of one component can bring down the entire system. There aredif ferent mechanisms available to handle cascading, such as applying timeout, traf fic rejection, implementing the  idempotent operation , and using circuit-breaking patterns. Y ou will learn more about these patterns in Chapter 6 , Solution Ar chitectur e Design Patterns . Design for performance W ith the availability of fast internet, customers are seeking high-performant applications with minimal load time. Or ganizations have noticed that a direct revenue impact is proportional to application performance, andslowness in application load time can significantly impact customer engagement. Modern-era companies are setting a high expectation when itcomes to performance, which results in high-performant applicationsbecoming a necessity in order to stay in the market.  Like resiliency , the solution architect needs to consider performance at every layer of architecture design. The team needs to put monitoring inplace to continue to perform ef fectively , and work to improve upon it continuously . Better performance means more user engagements and increases in return on investment—high-performance applications aredesigned to handle application slowness due to external factors such as aslow internet connection. For example, you may have designed your blogweb page to load within 500 milliseconds where there is good internetavailability . However , in cases of a slow internet, you can load text and engage the user while images and videos are still loading. In an ideal environment, as your application workload increases, automatedscaling mechanisms start handling additional requests without impactingupon application performance. But in the real world, your applicationlatency goes down for duration when scaling takes ef fect. In a real-world situation, it's better to test your application for performance by increasingthe load and understand if you can achieve the desired concurrency and userexperience. At the server level, you need to choose the right kind of server dependingupon your workload. For example, choose the right amount of memory andcompute to handle the workload, as memory congestion can slow downapplication performance, and eventually , the server may crash. For storage, it is important to choose the right input/output operations per second  ( IOPS ). For write-intensive applications, you need high IOPS to reduce latency and to increase disk write speed. T o achieve great performance, apply caching at every layer of your architecture design. Caching makes your data locally available to the usersor keeps data in-memory in order to serve an ultra-fast response. Thefollowing are the considerations that are required to add caching to variouslayers of your application design: Use browser cache on the user's system to load frequently requestedweb pages.Use the DNS cache for quick website lookup.Use the CDN cache for high-resolution images and videos that are nearto the user's location.At the server level, maximize the memory cache to serve user requests.Use cache engines such as Redis and Memcached to serve frequentqueries from the caching engine.Use the database cache to serve frequent queries from memory . T ake care of cache expiration and cache eviction at every layer . As you can see, keeping your application performant is one of the essentialdesign aspects and is directly related to or ganization profitability . The solution architect needs to think about performance when creating solutiondesign and should work relentlessly to keep improving the performance ofthe application. Y ou will learn more about dif ferent caching patterns in Chapter 6 , Solution Ar chitectur e Design Patterns . Performance is a critical factor , and in Chapter 7 , Performance Considerations , you will dive deep further to learn techniques to optimize your application for better performance. Using r eplaceable r esour ces Or ganizations make a significant capital investment in hardware, and they develop the practice of updating them with a new version of the applicationand configuration. Over time, this leads to dif ferent servers running in varied configurations, and troubleshooting them becomes a very tedioustask. Sometimes, you have to keep running unnecessary resources whenthey are not needed, as you are not sure which server to shut down. The inability to replace servers makes it challenging to roll out and test any new updates in your server fleet. These problems can be solved by treatingyour server as a replaceable resource, which enables you to move morequickly to accommodate changes such as upgrading applications andunderlying software. That is why , while designing your application, always think of immutable infrastructure. Cr eating immutable infrastructur e Immutable means, during application upgrades, you will not only replace software, but hardware too. Or ganizations make a significant capital investment in hardware and develop the practice of updating them with anew version of the application and configuration. T o create replaceable servers, you need to make your application stateless and avoid the hardcoding of any server IP or database DNS name.Basically , you need to apply the idea of treating your infrastructure as software instead of hardware, and not apply updates to the live system. Y ou should always spin up new server instances from the golden machineimage, which has all necessary security and software in place. Creating immutable infrastructure becomes comfortable with the use of avirtual machine, where you can create a golden image of your virtualmachine and deploy it with the new version, rather than trying to update anexisting version. This deployment strategy is also beneficial fortroubleshooting, where you can dispose the server that has an issue and spinup a new server from a golden image. Y ou should take a backup of logs for root cause analysis before disposing the server with issues. This approachalso ensures consistency across the environment, as you are using the samebaseline server image to create all of your environment. Canary testing Canary testing is one of the popular methods that is used to apply rolling deployment with immutable infrastructure. It helps you to ensure that old-version production servers are replaced safely with new servers, withoutimpacting the end users. In canary testing, you deploy your software updatein a new server and route a small amount of traf fic to it. If everything goes well, you will keep increasing traf fic by adding more new servers, while disposing the old servers. Canary deployment gives you a safe option to push your changes in the live production environment. Ifsomething goes wrong, only small numbers of users are impacted, and youhave the option of immediate recovery by routing traf fic back to old servers. The solution architect needs to think ahead to use replaceable resources fordeployment. They need to plan session management and avoid serverdependency on hardcoded resources ahead of time. Always treat resourcesas replaceable and design your applications to support changes in hardware. The solution architect needs to set a standard to use various rollingdeployment strategies, such as A/B testing or Blue/Green deployment.  T reat your server like cattle, not like a pet; when this principle is applied to thereplacement of problematic IT resources, quick recovery is ensured, and troubleshooting time reduced. Think loose coupling A traditional application builds a tightly integrated server , where each server has a specific responsibility . Applications depend upon other servers for completeness of functionality . As shown in the following diagram, in a tightly coupled application, the web server fleet has a direct dependency onall application servers, and vice versa: T i g h t l y c o u p l e d a r c h i t e c t u r e In the preceding architecture diagram, if one application server goes down,then all web servers will start receiving errors, as the request will route toan unhealthy application server , which may cause a complete system failure. In this case, if you want to scale by adding and removing servers, itrequires lots of work, as all connections need to be set up appropriately . W ith loose coupling, you can add an intermediate layer such as a load balancer or a queue, which automatically handles failures or scaling foryou. In the following architecture diagram, there is a load balancer between the web server and the application server fleet, which makes sure to alwaysserve user requests from a healthy application server: L o a d b a l a n c e r - b a s e d l o o s e l y c o u p l e d a r c h i t e c t u r e If one of the application servers goes down, the load balancer willautomatically start directing all the traf fic to the other three healthy servers. Loosely coupled architecture also helps you to scale your serversindependently and replace unhealthy instances gracefully . It makes your application more fault tolerant as an error radius is limited to a singleinstance only . For queue-based loosely coupled architecture, take an example of an image-processing website, where you need to store an image, and then process itfor encoding, thumbnail, and copyright. The following architecture diagramhas a queue-based decoupling. Y ou can achieve loose coupling of systems by using queues between systems and exchanging messages that transferjobs: Q u e u e - b a s e d l o o s e l y c o u p l e d a r c h i t e c t u r e Queue-based decoupling enables asynchronous linking of systems, whereone server is not waiting for a response from another server and it isworking independently . This method lets you increase the number of virtual servers that receive and process the messages in parallel. If there is noimage to process, you can configure auto-scaling in order to terminate theexcess servers. In a complex system, a loosely coupled architecture is achieved by creatinga service-oriented ar chitectur e ( SOA ), where independent services contain a complete set of functionalities and communicate with each otherover a standard protocol. In modern design, microservice architecture isbecoming highly popular , which facilitates the decoupling of an application component. The loosely coupled design has many benefits, from providingscalability and high availability , to ease of integration.  In the next section, you will learn more about SOA, and you will also divedeep into the details of this topic in Chapter 6 , Solution Ar chitectur e Design Pattern .  Think service not server In the previous section, you learned about loose coupling and how importantit is for our architecture to be loosely coupled for scalability and fault-tolerance. Developing service-oriented thinking will help to achieve aloosely coupled architecture (as opposed to a server -oriented design, which can lead to hardware dependency and a tightly coupled architecture). SOAhelps us to achieve ease of deployment and maintenance for your solutiondesign. When it comes to service-oriented thinking, solution architects always geartowards SOA. The two most popular SOAs are based on Simple Object Access Pr otocol  ( SOAP ) services and RESTful  services. In SOAP-based architecture, you format your message in XML and send it over the internetusing the SOAP protocol, which builds on top of the HTTP . In RESTful architecture, you can format a message in XML, JSON, or plaintext, and send it over a simple HTTP . However , RESTful architecture is comparatively more popular , as it is very lightweight and much more straightforward than SOAP .  When talking about SOA  nowadays, microservice architecture is increasingly popular . Microservices are independently scalable, which makes it easier to expand or shrink one component of your applicationwithout impacting others. As you can see in the following diagram, in amonolithic architecture, all components are built in a single server and tiedup with a single database, which creates a hard dependency , whereas, in a microservice architecture, each component is independent with theirframework and database, which allows them to be scaled independently: M o n o l i t h i c t o t h e m i c r o s e r v i c e a r c h i t e c t u r e In the preceding diagram, you can see an example of an e-commercewebsite, where customers can log in and place an order , assuming the items they want are available, by adding items to the cart. T o convert a monolithic architecture to a microservice-based architecture, you can create applicationsthat are made of small independent components, which constitute smallerparts to iterate. T aking the modularization appr oach means that the cost, size, and risk of change reduces. In the preceding case, each component is created as a service. Here, the Login Service can independently scale to handle more traf fic, as the customer may log in frequently to explore the product catalog and order status, while the Order Service and the Cart Servic e may have less traf fic, as a customer may not place the order very often. Solution architects need to think of SOA while designing a solution. Theclear advantage of services is that you have a smaller surface area of code tomaintain and services are self- contained. Y ou can build them with no external dependencies. All prerequisites are included in the service, whichenables loose coupling, scaling, and reduces the blast radius in   case of failure.Using the right storage for the rightneed For decades, or ganizations have been using a traditional relational database and trying to fit everything there, whether it is key/value-based user sessiondata, unstructured log data, or analytics data for a data warehouse.However , the truth is, the relational database is meant for transaction data, and it doesn't work very well for other data types—it's like using a SwissArmy knife, which has multiple tools that work but to a limited capacity; ifyou want to build a house, then the screwdriver will not be able to performa heavy lift. Similarly , for specific data needs, you should choose the right tool that can do the heavy lifting, and scale without compromisingperformance. Solution architects need to consider multiple factors while choosing thedata storage to match the right technology . Here are the important ones: Durability r equir ement : How should data be stored to prevent data corruption?Data availability : Which data storage system should be available to deliver data?Latency r equir ement : How fast should the data be available? Data thr oughput : What is the data read and write need? Data size : What is the data storage requirement? Data load : How many concurrent users need to be supported? Data integrity : How to maintain the accuracy and consistency of data?Data queries : What will be the nature of queries? In the following table, you can see dif ferent types of data with examples and appropriate storage types to use. T echnology decisions need to be made based on storage type, as shown here: Data T ype DataExample StorageT ype Storage Example T ransactional, structuredschema User orderdata,financialtransaction Relationaldatabase Amazon RDS, Oracle,MySQL, PostgreSQL , MariaDB Microsoft SQL Server Key-valuepair , semi- structured,unstructured User sessiondata,applicationlog, review , comments NoSQL AmazonDynamoDB ,  MongoDB , Apache HBase ,  Apache Cassandra, AzureT ables, Analytics Sales data,Supplychainintelligence,Businessflow Datawarehouse IBM  Netezza,  Amazon Redshift , T eradata,  Greenplum, Google  BigQuery In-memory User homepage data,commondashboard Cache Redis cache,  Amazon ElastiCache , Memcached Object Image,video File-based SAN, Amazon S3,Azure BlobStorage,  Google Storage Block Installablesoftware Block-based NAS, Amazon EBS,Amazon EFS, Azure Disk Storage Streaming IoT sensordata,clickstreamdata T emporary storage forstreamingdata Apache Kafka , Amazon Kinesis, SparkStreaming ,  Apache Flink Archive Any kind ofdata Archivestorage Amazon Glacier , magnetic tape storage,v irtual tape library storage W eb storage Static webcontentssuch asimages,videos,HTMLpages CDN Amazon CloudFront,Akamai CDN, AzureCDN, GoogleCDN,  Cloudflare Search Productsearch, Searchindex store Amazon Elastic Search,Apache Solr , contentsearch and query Apache Lucene Data catalog T able metadata,data aboutdata Meta-datastore A WS Glue, Hive metastore,Informatica  data catalog, Collibra  data catalog Monitoring System log,network log,audit log Monitordashboardand alert Splunk, AmazonCloudW atch, SumoLogic,  Loggly   As you can see in the preceding table, there are various properties of data,such as structured, semi-structured, unstructured, key-value pair , streaming, and so on. Choosing the right storage helps to improve not only theperformance of the application, but also its scalability . For example, you can store user session data in the NoSQL database, which will allowapplication servers to scale horizontally and maintain user sessions at thesame time. While choosing storage options, you need to consider the temperature of thedata, which could be hot, warm, or cold: For hot data, you are looking for sub-millisecond latency and requiredcache data storage. Some examples of hot data are stock trading andmaking product recommendations in runtime. For warm data, such as financial statement preparation or productperformance reporting, you can live with the right amount of latency , from seconds to minutes, and you should use a data warehouse or arelational database. For cold data, such as storing 3 years of financial records for auditpurposes, you can plan latency in hours, and store it in archive storage. Choosing the appropriate storage, as per the data temperature also savescosts in addition to achieving performance SLA. As any solution designrevolves around handling the data, so a solution architect always needs tounderstand their data thoroughly and then choose the right technology . In this section, we have covered a high-level view of data in order to get theidea of using the proper storage, as per data nature.  Y ou will learn more about data engineering in Chapter 13 , Data Engineering   and Machine Learning . Using the right tool for th e right job helps to save costs and improve performance, so it's essential to choose the right data storage forthe right need. Think data-driven design Any software solution revolves around the collection and management ofdata. T ake the example of an e-commerce website; the software application is built to showcase product data on the website and encourage thecustomers to buy them. It starts by collecting customer data when theycreate a login, adding a payment method, store order transactions, andmaintaining inventory data as the product gets sold. Another example is abanking application, which is about storing customer financial informationand handling all financial transaction data with integrity and consistency . For any application, the most important thing is to handle, store, and securedata appropriately . In the previous section, you have learned about dif ferent kinds of data types, along with the storage needs, which should help you to apply datathinking in your design. Solution design is heavily influenced by data andenables you to apply the right design-driven solution by keeping data inmind. While designing a solution, if your application needs ultra-lowlatency , then you need to use cache storage such as Redis and Memcached . If your website needs to improve its page load time with an attractive high-quality image, then you need to use a content distribution network such asAmazon CloudFront or Akamai to store data near the user location.Similarly , to improve your application performance, you need to understand if your database will be read-heavy (such as a blog website) or write-heavy(such as survey collection) and plan your design accordingly . It's not just application design, but operational maintenance and businessdecisions all revolves around data. Y ou need to add monitoring capabilities to make sure that your application, and in turn, your business, is runningwithout any issues. For application monitoring, you collect log data fromthe server and create a dashboard to visualize the metrics. Continuous data monitoring and sending alerts in the case of issues helpsyou to recover quickly from failure, by triggering the auto-healing mechanism. From a business perspective, collecting sales data helps you torun a marketing campaign to increase the overall business revenue.Analyzing review sentiments data helps to improve the customer experienceand retain more customers, which is critical for any business. Collectingoverall order data and feeding to the machine learning algorithm helps youto forecast future growth and maintains the desired inventory . As a solution architect, you are not only thinking about application design,but also about overall business value proposition. It's about other factorsaround the application, which can help to increase customer satisfaction andmaximize the return on your investment. Data is gold and getting insightsinto data can make a tremendous dif ference to an or ganization's profitability . Over coming constraints Earlier , in Chapter 2 , S olution Ar chitect in an Or ganization , you learned about the various constraints that a solution architecture needs to handle andbalance. The major limitations are cost, time, budget, scope, schedule, andresources. Overcoming with these constraints is one of the significantfactors that needs to be considered while designing a solution. Y ou should look at the limitations as challenges that can be overcome rather thanobstacles, as challenges always push you to the limit for innovation in apositive way . For a successful solution, always put the customer first, while also takingcare of architectural constraints. Think backward from the customers' needs,determine what is critical for them, and plan to put your solution delivery inan agile way . One popular method of prioritized requirement is MoSCoW , where you divide customer requirements into the following categories: Mo  (Must have) : Requirements that are very critical for your customers, without which the product cannot launch.S (Should have) : Requirements that are the most desirable to the customer , once they start utilizing the application. Co (Could have) : Requirements that are nice to have, but their absence will not impact upon the desired functionality of theapplication.W   (W on't have) : Requirements that customers may not notice if it is not there. Y ou need to plan a minimum viable pr oduct ( MVP ) for your customer with must-have ( Mo ) requirements and go for the next iteration of delivery with should-have ( S ) requirements. W ith this phased delivery approach, you can thoroughly utilize your resources and overcome the challenges of time,budget, scope, and resources. The MVP approach helps you to determinecustomer needs. Y ou are not trying to build everything without knowing if the features you've built have added value for the customer . This customer - focused approach helps to utilize resources wisely and reduces the waste ofresources. In the following diagram, you can see the evaluation for a truckmanufacturing delivery , where the customer wants a delivery truck that gets delivered initially , and you evolve the process based on the customer's requirements: M V P a p p r o a c h t o b u i l d i n g t h e s o l u t i o n Here, once a customer gets a first delivery truck, which is a full functioning,they can determine if they need a more significant load to handle, and basedon that, the manufacturer can build a six-wheel, a 10-wheel, and finally an18-wheel truck trailer . This stepwise approach provides working products with essential features that the customers can use, and the team can buildupon it, as per the customer requirement. T echnology constraints become evident in a lar ge or ganization, as bringing changes across hundreds of systems will be challenging. When designingapplications, you need to use the most common technique that is usedacross the or ganization, which will help to remove the everyday challenges. Y ou also need to make sure that the application is upgradable in order to adopt new technology , and able to plug in components that are built in a dif ferent platform. A RESTful service model is pretty popular when teams are free to use anytechnology for their development, the only thing they need to provide is aURL with which their services can be accessed. Even legacy systems suchas mainframes can be integrated into the new system using an API wrapperaround it and overcome technology challenges. Y ou can see how the MVP approach helps to utilize limited resources in an ef ficient way , which helps to buy more time and clarify the scope. In terms of the other factors, when you put the working product in the customer'shands early , it gives you an idea of where to invest. As your application has already started generating revenue, you can present use cases to ask formore resources, as required. T aking an agile approach helps you to overcome various constraints and build a customer -centric product. In design principles, take everything as a challenge and not an obstacle. Consider any constraint as a challenge andfind a solution to solve it.  Adding security everywher e Security is one of the essential aspects of solution design; any gap insecurity can have a devastating ef fect on business and the or ganization's future. The security aspect can have a significant impact on solution design,so you need to understand your security needs even before starting theapplication design. Security needs to include in platform readiness at thehardware level and in application development at the software level. Thefollowing are the security aspects that need to be considered during thedesign phase: Physical security of data center : All IT resources in data centers should be secure from unauthorized access.Network security : The network should be secure to prevent any unauthorized server access.Identity and Access Management (IAM) : Only authenticated users should have access to the application, and they can do the activity asper their authorization.Data security in-transit : Data should be secure while traveling over the network or the internet.Data security at r est : Data should be secure while stored in the database or any other storage.Security monitoring : Any security incident should be captured, and the team alerted to act. Application design needs to balance security requirements such asencryption, and other factors such as performance and latency . Data encryption always has a performance impact as it adds a layer of additionalprocessing because data needs to be decrypted in order to be utilized.  Y our application needs to accommodate the overhead of additional encryptionprocessing without impacting overall performance. S o, while designing your application, think of use cases where encryption is really required. Forexample, if the data is not confidential, you don't need to encrypt it. The other aspect of application design to consider is regulatory compliancefor adherence to local law . Compliance is essential if your application belongs to a regulated industry such as health care, finance, or federalgovernment. Each compliance has its requirement, which commonlyincludes the protection of data and the recording of each activity for auditpurposes. Y our application design should build comprehensive logging, and ensure through monitoring, which will fulfill the audit requirement. In this section, you have learned to apply security thinking while designingand keeping any regulatory needs in mind. Security automation is anotherfactor , which you should always implement along with your design, in order to reduce and mitigate any security incidence. However , you have a high-level overview here. Y ou will learn more details in Chapter 8 , Security Considerations . Automating everything Most accidents happen due to human error , which can be avoided by automation. Automation not only handles jobs ef ficiently , but also increases productivity and saves costs. Anything identified as a repeatable task shouldbe automated to free up valuable human resources so that team memberscan spend their time on more exciting work and focus on solving a realproblem. It also helps to increase team morale. When designing a solution, think about what can be automated. Considerthe following components to be automated in your solution: Application testing : Y ou need to test your application every time you make any changes to make sure that nothing breaks. Also, manualtesting is very time consuming and requires lots of resources. It's betterto think about automating repeatable test cases to speed up deploymentand product launch. Automate your testing at production scale and userolling deployment techniques, such as canary testing and A/B testing,to release changes.IT infrastructur e : Y ou can automate your infrastructure by using infrastructur e as code scripting, for example, Ansible, T erraform, and Amazon CloudFormation. Automation of infrastructure allowsenvironments to be created in minutes compared to days. Automationof infrastructure as code helps to avoid configuration errors and createsa replica of the environment.Logging, monitoring, and alerting : Monitoring is a critical component, and you want to monitor everything every time. Also,based on monitoring, you may want to take automated action such asscaling up your system or alerting your team to act. Y ou can monitor the vast system only by using automation. Y ou need to automate all activity monitoring and logs in order to make sure that yourapplication is running smoothly , and that it is functioning as desired. Deployment automation : Deployment is a repeatable task that is very time consuming and delays the last-minute launch in many real-time scenarios. Automating your deployment pipeline by applyingcontinuous integration and continuous deployment helps you to beagile and iterate quickly on product features with a frequent launch.Continuous deployment and continuous integration help you to makesmall incremental changes to your application.Security automation : While automating everything, don't for get to add automation for security . If someone is trying to hack your application, you want to know immediately and act quickly . Y ou want to take preventive action by automating any incoming or outgoingtraf fic in your system boundary and alert any suspicious activity .  Automation provides peace of mind by making sure the product isfunctioning without a glitch. While designing an application, always makessure to think from an automation perspective and consider that as a criticalcomponent. Y ou will learn more details about automation throughout the book in the coming chapters.   Summary In this chapter , you learned about the various principles of solution architecture design which you need to apply when creating a solutiondesign. These principles help you to take a multi-dimensional look intoarchitecture and consider the important aspects for the success of theapplication. W e started the chapter with predictive and reactive patterns of scaling, along with the methodology and benefits. W e also learned about building a resilient architecture that can withstand failure and recover quickly withoutimpacting the customer experience. Designing flexible architecture is the core of any design principle, and welearned about how to achieve a loosely coupled design in your architecture.SOA helps to build an architecture that can be easily scaled and integrated.W e also covered microservice architecture, and how it is dif ferent from the traditional monolithic architecture, and its benefits. W e then learned about the principle of data-focused design, as pretty much all applications revolve around data. Y ou learned about dif ferent data types, with the example of storage and associated technology . Finally , you learned the design principle of security and automation, which applies everywhereand in all components. As cloud-based services and architecture are becoming standard, in the next c hapter , you will learn about cloud-native architecture and develop a cloud- oriented design. Y ou will learn about dif ferent cloud migration strategies and how to create an ef fective hybrid cloud. Y ou will also learn about the popular public cloud providers, with which you can explore cloudtechnologies further . Cloud Migration and HybridCloud Ar chitectur e Design So far , you have learned about various aspects of solution architecture, architecture attributes, and architecting principals. Since the cloud is adeveloping trend, you will see dif ferent examples from popular cloud technologies. The public cloud is becoming one of the primary destinationsto host applications, so it's important not to undervalue the preposition andmigration method in the cloud. In this chapter , you will learn about the various aspects of the cloud and develop cloud thinking, which will alsohelp you understand the upcoming chapters better . As you learned in Chapter 1 , The Meaning of Solution Ar chitectur e , cloud computing refers to the on-demand delivery of IT resources over the web,and you pay as you utilize resources. The public cloud helps you acquiretechnologies such as compute, storage, network, and databases on an as-needed basis, instead of buying and maintaining your own data centers. W ith cloud computing, the cloud vendor manages and maintains the technology infrastructure in a secure environment and or ganizations access these resources over the web to develop and run their applications. An ITresource's capacity can go up or down instantly and or ganizations only pay for what they use. Now , the cloud is becoming essential for every enterprise strategy . Almost every or ganization increases its spending in IT , and on top of saving cost, they convert upfront capital expenditures into operational expenditures. Alot of startups born in the last decade started in the cloud and were fueled bycloud infrastructur e for rapid growth. As an enterprise moving to the cloud, they are focusing on cloud migrationstrategy and hybrid cloud first before becoming cloud-native. In this chapter , you will learn about the various strategies of cloud migrations and hybrid cloud by covering the following topics: Benefits of cloud-native architectureCreating a cloud migration strategySteps for cloud migrationCreating a hybrid cloud architectureDesigning a cloud-native architecturePopular public cloud choices By the end of this chapter , you will learn about the benefits of the cloud and be able to design cloud-native architecture. Y ou will understand dif ferent cloud migration strategies and steps. Y ou will also learn about hybrid cloud design and popular public cloud providers. Benefits of cloud-nativear chitectur e In recent years, technology has been changing rapidly and new companieshave been born in the cloud world, disrupting old and long-standingor ganizations.  Rapid growth  is possible because of no upfront cost involved when or ganizations are using the cloud, and there is less risk in experimentation due to the  pay-as-you-go model of the cloud. The cloud  agile appr oach helps employees in an or ganization develop innovative thinking and implement their ideas without waiting for the longcycle of infrastructure. W ith the cloud, customers don't need to plan excess capacity in advance to handle their peak season, such as holiday shoppingseason for retailers; they have the elasticity to provision resources perdemand instantly . This significantly helps reduce costs and improve the customer's experience. For any or ganization to stay in the competition, they have to move fast and innovatively . W ith the cloud, enterprises are not only able to get their infrastructure quickly across the globe but can also access a wide variety of technologiesthat were never available before. These include a ccess to cutting edge technologies such as the following: Big dataAnalyticsMachine learning Artificial intelligenceRoboticsIoT (Internet of Things)Blockchain Also, to achieve scalability and elasticity , these are some of the reasons that can trigger the initiative for cloud migration and hybrid cloud strategy: The data center needs a technology refreshThe data center's lease is endingThe data center has run out of storage and compute capacityModernization of an applicationLeverage cutting-edge technologiesNeed to optimize IT resources to save costDisaster recovery planningUtilizing a content distribution network for the website Every or ganization has a dif ferent strategy , and one size does not fit all when it comes to cloud adoption. The frequent use cases are puttingdevelopment and testing environments in the cloud to add agility fordevelopers so that they can move faster . As hosting web applications is becoming more economical a nd more comfortable with the cloud, or ganizations are using the cloud for digital transformation by hosting their websites and digital properties in the cloud. For application accessibility , it is essential to not only build an application for the web browser but to ensure it is accessible through smart mobiles and tablets . The cloud is helping with such transformations. Data processing and analytics is another area where enterprises are utilizing the cloud sinceit is less expensive to collect, store, analyze, and share data with the cloud. Building a solution architecture for the cloud is slightly dif ferent than it is for regular enterprise architecting. While moving to the cloud, you have todevelop cloud thinking and understand how to leverage the in-builtcapabilities of the cloud. For cloud thinking, you follow the pay-as-you-go model. Y ou need to make sure that you optimize your workload properly and run your servers when it's required. Y ou need to think about how to optimize costs by bringing up your workload (when needed) and choosing the right strategy for the workload,which always needs to be running. In the cloud, the solution architect needs to have a holistic view of each component regarding performance, scaling,high availability , disaster recovery , fault tolerance, security , and automation. The  other areas of optimization are  cloud-native monitoring and alerting mechanisms . Y ou may not need to bring your existing third-party tool from on-premise to the cloud as you can utilize native cloud monitoringbetter and get rid of costly third-party licensing software. Also, now , you get to have deployment capabilities to any parts of the world in minutes, sodon't restrict yourself to a particular region and utilize the globaldeployment model to build better high availability and disaster recoverymechanisms.  The cloud provides excellent deals for automation; you can pr etty much automate everything . Automation not only reduces errors and speeds up process time to market; it also saves lots of cost by utilizing humanresources ef ficiently and freeing them up from performing tedious and repetitive tasks. The cloud works on a shar ed r esponsibility model where cloud vendors are responsible for securing physical infrastructure.However , the security of an application and its data is entirely the customer's responsibility . Therefore, it's important to lock down your environment and keep a tab on security by utilizing cloud-native tools formonitoring, alerts, and automation. Throughout this book, you will learn about the cloud perspective of solutionarchitecture and get an in-depth understanding of cloud  architecture. In the next section, you will learn about various strategies for cloud migration. Cr eating a cloud migration strategy As we mentioned in the previous section, there could be various reasons formigrating to the cloud, and those reasons play an essential role in your cloudmigration journey . They will help you determine a migration strategy and prioritize applications. In addition to primary business drivers for cloudmigration, you could have more reasons related to the data center , business, application, team, and workload for cloud migration. Often, migration projects adopt multiple strategies and utilize dif ferent tools accordingly . The migration strategy will influence the time it takes to migrate and how the applications are grouped for the migration process. Thefollowing diagram shows some of the commonly used strategies formigrating existing applications to the cloud: C l o u d m i g r a t i o n s t r a t e g y As shown in the preceding diagram, an or ganization can take a mix of migration strategies; for example, if an application-hosted operatingsystem is at its end of life, then you need to upgrade the OS. Y ou can take this opportunity to migrate to the cloud for better flexibility . In this case, most likely , you will choose the  Replatform method to recompile your code into a new version of the OS and validate all its features. After you've finished testing, you can migrate the application to the OS hosted in theinfrastructure provided by the cloud. Y ou can do a  Lift and Shift of the server or application from the source environment to the cloud. Migrating a resource only needs minimal changesfor it to work in the cloud. T o take a more c loud-native approach, you can refactor your application to fully utilize the cloud-native feature, forexample, converting monolithic into microservice. If y our application is a legacy application and cannot be moved, or it is not cloud compatible, youmay want to retire it and replace it with a cloud-native SaaS product or third-party solution. Y our business objectives will drive your decision to migrate applications and their priority and strategy for migration. For example, when cost ef ficiency is the main driver , the migration strategy typically involves mass migration with a heavy focus on the Lift and Shift approach. However , if the main goal is to enable agility and innovation, the cloud-native approach (such asrearchitecting and refactoring) plays a key role in the cloud migrationstrategy . W e'll learn more about each strategy in the following subsections. Lift and Shift migration  Lift and Shift are the fastest modes of migration, as you need minimal workto move your application. However , it does not take advantage of the cloud- native feature. Let's look at the most common migration strategies, thatis,  r ehost , r eplatform , and  r elocate , which are often utilized to do Lift and Shift with minimal changes needing to be made to the application. Rehost Rehost is fast, predictable, repeatable, and economical, which makes it themost preferred method for migrating to the cloud. Rehost is one of thequickest cloud migration strategies, where the server or application is liftedand shifted from the source on-premises environment to the cloud. Minimalchanges may be made to the resources during the migration pr ocess . Customers often use rehost to migrate their applications to the cloudquickly and then focus on optimization when the resources are running inthe cloud. This technique allows them to realize the cost benefits of usingthe cloud. Customers typically use the rehost technique for the following reasons: The temporary development and testing environmentFor when servers are running packaged software, such as SAP andMicrosoft SharePointAn application doesn't have an active roadmap While rehost is an application for packaged software and helps us movequickly into the cloud, you may need to upgrade underlying applicationplatforms such as operating systems. In such a situation, you can use thereplatform approach of cloud migration. Replatform When the operating system, server , or database version goes to its end of life from providers, then it can trigger the cloud migration project, forexample, upgrading the operating system of your web server fromMicrosoft W indows 2003 to Microsoft W indows 2008/2012/2016 or upgrading your Oracle database engine, and so on. Replatform involvesupgrading the platform as a part of the cloud migration project. Y ou can decide to update your operating system or application to a newer release aspart of the migration. When using the replatform migration strategy , you may need to reinstall your application on the tar get environment, which triggers application changes. This requires thorough testing on your application after replatformto ensure and validate its post-migration operational ef ficiency . The following common reasons warrant the use of the replatform technique: Changing the operating system from 32-bit to 64-bit OSChanging the database engineUpdating the latest release of the applicationUpgrading an operating system from W indows 2008 to W indows 2012 or 2016Upgrading the Oracle database engine from Oracle 8 to Oracle 1 1 T o get the benefits of managed services that are available from cloud vendors such as managed storage, databases, application deployment,and monitoring tools Replatform helps you advance your application's underlying platform whilemigrating to the cloud. Y ou can simply relocate your application to the cloud if it was deployed in containers or VMware. Now , let's learn more about the Relocate strategy . Relocate Y ou may deploy your application using containers or VMware appliances in your on-premise data center . Y ou can move such workloads to the cloud using the accelerate migrations strategy known as  r elocate . Relocate helps you move hundreds of applications in days. Y ou can quickly relocate applications based on VMware and container technologies to the cloud withminimal ef fort and complexity . The relocation strategy does not require much upfront developer investmentor an expensive test schedule since it provides the agility and automationyou expect from the cloud. Y ou need to determine existing configurations and use VMotion or Docker to relocate your servers to the cloud. VMotion is known for live migration. It's a  VMware technology that enables a virtual instance to be moved from one physical host server to another without anyinterruption in the service.  Customers typically use the relocating  technique for the following reasons: W orkloads have been deployed in the container  Applications have been deployed in VMware appliances  VMwar e Cloud ( VMC ) on A WS not only migrates applications, but it migrates thousands of virtual machines, live from individual application toyour entire data centers. While migrating your application to the cloud, youmay want to take the opportunity to rebuild and rearchitect your entireapplication to make it more cloud-native. The cloud-native approach allowsyou to use the full capability of the cloud. Let's learn more about the cloud-native approach. Cloud-native appr oach When your team decides to move cloud-native for the short term, it seemslike more upfront work and slower mig ration to the cloud. This is  a bit costly , but it pays of f in the long term when you start using all the cloud benefits with the agile team to innovate. Y ou will see a drastic decrease in cost over time with the cloud-native approach as you can optimize your workload for the right price whilekeeping performance intact with the  pay as you go model. Cloud-native includes containerizing your application by rearchitecting as a microserviceor opting for a purely serverless approach. For your business needs, you may want to replace the entire product witha  r eady-to-use  SaaS of fering, for example, replacing on-premise Sales and HR solutions with Salesforce and W orkday SaaS of ferings. Let's learn more about the refactor and repurchase methods for the cloud-native migrationapproach. Refactor Refactor involves rearchitecting and rewriting an application beforemigrating it to the cloud to make it a cloud-native application. Cloud-nativeapplications are applications that have been designed, architected, and builtto perform ef ficiently in a cloud environment. The benefits of these cloud- inherent capabilities include  scalability , security , agility , and cost- efficiency . Refactoring requires more time and resources to recode the application andre-architecture it before it can be migrated. This approach is commonlyused by or ganizations that have extensive cloud experience or a highly skilled workforce. An alternative option for refactoring is to migrate yourapplication to the cloud and then optimize it. Common examples of refactoring include the following: Changing platforms such as AIX to UNIXDatabase transition from traditional to the cloud buildReplacing middleware productsRearchitecting the application from monolithic to microserviceRebuilding application architecture such as containerizing or serverlessRecoding application components Sometimes, you may find a lar ge ef fort being made to rebuild an application. As an architect, you should evaluate if purchasingthe  Softwar e-As-A-Service ( SaaS ) product helps you get a better r eturn on investment ( ROI ). Let's explore the repurchase strategy in more detail. Repur chase When your IT resources and projects are migrated to the cloud, you mayneed servers or applications, which requires you to purchase a cloud-compatible license or release. For example, the current on-premises licensefor your application might not be valid when you run the application in thecloud. There are multiple ways to address such scenarios of licensing. Y ou can purchase a new license and continue to use your application in the cloud, oryou can drop the existing application and replace it with another one in thecloud. This replacement could be a SaaS of fering of the same application. Common examples of repurchase include the following: Replacing the application with a SaaS such as Salesforce CRM orW orkday HR Purchasing a cloud-compatible license The cloud may not be the answer for all of your problems and sometimes,you will find a legacy application that may not benefit from cloud migrationor discover rarely used applications that can be retired. Let's learn aboutthe  r etain or r etir e  strategy  in more detail. Retain or r etir e When you are planning a cloud migration, it may not be necessary to moveall the applications. Y ou may need to retain some applications due to technology constraints; for instance, they may be legacy applicationscoupled with an on-premise server and cannot move. On the other hand,you may want to retire some applications and use cloud-native capabilities,for example, third-party application monitoring and alerting system. Let'slearn more about the retain or retire decision. Retain Y ou might encounter a few applications in your on-premises environment that are essential for your business but are not suitable for migrationbecause of technical reasons, such as the OS/application not beingsupported on a cloud platform. In such situations, your application cannotbe migrated to the cloud, but you can continue running it in your on-premises environment. For such servers and applications, you may need to perform only the initialanalysis to determine their suitability for cloud migration. However , the server or application may still have dependencies with applications that aremigrated. Therefore, you may have to maintain the connectivity of these on-premises servers to your cloud environment. Y ou will learn more about on- premises to cloud connectivity in the Cr eating a hybrid cloud ar chitectur e section of this chapter . Some typical workload examples for retentions are as follows: The legacy application where the customer doesn't see the benefit ofmoving to the cloud.The operating system or application support is not available in thecloud such as AS400 and mainframe application. Y ou may want to retain complex legacy systems to on-premise and prioritize them so that they can be moved a later point; however , during discovery , often, or ganizations find applications that are not in use anymore but still sitting around and consuming infrastructure space. Y ou can choose to retire such applications. Let's explore more about the retiring strategy .  Retir e While migrating to the cloud, you may discover the following: Rarely used applicationsApplications consuming an excessive amount of server capacityApplications that may not be required due to cloud incompatibility In such  a  situation , you may want to retire the existing workload and take a fresh approach, which is more cloud-native. A retirement strategy can be applied to hosts and applications that are soongoing to be decommissioned. This can also be applied to unnecessary andredundant hosts and applications. Depending on your business needs, suchapplications can be decommissioned on-premises without even migrating tothe cloud.  Hosts and applications that are commonly suited for retirement include the following: On-premise servers and storage for disaster recovery purposesServer consolidation to resolve redundanciesDuplicate resources due to mer gers and acquisitions Alternative hosts in a typical high-availability setupThird-party licensed tools such as workload monitoring andautomation, which is available as in-built capabilities in the cloud Most migration projects employ multiple strategies, and there are dif ferent tools available for each strategy . The migration stra tegy will influence the time it takes to migrate and how the applications are grouped for themigration process. Cloud migration is a good time to examine your overall inventory and getrid of the ghost server running under the developer desk and goingunaccounted for . Often, the customer sees the typically unseen advantage of optimizing the workload and tightening security while running applicationdiscovery to prepare for migration. There are multiple phases involved in cloud migration. In the next section, you will learn about the steps for cloudmigration.Steps for cloud migration  In the previous section, you learned about dif ferent migration strategies and grouped your application in order to apply the appropriate migrationtechnique. Since you may need to perform and manage multipleapplications in the cloud, it's better to set up a cloud Center of Excellence ( CoE ) and standardize this process with a cloud migration factory . The cloud CoE includes experienced people from various IT and businessteams across the or ganization that act as a dedicated cloud team focused on accelerating the building of cloud expertise in the or ganization. The cloud migration factory defines migration processes and tools, as well as the stepsthat need to be taken, as shown in the following diagram: C l o u d m i g r a t i o n s t e p s As shown in the previous diagram, the cloud migration steps include thefollowing: Discover : Discovery of cloud migration portfolios and on-premise workloadsAnalyze : Analyze discovered data and workloads Plan : Plan migration to the cloud and define the migration strategy Design : Design the application as per the migration strategy Migrate : Execute the migration strategy Integrate : Integrate with dependencies V alidate : V alidate functionality after migration Operate : Plan to operate in the cloud Optimize : Optimize your workload for the cloud One of the initial steps of a cloud migration project is to assess andprioritize the applications for migration. T o accomplish this, you need to get a complete inventory of the IT assets in your environment to determinewhich servers, applications, and business units are suitable for migrating tothe cloud, prioritize the migration plan, and determine a migration strategyfor these applications. Let's drill down each step and learn more about them. Discovering your workload In the discovery phase of your migration project, you discover and capturedetailed data about your cloud migration portfolio , for example, the scope of your migration project. Y ou identify servers and applications in your portfolio, as well as their interdependencies and current baselineperformance metrics. Then, you analyze the gathered information todetermine application connectivity and capacity requirements, which canguide you in designing and architecting the tar get cloud environment and identifying the cost. A detailed discovery can also help in identifying any issues in the currentstate of the application that might need mitigation before you migrate to thecloud. While analyzing the discovery data, you will also determine anappropriate migration method for your application. Portfolio discovery is the process of identifying all the IT assets that are involved in your cloudmigration project, including servers and applications, their dependencies,and performance metrics. Y ou will also need to gather business details about your resources, such as the Net Pr esent V alue ( NPV ) of the resource, the refresh cycle of the application, the roadmap of the application, and the business criticality ofthe server or application. These details will help you determine yourmigration strategy and create a migration plan. In most or ganizations, these details are maintained across multiple business units and teams. Therefore,during the process of discovery , you may have to interact with various teams, such as business, development, data center , network, and finance. It is essential to understand that your discovery landscape will depend onvarious factors: What has already been migrated to the cloud?What application dependencies are there, along with resources andassets? What are the business drivers for cloud migration?What is the estimated duration for the entire migration project?How many phases is the migration process going to happen in? One of the top challenges of a migration project is determining inter dependencies among applications , particularly since they pertain to Input/Output ( I/O ) operations and communications. Cloud migration becomes even more challenging as or ganizations expand due to mer gers, acquisitions, and growth. Or ganizations often do not have complete information about the following: The inventory of the number of serversServer specifications such as the type and version of OS, RAM, CPU,and diskServer utilization and performance metricsServer dependenciesOverall networking details Performing a thorough portfolio discovery helps in answering questionssuch as the following: Which applications, business units, and data centers are goodcandidates for migration?How suitable are the applications for migrating to the cloud?What known or unknown risks are associated with migrating anapplication to the cloud?How should the applications be prioritized for migration? Which other IT assets is the application dependent on?What is the best migration strategy for the application?Is it better to have some downtime for the application than to performa live migration due to its dependencies and risks? Several tools are available in the market that can help automate thediscovery process and provide more detailed information in a variety offormats. These tools can be classified based on various characteristics, such as deployment type, operation, support, and type of data discovered andreported. Most of the available solutions can be broadly classified into twocategories: Agent-based solutions : They require their software client to be installed on a server to gather the necessary details.Agentless solutions : They may be able to capture this information without any additional installations. Some solutions perform port scanning to probe a server or host for open ports, while others perform packet scanning , which often involves capturing and analyzing network packets to decode the information.  The tools also vary based on the granularity of the data that's discovered, the storage types,and the reporting options . For example, some tools can provide a higher stack of intelligence beyond the network and can also determine the type ofapplications running. The complexity of the discovery process depends on the or ganization's workload and if they already have a well-maintained inventory in place.Discovery processes are typically run for at least a couple of weeks togather more holistic information about your environment. Once youdiscover all the necessary information, you need to analyze it. Let's look atthe analysis step in more detail. Analyzing the information T o identify server and application dependencies, you need to analyze the network connectivity data, port connections, system, and processinformation on the hosts. Depending on your tool, you can visualize all thecontacts from a server to identify its dependencies, or you can run queriesto list all the servers running a specific process, using a particular port, ortalking to a specific host. T o group your servers and applications for migration scheduling, you need to identify patterns in your host configurations. Often, some prefixes areembedded in the server hostnames to signify their association with aparticular workload, business unit, application, or requirement. Someenvironments might also use tags and other metadata to associate suchdetails with the host. T o right-size your tar get environment, you can analyze the performance metrics for your servers and applications: If a server is over -pr ovisioned , you can revise your right-size mapping information. Y ou can also optimize this process by leveraging the utilization data for the server/application instead of the serverspecifications.If a server is under -pr ovisioned , you might assign a higher priority to the server to migrate to the cloud. Depending on the environment, the type of data that's captured during thediscovery process might vary . The data analyzed for migration planning is to determine tar get network details such as fir ewall configuration and workload distribution and also the phase , in which the application will be migrated. Y ou can combine this insight with the availability of your resources and business requirements to prioritize your cloud migration workload. This insight can help you in determining the number of servers to be included aspart of each cloud migration sprint. Based on the discovery and analysis of your cloud migration portfolio, youcan determine an appropriate cloud migration strategy for your applications.For instance, servers and applications that are less complex and running ona supported OS might be suitable candidates for a Lift and Shift  strategy . Servers or applications that run on unsupported OS might need furtheranalysis to determine an appropriate strategy . In a cloud migration project, discovery , analysis, and planning are tightly integrated. Y ou are performing a full discovery of your cloud migration portfolio and analyzing the data to create a migration plan. By the end ofthe analysis phase, based on your analysis and the details you've gatheredfrom business owners, you should be able to do the following for eachserver/application that is part of your cloud migration portfolio: Choose a migration strategy for the server/application, depending onyour or ganization's cloud adoption strategy . Y ou may be limited to specific choices within retaining, retire, repurchase, rehost, replatform,and refactor . Assign a priority for migrating the resource to the cloud.  Eventually , all the resources that are part of the   cloud   migration portfolio may migrate to the cloud, but this priority will determine the ur gency of that migration. A higher priority resource might move earlier in themigration schedule. Document the business driver for migrating the resource to the cloud,which will drive the need and priority for migrating the resource to thecloud. Let's look at migration planning in more detail. Cr eating migration plan The next phase in your migration project is planning cloud migration . Y ou will use the information you gathered during the portfolio discovery phaseto create an ef ficient migration plan. By the end of this phase in your migration project, you should be able to create an ordered backlog ofapplications that can migrate to the cloud. The main goals of the migration planning phase include the following: Choosing a migration strategyDefining the success criteria for the migrationDetermining the right-sizing of the resources in the cloudDetermining a priority for applications to migrate to the cloudIdentifying migration patternsCreating a detailed migration plan, checklist, and scheduleCreating migration sprint teamsIdentifying tools for migration In preparation for the migration planning phase, you must perform adetailed discovery of all the IT assets that are part of your cloud migrationportfolio. The tar get destination environment in the cloud is also architected before the planning phase. Migration planning includes determining thecloud account structure and creating a network structure for yourapplication. It is also essential to understand hybrid connectivity with thetar get cloud environment. Hybrid connectivity will help you plan for applications that might have dependencies with resources that are stillrunning on-premise. The order of application migration can be determined through three high-level steps: 1 . Evaluate each application across several business and technicaldimensions associated with a potential migration to accurately quantifythe environment. 2 . Identify the dependencies for each application with qualifications suchas locked, tightly coupled, and loosely coupled to identify anydependency-based ordering requirements. 3 . Determine the desired prioritization strategy of the or ganization to determine the appropriate relative weighting of the variousdimensions. The initiation of an application or server migration depends on two factors: First, the prioritization strategy of your or ganization and the application priority . Y our or ganization might place varying emphasis on a few dimensions, such as maximizing ROI, minimizing risk, easeof migration, or another custom dimension.Second, the insight gained through the portfolio discovery and analysisphase can help you identify application patterns that match its strategy . For example, if the or ganizational strategy is to minimize the risk, then business criticality will have more weight in identifying the applications. Ifease of migration is the strategy , applications that can be migrated using rehost will have higher priority . The outcome of planning should be an ordered list of applications that can be used to schedule the cloud migration. The following are the planning aspects of migration: Gather baseline performance metrics for your applications beforemigration. Performance metrics will help you design or optimize yourapplication architecture in the cloud quantitatively . Y ou might have captured most of these performance details during the discovery phase.Create test plans and user acceptance plans for your applications.These plans will help in determining the outcome (success or failure)of the migration process.Y ou also need to have cutover strategies and rollback plans that define how and where the applications will continue to run based on theoutcome of the migration.Operations and management plans will be useful for determining theownership of roles during migration and post-migration. Y ou can leverage RACI matrix spreadsheets to define these roles and responsibilities for your application that span the entire cloudmigration journey . Identify points of contact within the application team that can providetimely support in case of escalations. Close collaboration across theteams will ensure the successful completion of the migration perschedule (sprint). If your or ganization has some of these processes already documented for your existing on-premises environment, for example, change controlprocess, test plans, and run books for operations and management, youmight be able to leverage them. Y ou need to compare performance and cost before, during, and after migration, which can be an indication that they are not currently capturingenough of the right Key Performance Indicators ( KPIs ) to enable this insight. The customer needs to identify and begin achieving useful KPIs sothat there is a baseline to compare against during and after migration. TheKPI approach in migration has a twofold goal. First, it needs to define thecapabilities of your existing application and then compare them with thecloud infrastructure. When the new products are added to the catalog or a new service islaunched, it increases your company revenue, and that's a count againstcompany KPI. Generally , IT metrics include the quality of the product and the number of bugs that are reported for an application.  Service-Level Agr eement ( SLA ) defined for fixing a critical bug, system downtime, and performance metrics include system resource utilization values such asmemory utilization, CPU utilization, disk utilization, and networkutilization. Y ou can use a continuous delivery methodology such as Scrum to ef ficiently migrate applications to the cloud. W ith the help of the Scrum methodology , you can create multiple sprints and add your applications to the sprint backlogs based on prioritization. Y ou can sometimes combine many applications that follow a similar migration strategy and are possiblyrelated to each other . T ypically , you would maintain a constant duration across sprints and vary the application based on factors such as sprint teamsize and the complexity of the application. If you have small teams that have in-depth knowledge about theapplications that need to be migrated, then you can use weekly sprints,where each sprint consists of the discover/analyze, plan/design, and migratephases, with a final cutover on the last day of the sprint. However , as the team iterates through the sprints, the workload in each sprint can increasebecause the teams have now gained experience in the migration process andcan incorporate the feedback from previous sprints to make the currentsprint more ef ficient with continuous learning and adaptation. If you are migrating a complex application, you could also use the entireweek for just the plan/design phase and perform the other phases in separatesprints. T asks that you perform within the sprint and their deliverables can vary , depending on factors such as complexity and team size. The key is to get some values from the sprint. Y ou can create multiple teams to assist in the migration process, depending on various factors such as your product backlog, migration strategy , and or ganizational structure. Some customers create groups focused on each migration strategy such as a Rehost team, a Refactor team, and aReplatform team. Y ou could also have a team specialized in optimizing your application architecture in the cloud. The multi-team strategy is thepreferred model for or ganizations that have a lar ge number of applications to be migrated to the cloud. The team can be divided into the following segments: First, the team can validate the essential components to ensure yourenvironment (dev , test, or prod) is working, adequately maintained, and monitored.The integration team will determine the application configuration andalso find the dependencies, which will help reduce the waste that'smade by another team.The Lift and Shift migration sprint team migrates lar ge applications that don't require refactoring or replatforming. The team will use automation tools to deliver small incremental value after every sprint.The replatform migration sprint team focuses on applicationarchitecture changes in order to migrate applications to the cloud, forexample, modernizing application design for microservices orupdating the operating system to the latest version.The refactor migration sprint team is responsible for managing variousmigration environments such as production, testing, and development.They make sure all the environments are scalable and functioning asrequired by monitoring them closely . The innovation migration sprint teamworks collaboratively withgroups such as the foundation and transition team to develop a packagesolution that can be used by other groups. It's recommended that you run a pilot migration project while planning andcontinuously building a product backlog so that these adaptations andlessons learned can be incorporated into the new plan. The successfulresults of the pilot project and sprint can also be used to help securestakeholders buy-in for the cloud transformation program. Designing the application During the design phase, your focus should be on successfully migratingapplications and making sure your application design to meet the requiredsuccess criteria is up to date after it has been migrated to the cloud. Forexample, if you are maintaining user sessions in the on-premise applicationserver (so that it can scale horizontally), make sure that a similar architectureis implemented in the cloud after the migration, which defines the successcriteria. It is essential to understand that the primary goal of this phase is to ensurethat your application has been designed to meet the migration successcriteria. Y ou need to identify opportunities that enhance your application, and these can be accomplished and achieved during the optimization  phase. For migration, first, you need to have a complete understanding of youror ganization's foundational architecture on-premises and in the cloud, which includes the following: User accountNetwork configurationNetwork connectivitySecurityGovernanceMonitoring Knowledge of these components will help you to create and maintain a newarchitect for your application. For example, if your application handlessensitive information such as  Personally Identifiable Information ( PII ) and has control access, this means your architecture needs a specific networksetting. During the design phase, you will identify the architecture gap and enhanceyour architecture as per your application requirements. When you havemultiple accounts, each account may have some level of relationship or dependency; for example, you can have a security account to ensure that allyour resources are compliant with company-wide security guidelines. When thinking about your application's network design, you need toconsider the following: Network packet flows entering the boundaries of your applicationExternal and internal traf fic routing  Firewall rules for network protectionApplication isolation from the internet and other internal applicationsOverall network compliance and governance Network log and flow audit Separation of application risk levels, as per their exposure to data andusersDDoS attack protection and preventionNetwork requirements for production and non-production environmentsSaaS-based multi-tenancy application access requirements Network boundaries at the business unit level in an or ganization  Billing and implementation of the shared services model across thebusiness unit Y ou can consider hybrid connectivity options with an on-premise system, depending on your connectivity needs. T o build and maintain a secure, reliable, performant, and cost-optimized architecture in the cloud, you needto apply best practices. Review your cloud foundational architecture againstthe cloud best practices before migrating to the cloud. Chapter 4 , Principles of Solution Ar chitectur e Design , highlights common architectural design patterns that you can consider when migrating yourapplication to the cloud. It is important to emphasize here that the primarygoal of the design phase in the migration process is to design yourapplication architecture so that it meets the migration success criteriaidentified in the planning phase. Y our application can be further optimized during the optimization phase of the migration project. In the process of migrating to the cloud, you can design your applicationarchitecture so that it benefits from the global cloud infrastructure and increases proximity to your end users, mitigates risk, improves security , and addresses data residency constraints. Systems that are expected to grow overtime should be built on top of a scalable architecture that can support growthin users, traf fic, or data with no drop in performance. For applications that need to maintain some state information, you couldmake specific components of the architecture stateless. If there are anylayers in the architecture that need to be stateful, you could leveragetechniques such as session af finity to be still able to scale such components. Leverage a distributed processing approach for applications that process vastamounts of data. Another approach to reducing the operational complexity of runningapplications is using serverless architectures. These architectures can alsoreduce cost because you are neither paying for underutilized servers norprovisioning redundant infrastructure to implement high availability . Y ou will learn more about the serverless architecture in Chapter 6 , Solution Ar chitectur e Design Patterns . The following diagram shows a migration design from on-premise to A WS Cloud: O n - p r e m i s e t o A W S C l o u d a r c h i t e c t u r e In the preceding diagram, as part of the cloud migration strategy , it was determined to rehost the web servers and introduce autoscaling to providethe elasticity that can help meet the spikes in demand. Elastic load balancersare also added to distribute the incoming traf fic to the web server instances. The application servers were migrated using refactor , and the platform for the database tier changed from the traditional database to a cloud-nativeAmazon RDS . The entire architecture is distributed across multiple availability zones to provide high availability , and the database replicates to a standby instance in the second availability zone. As an output of your design phase, you should create a detailed designdocument for the architecture of your application in the cloud. The designdocument should include details such as the user account that the applicationmust migrate to, network configuration, and a list of users, groups, andapplications that need access to the data held by this application. The designdocument should clearly articulate application hosting details andapplication-specific requirements for backup, licensing, monitoring, security , compliance, patching, and maintenance. Ensure that you create a designdocument for each application. Y ou will need it during the migration validation phase to perform a basic cloud functionality check and anapplication functionality check. Performing application migrationto the cloud Before executing on migration, ensure that you have a migration plan andthat you have identified the sprint teams and schedules, created a prioritizedbacklog, and notified all the application stakeholders about the migrationschedule, timelines, and their roles and responsibilities. Y ou must also ensure that the tar get environment in the cloud has already been set up with the foundational architecture and core services. Y ou might have some application-specific pre-steps, such as performing a backup or sync beforemigration, shutting down the servers, or unmounting disks and devices fromthe server . Make sure you have good network connectivity with the cloud environmentduring the migration process. A good estimate of the amount of data thatneeds to be migrated also helps you properly estimate the time it will taketo migrate your data to the cloud, given other factors such as bandwidth andnetwork connectivity . Y ou also need to understand the tools that are available to perform the migration. Given the number of devices that areavailable in the market, you might have to narrow down the selectioncriteria based on your requirements and other constraints. As you know , rehost is often the fastest way to migrate your application to the cloud. When the application is running in the cloud, you can furtheroptimize it to leverage all the benefits that the cloud has to of fer . By quickly migrating your applications to the cloud, you may start realizing the costand agility benefits sooner . Depending on the migration strategy , you typically migrate the entire server , including the application and the infrastructure that the application is running on, or just the data that belongs to an application. Let's look at howto migrate data and servers. Data migration Cloud data migration refers to the process of moving existing data to a newcloud storage location. Most applications will require data storagethroughout their progression into the cloud. Storage migration typicallyaligns with one of two approaches, but or ganizations may perform both at the same time: First, a single lift-and-shift  move. This may be required before new applications can be started up in the cloud.Second, a hybrid model weighted toward the cloud, which results in newly architected cloud-native projects with some legacy on-premisesdata. The legacy data stores may shift toward the cloud over time. However , your approach to migrating data will vary . It depends on factors such as the amount of data, network and bandwidth constraints, the dataclassification tier such as backup data, mission-critical data, datawarehouses, or archive data, and the amount of time you can allocate for themigration process. If you have extensive archives of data or data lakes in a situation wherebandwidth and data volumes are unrealistic, you might want to Lift and Shift  the data from its current location straight into a cloud provider's data centers. Y ou can do this either by using dedicated network connections to accelerate network transfers or by physically transferring the data. If your data stores can gradually migrate over time, or when new data isaggregating from many non-cloud sources, consider methods that provide afriendly interface to cloud storage service. These migration services canleverage or complement existing installations such as backup and recoverysoftware or a Storage Ar ea Network  ( SAN ). For a small-scale database, one-step migration is the best option, whichrequires you to shut down the application for 2 to 4 days . During the downtime, all information from the database is extracted and migrated tothe destination database in the cloud. Once the database has been migrated,it needs to be validated with the source database for no data loss. After that,a final cutover can be completed. In the other case, if a system requires minimal downtime, a two-stepmigration process is more commonly used for databases of any size: In the first step, information is extracted from the source database.In the next step, data is migrated while the database is still up andrunning. In the entir e pr ocess, ther e is no downtime. After the migration task has been completed, you can perform functionality and performance tests for connectivity toexternal applications or any other criteria as needed. During this time, because the source database is still up and running,changes will need to be propagated or replicated before the final cutover . At this point, you would schedule downtime for the database, usually a fewhours, and synchronize the source and destination databases. After all thechange data has been transferred to the tar get database, you should perform data validation to ensure a successful migration and finally routeapplication traf fic to a new cloud database. Y ou might have mission-critical databases that cannot have any downtime. Performing such zero-downtime migrations requires detailed planning andthe appropriate data replication tools. Y ou will need to use continuous data replication tools for such scenarios. It is important to note here that sourcedatabase latency can be impacted in the case of synchronous replication asit waits for data to be replicated everywhere before responding to theapplication while the replication is happening. Y ou can use asynchronous replication if your database downtime is only for a few minutes. W ith the zero-downtime migration, you have more flexibility regarding when to perform the cutover since the source and tar get databases are always in sync. Server migration There are several methods you can use to migrate a server to the cloud: The host or OS cloning technique involves installing an agent on thesource system that will clone the OS image of the system. A snapshotis created on the source system and then sent to the tar get system. This type of cloning is used for a one-time migration. W ith the OS Copy method, all operating system files are copied from the source machineand hosted on a cloud instance. For the OS Copy method to beef fective, the people and/or tool that executes the migration must understand the underlying OS environment.The Disaster Recovery ( DR ) replication technique deploys an agent on the source system that's used to replicate data to the tar get. However , the data is replicated at the filesystem or block level. A few solutions continuously replicate the data to tar get volumes, of fering a continuous data replication solution. W ith the Disk Copy method, the disk volume is copied in its entirety . Once the disk volume has been captured, it can be loaded into the cloud as volumes, which can then beattached to a cloud instance.For virtual machines, you could use agentless techniques toexport/import your virtual machine into the cloud. W ith the VM Copy method, the on-premise virtual machine image is copied. If the on-premise servers are running as virtual machines, such as VMware orOpenStack, then you can copy the VM image and import it into thecloud as a machine image . One main benefit of this technique is that you can have server backup images that can be launched over and overagain. W ith the User Data Copy method, only the application's user data is copied. Once the data has been exported from the original server , you can choose one of three migration strategies— r epur chase, r eplatform, or r efactor . The User Data Copy method is only viable for those who know the application's internals. However , because it only extracts user data, the User Data Copy method is an OS-agnostic technique. Y ou can containerize your application and then redeploy it in the cloud. W ith the containerization method, both the application binary and user data are copied. Once the application binary and user datahave been copied, it can be run on a container runtime that is hosted onthe cloud. Because the underlying platform is dif ferent, this is an example of the replatform migration strategy . Several migration tools in the market can help you migrate your data and/orserver to the cloud. Some tools take a DR strategy for migration, and someDR tools also support continuous replication to facilitate live migrations.There are some that specialize in forklifting your servers, performingdatabase migrations across platforms, or database schema conversion. Thetool must be able to support business processes that you are comfortablewith, and you must have the operational staf f to manage it. Integration, validation, and cutover Migration, integration , and validation go hand in hand as you want to do continuous validation while performing various integration with yourapplication in the cloud. The team starts by performing the necessary cloud functionality checks toensure that the application is running with proper network configuration (inthe desired geolocation) with some designated traf fic flow . Instances can start or stop as desired when the basic cloud functionality check iscomplete. Y ou need to validate that the server configuration (such as RAM, CPU, and hard disk) is the same as intended. Some knowledge of the application and its functionality is required toperform these checks. When the primary check is complete, then you canperform integration tests for the application. These integration tests includechecking integration with external dependencies and applications; forexample, to make sure the application can connect to the Active Directory , Customer Relationship Management ( CRM ), patch or configuration management servers, and shared services. When integration validation issuccessful, the application is ready for cutover . During the integration phase, you integrate the application and migrate it tothe cloud with external dependencies to validate its functionality . For example, your application might have to communicate with an ActiveDirectory server , a configuration management server , or shared services resources that are all external to the application. Y our application may also need to be integrated with external applications that belong to your clientsor vendors, such as a supplier receiving a feed from your APIs after apurchase order placement. When the integration process is complete, you need to validate theintegration by performing unit tests, smoke tests, and user acceptance test ( UA T ). The results from these tests help you get approval from the application and business owners. The final step of the integration andvalidation phase includes a sign-of f process from the application and business owner of the application, which will allow you to cut over theapplication from on-premises to the cloud. The final phase of the cloud migration factory is the cutover pr ocess . In this phase, you take the necessary steps to redirect your application traf fic from the source on-premise environment to the tar get cloud environment. Depending on the type of data or server migration (one-step, two-step, orzero-downtime migration), the steps in your cutover process may vary . Some factors to consider when determining a cutover strategy include thefollowing: Acceptable downtime for the applicationThe frequency of the data updateData access patterns such as read-only or static dataApplication-specific requirements such as database syncs, backups,and DNS name resolutionsBusiness constraints, such as the day or time during which the cutovercan happen and the criticality of the dataChanging management guidelines and approvals Live migration is most popular for business-critical workload migration,let's learn more about it. Live migration cutover The following diagram illustrates a cutover strategy for live zero-downtimemigration. In this method, the data is continuously replicated to thedestination, and you perform most of the functional validation andintegration testing at the destination while the application is still up andrunning: L i v e m i g r a t i o n c u t o v e r u s i n g b l u e - g r e e n d e p l o y m e n t In the replication process, the source on-premise database and tar get cloud database are always in sync. When all the integration and validation tests arecompleted successfully and the application is ready for cutover , you can take a blue-gr een appr oach to do the cutover . Y ou will learn more about blue- green deployments in Chapter 12 , DevOps and Solution Ar chitectur e Framework . Initially , the application continues to run both on-premises and in the cloud, resulting in traf fic being distributed between the two sides. Y ou can increase traf fic to cloud applications gradually until all the traf fic is directed to the new application, thus resulting in a cutover with no downtime. The other most commonly used cutover strategies involve some downtime.Y ou schedule downtime for the application, pause the traf fic, take the application of fline, and perform a final sync. After the final sync, it might be a good idea to perform a quick smoke test on the destination side. At thispoint, you can redirect the traf fic from the source to the application running in cloud, thus completing the cutover . Operating cloud application The operation phase of the migration process helps you to allow , run, use, and operate applications in the cloud to the level agreed upon with thebusiness stakeholders. Most or ganizations typically already have guidelines defined for their on-premises environments. This operational excellenceprocedure will help you identify the process changes and training that willallow operations to support the goals of cloud adoption. Let's discuss the dif ferences between deploying complex computing systems in a data center versus deploying them in the cloud. In a data centerenvironment, the burden of building out the physical infrastructure for aproject falls on the company's IT department. This means you need to makesure that you have the appropriate physical environmental safeguards foryour servers, such as power and cooling, that you can physically safeguardthese assets, and that you have maintained multiple redundant facilities atvarious locations to reduce the chances of a disaster . The downside of the data center approach is that it requires significantinvestment; it can be challenging to secure the resources that are necessaryif you wish to experiment with new systems and solutions. In a cloud computing environment, this changes dramatically . Instead of your company owning the physical data center , the physical data center is managed by the cloud provider . When you want to provision a new server , you ask your cloud provider for a new server with a certain amount ofmemory , disk space, data I/O throughput rate, processor capability , and so on. In other words, computing resources becomes a service that you canprovision, and de-provision as needed. The following are the IT operations that you would want to address in thecloud: Server patching Service and application loggingCloud monitoringEvent managementCloud security operationsConfiguration managementCloud asset managementChange managementBusiness continuity with disaster recovery and high availability IT or ganizations typically follow standards such as  Information T echnology Infrastructur e Library ( ITIL ) and Information T echnology Service Management ( ITSM ) for most of these operations. ITSM or ganizes and describes the activities and processes involved in planning, creating, managing, and supporting IT services, while ITIL applies bestpractices to implement ITSM. Y ou need to modernize your ITSM practices so that it can take advantage of the agility , security , and cost benefits provided by the cloud. In traditional environments, the development team and the IT operationsteamwork in their silos. The development team gathers the requirementsfrom business owners and develops the builds. System administrators aresolely responsible for operations and for meeting uptime requirements.These teams generally do not have any direct communications during thedevelopment life cycle, and each team rarely understands the processes andrequirements of the other team. Each team has its own set of tools,processes, and approaches, which often leads to redundant and sometimesconflicting ef forts. In a DevOps approach, both the development team and the operationsteamwork collaboratively during the build and deployment phases of thesoftware development life cycle to share responsibilities and providecontinuous feedback. The software builds are tested frequently throughoutthe build phase on production-like environments, which allows for the earlydetection of defects or bugs. DevOps is a methodology that promotes collaboration and coordinationbetween developers and operational teams to deliver products or services continuously . This approach is beneficial in or ganizations where the teams rely on multiple applications, tools, technologies, platforms, databases,devices, and so on in the process of developing or delivering a product orservice. Y ou will learn more about DevOps in Chapter 12 , DevOps and Solution Ar chitectur e Framework . Application optimization in thecloud Optimization is a very important aspect of operating in the cloud, and this isa continuous process of improvement. In this section, you will learn aboutthe various optimization areas. There are chapters dedicated to eachoptimization consideration in this book. The following are the majoroptimization areas: Cost : Optimize the cost ef ficiency of an application or a group of applications, while considering fluctuating resource needs. Y ou will learn more about architecture costconsiderations in Chapter 11 , Cost Considerations . Security : Continuously review and improve security policies and processes for the or ganization to protect data and assets in the A WS cloud. Y ou will learn more about architecture security considerations in Chapter 8 , Security Considerations . Reliability : Optimize applications for reliability to achieve high availability and defined downtime thresholds for applications, which will aid in recovering from failures, handling increased demand, andmitigating disruptions over time. Y ou will learn more about architecture reliability consideration in Chapter 9 , Ar chitectural Reliability Considerations . Operational excellence : Optimize operational ef ficiency and the ability to run and monitor systems to deliver business value and toimprove supporting processes and procedures continually . Y ou will learn more about architecture operational considerations in Chapter 10 , Operational Excellence Considerations . Performance : Optimize for performance to ensure that a system is architected to deliver ef ficient performance for a set of resources, such as instances, storage, databases, and space/time. Y ou will learn more about architecture performance considerations in Chapter 7 , Performance Considerations . T o optimize costs, you need to understand what is currently being deployed in your cloud environment and the price of each of those resources. Byusing detailed billing reports and enabling billing alerts, you can proactivelymonitor your costs in the cloud. Remember that, in the public cloud, you pay for what you use. Therefore,you might be able to reduce costs by turning of f instances when they are not needed. By automating your instance deployment, you can also tear downand build up the instance entirely as required. As you of fload more, you need to maintain, scale, and pay for less infrastructure. Another way to optimize costs is by designing yourarchitecture for elasticity . Make sure you right-size your resources, use auto-scaling, and adjust your utilization based on price and need. Forexample, it might be more cost-ef ficient for an application to use more small instances than fewer lar ge instances. Several application architectural modifications can help you improve theperformance of your application. One way to improve the performance ofyour web servers is to of fload your web page through caching. Y ou can write an application that lets you cache images, JavaScript, or even fullpages to provide a better experience to your users. Y ou can design n-tier and service-oriented architectures to scale each layer and module independently , which will help optimize performance. Y ou will learn more about this architecture pattern in Chapter 6 , Solution Ar chitectur e Design Patterns . Cr eating a hybrid cloud ar chitectur e The value of the cloud is growing, and many lar ge enterprises are moving their workload to the cloud. However , often, it's not possible to move entirely to the cloud in one day , and for most customers, this is a journey . Those customers seek a hybrid cloud model where they maintain a part ofthe application in an on-premise environment that needs to communicatewith the cloud module.  In a hybrid deployment, you need to establish connectivity between theresources running in the on-premises environment and the cloudenvironment. The most common method of hybrid deployment is betweenthe cloud and existing on-premises infrastructure to extend and grow anor ganization's infrastructure into the cloud while connecting cloud resources to the internal system. The common causes of setting up a hybrid cloud mayinclude the following: Y ou want to have operating legacy applications in an on-premise environment while you refactor and deploy in the cloud with a blue-green deployment model.A legacy application such as a mainframe may not have a compatiblecloud option and has to continue running on-premise.Need to keep part of the application on-premise due to compliancerequirements.T o speed up migration, keep the database on-premise and move the application server to the cloud.The customer wants to have more granular control of part of theapplication.Data ingestion in the cloud from on-premise for the cloud's  Extract, T ransform, Load  ( ETL ) pipeline. Cloud providers provide a mechanism for integrations between a customer'sexisting infrastructure and the cloud so that customers can easily use the cloud as a seamless extension to their current infrastructure investments.These hybrid architecture functionalities allow customers to do everythingfrom integrating networking, security , and access control to powering automated workload migrations and controlling the cloud from their on-premises infrastructure management tools.  T aking the example of A WS Cloud, you can establish a secure connection to A WS Cloud using a VPN. Since a VPN connection is set up over the internet, there may be latency issues due to multiple router hops from third-party internet providers. Y ou can have your fiber optics private line go to A WS Cloud for better latency using A WS Direct Connect. As shown in the following diagram, with A WS Direct Connect, you can establish high-speed connectivity between your data center and A WS Cloud to achieve a low latency hybrid deployment: H y b r i d c l o u d a r c h i t e c t u r e ( o n - p r e m i s e t o c l o u d c o n n e c t i v i t y ) As shown in the preceding diagram, A WS Dir ect Connect Location  establishes the connectivity between the on-premises data center to A WS Cloud. This helps you achieve the customer need of having dedicated fiber -optic lines to an A WS Direct Connect location; the customer can opt for this fiber optic line from a third-party vendor such as A T&T , V erizon, or Comcast in the USA. A WS has a directly connected partner in each region of the world. At the A WS Direct Connect location, the customer fiber optic line is connected to A WS private network, which provides dedicated end-to-end connectivity from the data center to A WS Cloud. These optic lines can provide speeds of up to 10 GB/s. T o secure traf fic over direct connect, you can set up a VPN, which will apply IPSec encryption to the traf fic flow . Designing a cloud-nativear chitectur e Y ou learned about the cloud-native approach earlier in this chapter from a migration point of view , where the focus was on refactoring and rearchitecting applications when migrating to the cloud. Each or ganization may have a dif ferent opinion on cloud-native architecture but, at the center of it, cloud-native is all about utilizing all the cloud capabilities in the bestway possible. T rue cloud-native architecture is about designing your application so that it can be built in the cloud from its foundations. Cloud-native doesn't mean hosting your application in the cloud platform;it's about leveraging services and features provided by the cloud. This mayinclude the following: Containerizing your monolithic architecture in a microservice andcreating a CI/CD pipeline for automated deployment.Building a serverless application with technology such as A WS Lambda Function as a Service ( FaaS ) and Amazon DynamoDB (a managed NoSQL database in the cloud).Creating a serverless data lake using Amazon S3 (managed ObjectStorage Service), A WS Glue (managed Spark Cluster for ETL), and Amazon Athena (managed Presto cluster for ad hoc queries).Using a cloud-native monitoring and logging service, for example,Amazon Cloud W atch. Using a cloud-native auditing service, for example, A WS CloudT rail. The following architecture is an example of a cloud-native serverlessarchitecture for a micro-blogging application: C l o u d - n a t i v e m i c r o - b l o g g i n g a p p l i c a t i o n a r c h i t e c t u r e The preceding diagram is utilizing cloud-native serverless services in A WS Cloud. Here, Amazon Route53, which manages the DNS service, is routinguser requests. Lambda manages function as a service to handle the code forUser V alidation , User Pr ofile , and Blog Page . All the blog assets are stored in Amazon S3, which manages object storage services and all userprofile data stored in Amazon DynamoDB, which is managed by theNoSQL store. As users send requests, A WS lambda validates the user and looks at their profile to ensure they have a subscription in Amazon DynamoDB; afterthat, it picks blog assets such as pictures, videos, and a static HTMLwriteup from Amazon S3 and displays them to the user . This architecture can be scaled in an unlimited manner as all services are cloud-nativemanaged services, and you are not handling any infrastructure. Crucialfactors such as high availability , disaster recovery , and scalability are taken care of by these cloud-native services so that you can focus on your featuredevelopment. In terms of cost, you will only pay if a request goes to a blogging application. If no one is browsing for the blog at night, you are notpaying anything for hosting your code; you are only paying nominal costfor storage. The benefit of the cloud-native architecture is that it enables fast-pacedinnovation and agility in the team. It simplifies building out a complexapplication and infrastructure. As system administrators and developers,you focus strictly on designing and building your networks, servers, filestorage, and other computing resources, and leave the physicalimplementation to your cloud computing provider . The cloud-native architecture provides several benefits: Fast scale-out, on-demand : Y ou can request the resources you need when you need them. Y ou only pay for what you use. Replicate quickly : Infrastructure-as-a-code means you can build once and replicate more. Instead of building your infrastructure by hand,you can structure it as a series of scripts or applications. Building yourinfrastructure programmatically gives you the ability to build andrebuild it on demand, when needed for development or testing.T ear up and tear down easily : In the cloud, services are provided on- demand, so it's easy to build up a lar ge experimental system. Y our system may include a cluster of scalable web and application servers,multiple databases, terabytes of capacity , workflow applications, and monitoring. Y ou can tear it all down as soon as the experiment is completed and save costs. There are many more examples in the area of storage, networking, andautomation for building the cloud-native architecture. Y ou will learn more about this architecture in Chapter 6 , Solution Ar chitectur e Design Patterns . Popular public cloud choices Since the cloud is becoming the norm, there are many cloud providers inthe market that provide cutting-edge technology platforms that arecompeting to get market share. The following are the major cloud providers(at the time of writing): Amazon W eb Services (A WS) : A WS is one of the oldest and lar gest cloud providers. A WS provides IT resources such as compute power , storage, databases, and other services on need basis over the internetwith a pay as you go model. A WS not only of fers IaaS; it has a broad range of of ferings in PaaS and SaaS. A WS provides multiple of ferings in cutting-edge technologies in the area of machine learning, artificialintelligence, blockchain, internet of things ( IoT ), and a comprehensive set of significant data capabilities. Y ou can almost host any workload in A WS and combined services to design an optimal solution. Micr osoft Azur e : Also known as Azure and like any cloud provider , it provides IT resources such as compute, network, storage, and databaseover the internet to their customers.Like A WS, Azure also provides IaaS, PaaS, and SaaS of ferings in the cloud, which include a range of services from the computer , mobile, storage, data management, content distribution network, container , big data, machine learning, and IoT . Also, Microsoft has wrapped its popular of ferings in the cloud through Microsoft Of fice, Microsoft Active Directory , Microsoft SharePoint, MS SQL Database, and so on. Google Cloud Platform (GCP) : GCP provides cloud of ferings in the area of computing, storage, networking, and machine learning. LikeA WS and Azure, they have a global network of data centers available as infrastructure as a service for their customers to consume ITresources over the internet. In compute, GCP of fers Google Cloud functions for the serverless environment, which you can compare withA WS Lambda functions in A WS and Azure Cloud functions in Azure. Similarly , GCP of fers multiple programming languages for application development with containers so that you can deploy applicationworkloads. There are many other cloud vendors available, such as Alibaba Cloud,Oracle Cloud, and IBM Cloud, but the major markets are captured by theaforementioned cloud providers. The choice of which cloud provider to useis up to the customer , which can be impacted by the availability of the functionality they are looking for or based on their existing relationshipwith providers. Sometimes, lar ge enterprises choose a multi-cloud strategy to utilize the best providers. Summary In this chapter , you learned how the cloud is becoming the most popular mainstream application hosting and development environment forenterprises. At the beginning of this chapter , you learned about cloud thinking and how it's related to solution architecture design. Since moreor ganizations are looking to move into the cloud, this chapter focused on various cloud migration strategies, techniques, and steps. Y ou learned about various cloud strategies, as per the nature of workload and migration priorities.   Migration strategies include the ability to rehost and replatform your application for Lift and Shift and takes the cloud-nativeapproach by refactoring and rearchitecting your application to takeadvantage of cloud-native capabilities. Y ou may find some unused inventory during application discovery and retire it. If you choose to notmigrate a certain workload then retain the application as is on-premises. Then, you learned about the steps involved in cloud migration, which helpyou discover your on-premise workload, analyze collected data, and createa plan to decide on which migration strategy to take. During the designphase, you create a detailed implementation plan and executed that duringthe migration steps, where you learned to set up connectivity with the cloudand move your application from on-premise to the cloud. After that, you learned about how to integrate, validate, and operate yourworkload into the cloud after migration and apply continuous optimizationfor cost, security , reliability , performance, and operational excellence. The hybrid cloud architecture is an integral part of the migration process, so youlearned about how to establish connectivity between on-premise and thecloud by looking at an architecture example of A WS Cloud.  At the end of this chapter , you learned about significant cloud providers and their of ferings. In the next chapter , you will dive deep and learn more about various architecture design patterns, along with the reference architecture. Y ou will learn about dif ferent architecture patterns such as multi-tier , service- oriented, serverless, and microservices.Further r eading T o learn more about the major public cloud providers, please refer to the following links: Amazon W eb Services (A WS) : https://aws.amazon.com Google Cloud Platform (GCP) : https://cloud.google.com Micr osoft Azur e : https://azure.microsoft.com Almost every cloud provider extends their learning credentials to new users,which means you can sign up with your email and try their of ferings out before you choose which one to go with. Solution Ar chitectur e Design Patterns There are multipl e ways to design a solution. A solution architect needs to take the right approach based on user requirements along with thearchitect ure constraint of cost, performance, scalability , and availability . In this chapter , you will learn about various solution architecture patterns along with reference architectures and how to apply them in real-worldscenarios. In the previous chapters, you learned about the attributes and principles ofsolution architecture design. This chapter is both exciting and essential asyou will be able to apply your learning to various architecture designpatterns. In this chapter , you will gain an understanding of some of the sign ificant solution archit ecture patterns, such as layered, event-driven, microservice, loosely coupled, service-oriented, and RESTful architectures. Y ou will learn the advantages of various architectural designs and examples that demonstrate when to utilize them. Y ou will also gain an understanding of architecture design anti-patterns in addition to the following architecturedesign patterns: Building an  n -tier layered architecture Creating multi-tenant SaaS-based architectureBuilding stateless and stateful architecture designsUnderstanding service-oriented ar chitectur e ( SOA ) Building serverless architectureCreating microservice architectureBuilding queue-based architectureCreating event-driven architecture Building cache-based architectureUnderstanding the circuit breaker pattern Implementing the bulkheads patternCreating a floating IP patternDeploying an application with a containerDatabase handling in application architectureA voiding anti-patterns in solution architecture By the end of the chapter , you will know how to optimize your solution architecture design and apply best practices, making this chapter the centerpoint and core of your learning. Building an n-tier layer ed ar chitectur e In n -tier architecture (also known as multi tier ar chitectur e ), you need to apply the principle of loosely coupled design (refer to Chapter 4 , Principles of Solution Ar chitectur e Design ) and attributes of scalability and elasticity (refer to Chapter 3 , Attributes of the Solution Ar chitectur e ). In multilayer architecture, you divide your product functions into multiple layers, such aspresentation, business, database, and services, so that each layer can beimplemented and scaled independently . W ith multitier architecture, it is easy to adopt new technologies and make development more ef ficient. This layered architecture provides flexibility to add new features in each layer without disturbing features of other layers. Interms of security , you can keep each layer secure and isolated from others, so if one layer gets compromised, the other layers won't be impacted.Application troubleshooting and management also become manageable asyou can quickly pinpoint where the issue is coming from and which part ofthe application needs to troubleshoot. The most famous architecture in multilayer design is thr ee-tier ar chitectur e , so let's learn more about it. The following diagram shows the most common architecture, where you interact with a web application fromthe browser and perform the required functions, for example, ordering yourfavorite T -shirt or reading a blog and leaving a comment: T h r e e - t i e r w e b s i t e a r c h i t e c t u r e In the preceding architecture, you have the following three layers: W eb Layer : The web layer is the user -facing part of the application. End users interact with the web layer to collect or provide information.Application Layer : The application layer mostly contains business logic and acts upon information received from the web layer . Database Layer : All kinds of user data and application data are stored in the database layer . Let's take a look at these layers in more detail. The web layer The web layer is also  known as the pr esentation tier . The web layer provides a user interface that helps the end user to interact with theapplication. The web layer is your user interface (in this case, the websitepage), where the user enters information or browses for it. W eb developers may build a presentation tier user interface in technologies such as HTML,CSS, AngularJS, Java Server Page ( JSP ), and Active Server Page ( ASP ). This tier collects the information from the user and passes it to theapplication layer . The application layer The application layer is also known as a logic tier , as this is the core of the product where all the business logic resides. The presentation tier collectsthe information from the user and passes it to the logic tier to process andget a result. For example, on an e-commerce website such as Amazon.com,users can enter a date range on the order page of the website to find theirorder summary . In return, the web layer passes the data range information to the application layer . The application layer processes the user input to perform business logic such as the count of orders, the sum of amounts, andthe number of items purchased. This returns information to the web layer torender it for the user . Generally , in three-tier architecture, all algorithms and complex logic live in the logic tier , which includes creating a recommendation engine or showing personalized pages to the user as per their browsing history . Developers may c hoose to implement this layer in a server -side programming language, for example, C++, Java, .NET , or Node.js. The application layer performs logic on the data, which is sto red in the database layer . The database layer The database layer , which is also known as the data tier , stores all the information related to user profiles and transactions. Essentially , it contains any data that needs to persist in being stored in the data tier . This information is sent back to the application layer for logic processing andthen, eventually , it renders to the user in the web layer . For example, if the user is logged in to a website with their user ID and password, then theapplication layer verifies the user credentials with information stored in thedatabase. If the credentials match the stored information, the user is allowedto log in and access the authorized area of the website. The architect may choose to build a data tier in relational databases, forexample,  PostgreSQL ,  MariaDB , Oracle Database , MySQL ,  Microsoft SQL Server , Amazon Aurora, or Amazon RDS. The architect may also add a  NoSQL database such as  Amazon DynamoDB ,  MongoDB , or  Apache Cassandra . The data tier is not only used to store transaction information but also to hold user -session information and application configuration. In the n-tier layered architecture diagram, you will notice that each layerhas its own Auto Scaling configuration, which means it can be scaledindependently . Also, each layer has a network boundary , which means having access to one layer doesn't allow access to other layers. Y ou will learn more about security considerations in Chapter 8 , Security Considerations . When designing multitier architecture, you need to take carefulconsideration of how many layers should be added to your design. Eachlayer requires its own fleet of servers and network configurations. So,adding more layers means increasing the cost and management overhead,whereas keeping fewer layers means creating a tightly coupled architecture.The architect needs to decide the number of tiers based on applicationcomplexity and user requirement. Cr eating multi-tenant SaaS-based ar chitectur e In the previous section, you learned about multitier architecture; however , the same architecture that is built for a single or ganization is also called a  single tenancy . Multi-tenant architecture is becoming more popular as or ganizations adopt the digital revolution while keeping the overall application cost low . The Softwar e-as-a-Service ( SaaS ) model is built on a multi-tenant architecture, where a single instance of the software andsupporting infrastructure serve multiple customers. In this design, eachcustomer shares the application and database, with each tenant isolated bytheir unique configuration, identity , and data. They remain invisible to each other while sharing the same product. As multi-tenant SaaS providers own everything from the hardware to thesoftware, SaaS-based products of fload an or ganization's responsibilities to the maintenance and updates of the application, as this is taken care by theSaaS provider . Each customer (tenant) can customize their user interface using a configuration without any code changes. As multiple customersshare a common infrastructure, they get the benefit of scale, which lowersthe cost further . Some of the most popular SaaS providers are Salesforce CRM, the JIRA bug tracking tool, and Amazon QuickSight. As shown in the following architecture diagram, there are two or ganizations (tenants) using the same software and infrastructure. The SaaS vendorprovides access to the application layer by allocating a unique tenant ID toeach or ganization. Each tenant can customize their user interface as per their business needs using a simple configuration: M u l t i - t e n a n t S a a S a r c h i t e c t u r e As shown in the preceding architecture design, the presentation layerprovides a user interface and the application layer holds the business logic.At the data access layer , each tenant will have data level isolation with one of the following methods: Database Level Isolation : In this model, each tenant has its database associated with its tenant ID. When each tenant queries data from theuser interface, they are redirected to their database. This model isrequired if the customer doesn't want a single shared database forcompliance and security reasons.T able Level Isolation : This isolation level can be achieved by providing a separate table for each tenant. In this model, tables need tobe uniquely assigned to each tenant, for example, with the tenant IDprefix. When each tenant queries data from the user interface, they areredirected to their tables as per their unique identifier . Row Level Isolation : In this isolation level, all tenants share the same table in a database. There is an additional column in a table where aunique tenant ID is stored against each row . When an individual tenant wants to access their data from the user interface, the data access layerof the application formulates a query based on the tenant ID to theshared table. Each tenant gets a row that belongs to their users only . For enterprise customers, a careful assessment of whether a SaaS solution isthe right fit for them based on their unique feature's requirements. This isbecause, often, a SaaS model has limited customization capabilities.Additionally , we need to find the cost value proposition if a vast number of users need to subscribe. The cost comparison should be calculated based onthe total cost of ownership when taking a build versus buy decision. This is because building software is not the primary business of most or ganizations, so the SaaS model is becoming highly popular as or ganizations can focus on their business and let the experts handle the IT side of it. Building stateless and statefular chitectur e designs While designing a complex application such as an e-commerce website, youneed to handle the user state to maintain activity flow , where users may be performing a chain of activities such as adding to the cart, placing an order , selecting a shipping method, and making a payment. Currently , users can use various channels to access an application, so there is a high possibility thatthey will be switching between devices; for example, adding items in thecart from their mobile and then completing checkout and payment from alaptop. In this situation, you would want to persist user activity acrossthe devices and maintain their state until the transaction is complete.Therefore, your architecture design and application implementation need toplan for user session management in order to fulfill this requirement. T o persist user states and make applications stateless, user -session information needs to be stored in persistent database layers such as theNoSQL database. This state can share between multiple web servers ormicroservices. T raditionally , a monolithic application uses stateful architecture, where user -session information is stored in the server itself rather than via any external persistence database storage. The session storage mechanism is the main dif ference between stateless and stateful application designs. Since session information in a statefulapplication is local to the server , it cannot be shared between other servers and also doesn't support modern microservice architecture. Y ou will learn more about microservice-based architecture in the Cr eating micr oservice ar chitectur e section. Often, a stateful application doesn't support horizontal scaling very well, asthe application state persists in the server which cannot be replaced. Thestateful application worked well early on when the user base was not veryhuge. However , as the internet becomes more popular , it is reasonable to assume that you will have millions of users active on a web application.Therefore, ef ficient horizontal scaling is important to handle such a lar ge user base and achieve low application  latency . In a stateful application, state information is handled by the server , so once users establish a connection with a particular server , they have to stick with it until the transaction completes. Y ou can put a load balancer in front of the stateful application, but to do that, you have to enable sticky sessions in aload balancer . The load balancer has to route user requests to one server , where session information has been established. Enabling stickysessions violates the load balancer's default round-robin request for thedistribution method. Other issues may include lots of open connections tothe server as you need to implement a session timeout for the client. Y our design approach should focus more on the shared session state using the stateless method, as it allows horizontal scaling. The following diagramshows an architecture that depicts a stateless application for a webapplication: A s t a t e l e s s a p p l i c a t i o n a r c h i t e c t u r e The preceding architecture diagram is a three-tier architecture with a web,application, and database layer . T o make applications loosely coupled and scalable, all user sessions are stored persistently in the NoSQL database, for example, Amazon DynamoDB. Y ou should use client-side storage, such as cookies, for the session ID. This architecture lets you use the scale-outpattern without having to worry about a loss of user state information. Astateless architecture removes the overhead to create and maintain usersessions and allows consistency across the application's modules. A statelessapplication has performance benefits too, as it reduces memory usage fromthe server -side end and eliminates the session timeout issue. Adopting a stateless pattern can complicate tasks; however , with the right approach, you can achieve a rewarding experience for your user base. Y ou can develop applications using the microservice approach with REST designpatterns and deploy them in containers. For this, use authentication andauthorization to connect users to the server . Y ou will learn more about the REST design pattern and microservices in the next section. As access to state information from multiple web serversfocuses on a single location, you must use caution to prevent theperformance of the data store from becoming a bottleneck. Understanding SOA In SOA patterns, dif ferent application components interact with each other using a communication protocol over the network. Each service providesend-to-end functionality , for example,  fetching an or der history . SOA is widely adopted by lar ge systems to integrate business processes, for example, taking your payment service from the main application andputting it as a separate solution. In a general sense, SOAs take monolithic applications and spread some ofthose operations out into individual services that operate independently to each other . The goal of using an SOA is to loosen the coupling of your application's services. Sometimes, an SOA includes not just splitting services apart from oneanother but splitting resources into separate instances of that service. Forinstance, while some choose to store all of their company's data in a singledatabase split by tables, an SOA would consider modularizing theapplication by function into separate databases altogether . This allows you to scale and manage throughput based on the individual needs of tables foreach database. SOA has multiple benefits, for example, the parallelization of development,deployment, and operation. It decouples the service so that you canoptimize and scale each service individually . However , it also requires more robust governance to ensure work performed by each service's team meetsthe same standard. W ith SOA, the solution could become complex enough to increase the overhead to balance that, so you need to make the rightchoice of tools and automation of service monitoring, deployment, andscaling. There are multiple ways in which to implement SOA architecture. Here,you will learn about the  Simple Object Access Pr otocol  ( SOAP ) web service architecture and Repr esentational State T ransfer  ( REST ) web service architecture. Originally , SOAP was the most popular messaging protocol, but it is a bit heavyweight as it entirely relies on XML for data interchange. Now , REST architecture is becoming more popular as developers need to build morelightweight mobile and web applications. Let's learn about botharchitectures and their dif ferences in more detail. SOAP web service ar chitectur e SOAP is a messaging protocol that is used to exchange data in a distributed environment in XML format. SOAP is a standard XML where data istransported in an envelope format called SOAP Envelope , as shown in the following diagram: S O A P E n v e l o p e f o r w e b s e r v i c e d a t a e x c h a n g e As shown in the preceding diagram, SOAP Envelope contains two parts: SOAP Header : The SOAP header provides information on how a recipient of a SOAP message should process it. It containsauthorization information to deliver the message to the right recipientand for data encoding. Message Body : The message body contains the actual message in W eb Services Description Language  ( WSDL ) specification. WSDL is an XML format file that describes the Application Pr ogramming Interface ( API ) contract with the message structure, API operations, and server Unique Resour ce Locator ( URL ) address. Using a WSDL service, a client application can determine where a service is beinghosted and what functionality it can perform. The following code shows an example of a SOAP envelope XML. Here, youcan see both the header and message wrap up under the SOAP envelope: <env:Envelope xmlns:env=\"http://www.w3.org/2003/05/soap-envelope\"> <env:Header> <n:orderinfo xmlns:n=\"http://exampleorder.org/orderinfo\"> <n:priority>1</n:priority> <n:expires>2019-06-30T16:00:00-09:00</n:expires> </n:orderinfo> </env:Header> <env:Body> <m:order xmlns:m=\"http://exampleorder.org/orderinfo\"> <m:getorderinfo> <m:orderno>12345</m:oderno> </m:getorderinfo> </m:order> </env:Body> SOAP commonly uses HTTP , but other protocols such as SMTP can be used. The following diagram shows details about a message exchange in a webservice using SOAP . Here, the web service client sends the request to the service provider who hosts the web service, and receives the response withthe desired result: S O A P - b a s e d w e b s e r v i c e In the preceding diagram, the client is an e-commerce website user interface.The user wants their order information and hence sends the SOAP messagein the XML-format to the application server with the order number . The application server hosts the order service, which then responds with thecustomer's order details. In a SOAP-based web service, the service provider creates an API contractin the form of WSDL. WSDL lists all of the operations that web services canperform, such as providing order information, updating orders, deletingorders, and more. The service provider shares WSDL with web service clientteam, using which the client generates an acceptable message format, sendsdata to the service provider , and gets the desired response. The web service client fills the values in the generated XML message and sends it across tothe service provider with authentication details for processing. The implementation of a SOAP-based web service has a higher complexityand requires more bandwidth, which can impact web applicationperformance, such as page loading time, and any major changes in the serverlogic require all clients to update their code. REST was created to address SOAP-based web service problems and provide a more flexible architecture.Let's learn more about RESTful architecture and why it is becoming widelypopular .RESTful web service ar chitectur e A RESTful web service of fers better performance due to its lightweight architecture. It allows dif ferent messaging formats such as JSON, plaintext, HTML, and XML, compared to SOAP , which only allows XML. REST is an architecture style that defines the standard for loosely coupledapplication design using the HTTP protocol for data transmission. JavaScript Object Notation ( JSON ) is a more accessible format for data exchange in REST architecture. JSON is lightweight and languageindependent. JSON contains a simple key-value pair that makes itcompatible with data structures defined in most programming languages. REST focuses on the design principle for creating a stateless service. Theweb service client doesn't need to generate a complex client skeleton, but itcan access the web server resources using the unique Uniform Resour ce Identifier  ( URI ). The client can access RESTful resources with the HTTP protocol and perform standard operations such as GET , PUT , DELETE, and POST on the resources. Y our choice of architecture design between REST and SOAP depends upon your or ganization's needs. The REST service of fers an ef fective way to integrate with lightweight clients such as smartphones, while SOAPprovides high security and is suitable for complex transactions. Let's take alook at the dif ferences between REST and SOAP: Attributes REST SOAP Design Architectural style with aninformal guideline  Predefined rules with astandard protocol MessageFormat JSON, Y AML, XML, HTML, plaintext, and CSV XML Protocol HTTP HTTP , SMTP , and UPD SessionState Default stateless Default stateful Security HTTPS and SSL W eb Services Security and ACID compliance   Cache Cached API calls Cannot cache API calls Performance Fast with fewer resources Needs more bandwidthand compute power   Now , let's learn about a reference architecture based on service-oriented design. Building an SOA-based e-commer ce website ar chitectur e An e-commerce website such as Amazon.com has users from all parts of theworld and a huge catalog with millions of products. Each product hasmultiple images, reviews, and videos. Maintaining such a big catalog for aglobal user base is a very challenging task. This reference architecture follows SOA principles. Each service isoperating as independently as possible from each other . This architecture can be implemented using either SOAP-based or RESTful-based webarchitecture: E - c o m m e r c e w e b s i t e S O A a r c h i t e c t u r e   As shown in the preceding architecture diagram, we can take note of thefollowing: When a user types a website address into the browser , the user request reaches out to the DNS server to load the website. The DNS requestsfor the website are routed by Amazon Route 53 to the server where theweb applications are being hosted. The user base is global, and users continue browsing for products topurchase as the website has a lar ge product catalog with static images and videos. A content distribution network such as Amazon CloudFrontcaches and delivers static assets to users.The catalog contents, such as static product images and videos, alongwith other application data, such as log files, are stored in Amazon S3.Users will browse the website from multiple devices, for example, theywill add items in a cart from their mobile and then make a payment on adesktop. T o handle user sessions, a persistent session store is required such as DynamoDB. In fact, DynamoDB is a NoSQL database whereyou don't need to provide a fixed schema, so it is a great storage optionfor product catalogs and attributes.T o provide high performance and reduce latency , Amazon ElastiCache is used as a caching layer for the product to reduce read andwrite operations on the database.A convenient search feature is key for product sales and businesssuccess. Amazon Cloud Search helps to build scalable search capabilityby loading the product catalog from DynamoDB.A recommendation can encourage a user to buy additional productsbased on their browsing history and past purchases. A separaterecommendation service can consume the log data stored on AmazonS3 and provide potential product recommendations to the user . The e-commerce application can also have multiple layers andcomponents that require frequent deployment. A WS Elastic Beanstalk handles the auto-provisioning of the infrastructure, deploys theapplication, handles the load by applying Auto Scaling, and monitorsthe application. In this section, you learned about SOA along with an architecture overview . Let's learn more about the critical aspect of modern architecture design witha serverless architecture. Building serverless ar chitectur e In a traditional scenario, if you want to develop an application, you need tohave a server where your desired operating system and required software canbe installed. While you are writing your code, you need to make sure thatyour server is up and running. During deployment, you need to add moreservers to keep up with user demand and add scaling mechanisms such asAuto Scaling to manage the desired number of servers to fulfill users'requests. In this entire situation, a lot of ef fort goes into infrastructure management and maintenance, which has nothing to do with your businessproblem. Going serverless gives you the ability to focus on your application and writecode for feature implementation without worrying about underlyinginfrastructure maintenance. Serverless means there is no server to run yourcode, which frees you up from Auto Scaling and decoupling overheads whileproviding a low-cost model. A public cloud, such as A WS, provides several serverless services in the area of computer and data storage, which makes it easier to develop an end-to-end serverless application. When you talk about serverless, the first thingthat comes to mind is A WS Lambda functions, which is a  Function as a Service ( FaaS ) and is provided by A WS cloud. T o make your application service-oriented, Amazon API Gateway of fers you the ability to put RESTful endpoints in front of your A WS Lambda functions and helps you to expose them as microservices. Amazon DynamoDB provides a highlyscalable NoSQL database, which is an entirely serverless NoSQL data store,and Amazon Simple Storage Service  ( S3 ) provides serverless object data storage. Let's take a look at an example of a reference serverless architecture in thefollowing architecture diagram for the delivery of a secure survey: S e r v e r l e s s a r c h i t e c t u r e f o r a s e c u r e s u r v e y d e l i v e r y In this example, a serverless architecture serves, delivers, and processessecure surveys, all on managed services: 1 . First, a customer requests the website over HTTPS. The web page isserved directly from Amazon S3. 2 . The customer's survey is submitted via an AJAX call to Amazon APIGateway . 3 . Amazon API Gateway logs this to Amazon CloudT rail. If a survey's results are lost, or if one of the AJAX calls includes malicious activityof some sort, these logs may be helpful in identifying and fixing theproblem. 4 . Amazon API Gateway then turns the AJAX call into an event triggerfor an A WS Lambda function, which pulls the survey data and processes it. 5 . The results of the survey are sent by the A WS Lambda function to an Amazon S3 bucket, where they are secured with server -side encryption . 6 . Metadata from the survey , which does not include any personally identifiable information, is then written and stored in a DynamoDBtable. This could be used for later queries and analysis. Due to the increasing popularity of serverless architecture, you will see morereference architectures using serverless services as we move forward with this book. The concept of microservices is also becoming popular with theadoption of RESTful-style architectures. Let's learn more about RESTarchitectures and microservices in the next sections.Cr eating micr oservice ar chitectur e Microservices are architected in REST -style web services and are independently scalable. This makes it easier to expand or shrink therelevant components of your system while leaving the rest untouched. Asystem that employs microservices can more easily withstand incidentswhere application availability can degrade gracefully to avoid anycascading failures. Y our system becomes fault-tolerant, that is, built with failure in mind. The clear advantage of microservices is that you have to maintain a smallersurface area of code. Microservices should always be  independent . Y ou can build each service with no external dependencies where all prerequisites areincluded, which reduces the inter -dependency between application modules and enables loose coupling. The other overarching concept of microservices is bounded contexts ,   which are the blocks that combine together to make a single business domain. A business domain could be something like carmanufacturing, bookselling, or social network interactions that involve acomplete business process. An individual microservice defines boundariesin which all the details are encapsulated. Scaling each service is essential while dealing with the lar ge-scale access of applications, where dif ferent workloads have dif ferent scaling demands. Let's learn about some best practices for designing microservicearchitecture: Cr eate a separate data stor e : Adopting a separate data store for each microservice allows the individual team to choose a database thatworks best for their service. For example, the team that handleswebsite traf fic can use a very scalable NoSQL database to store semi- structured data. The team handling order services can use a relationaldatabase to ensure data integrity and the consistency of transactions. This also helps to achieve loose coupling where changes in onedatabase do not impact other services.Keep servers stateless : As you learned in the previous section, Building stateless and stateful ar chitectur e designs , keeping your server stateless helps in scaling. Servers should be able to go down andbe replaced easily , with minimal or no need for storing state on the servers.Cr eate a separate build : Creating a separate build for each microservice makes it easier for the development team to introducenew changes and improve the agility of the new feature release. Thishelps to make sure that the development team is only building codethat is required for a particular microservice and not impacting otherservices.Deploy in a container : Deploying in a container gives you the tool to deploy everything in the same standard way . Y ou can choose to deploy all microservices in the same way regardless of their nature usingcontainers. Y ou will learn more about container deployment in the, Deploying an application with a container section. Blue-gr een deployment : The better approach is to create a copy of the production environment. Deploy the new feature and route a smallpercentage of the user traf fic to make sure the new feature is working as per expectation in a new environment. After that, increase the traf fic in the new environment until the entire user base is able to see the newfeature. Y ou will learn more about blue-green deployment in Chapter 12 , DevOps and Solution Ar chitectur e Framework . Monitor your envir onment : Good monitoring is the dif ference between reacting to an outage and proactively preventing an outagewith proper rerouting, scaling, and managed degradation. T o prevent any application downtime, you want services to of fer and push their health status to the monitoring layer , because who knows better about status than the service itself! Monitoring can be done in many ways,such as plugins, or by writing to a monitoring API. As you learned about the various advantages of microservice in this section,let's take a look at a microservice-based reference architecture for a real-time voting application. Real-time voting applicationr efer ence ar chitectur e A microservice-based architecture is illustrated in the following diagram,representing a real-time voting application, where small microservicesprocess and consolidate user votes. The voting application collectsindividual user votes from each mobile device and stores all the votes in aNoSQL-based Amazon DynamoDB database. Finally , there is application logic in the A WS Lambda function, which aggregates all of the voting data cast by users to their favorite actor and returns the final results: M i c r o s e r v i c e b a s e d r e a l - t i m e v o t i n g a p p l i c a t i o n a r c h i t e c t u r e In the preceding architecture, the following things are happening: 1 . Users text a vote to a phone number or shortcode provided by a thirdparty such as  T wilio . 2 . The third party is configured to send the content of the message to anendpoint created by Amazon API Gateway , which then forwards the response to a function built in A WS Lambda. 3 . This function extracts the vote from the message content and writes theresult and any metadata into a table in Amazon DynamoDB. 4 . This table has DynamoDB Streams enabled, which allows you to trackchanges to your tables on a rolling basis. 5 . After the update, DynamoDB Streams notifies a second A WS Lambda function, which has the application logic to aggregate the votes (toevery second) and writes them back to another DynamoDB table. Thesecond table only stores the sum of the votes for each category . 6 . A dashboard to display a summary of votes is created using HTML andJavaScript and hosted as a static website in Amazon S3. This page usesthe A WS JavaScript SDK to query the aggregate Amazon DynamoDB table and display the voting results in real time. 7 . Finally , Amazon Route 53 is used as a DNS provider to create a hosted zone pointing a custom domain name to the Amazon S3 bucket. This architecture is not only microservice-based but also serverless. Usingmicroservices, you can create applications made of small independentcomponents, which constitute smaller parts to iterate. Microservice-basedarchitecture means that the cost, size, and risk of change reduces, increasingthe rate of change. Message queues play a vital role to achieve accurate loose coupling and helpto avoid application throttling. A queue allows secure and reliablecommunication between components. Let's learn more about queue-basedarchitecture in the next section. Building queue-based ar chitectur e In the previous section, you learned about microservice design usingRESTful architecture. The RESTful architecture helps your microservice tobe easily discoverable, but what happens if your service goes down? This isa contemporary architecture, where your client service waits for a responsefrom the host service, which means that the HTTP request blocks the API.Sometimes, your information may be lost due to the unavailability of adownstream service. In such cases, you must implement some retry logic inorder to retain your information. A queue-based architecture provides a solution to this problem by addingmessage queues between services, which holds information on behalf ofservices. The queue-based architecture provides fully asynchronouscommunication and a loosely coupled architecture. In a queue-basedarchitecture, your information is still available in the message. If a servicecrashes, the message can get the process as soon as the service becomesavailable. Let's learn some terminology of a queue-based architecture: Message : A message can have two parts—the header and the body . The header contains metadata about the message, while the bodycontains the actual message.Queue : The queue holds the messages that can be used when required. Pr oducer : A service that produces a message and publishes it to the queue.Consumer : A service that consumes and utilizes the message. Message br oker:  Helps to gather , route, and distribute messages between the producer and consumer . Let's learn about some typical queue-based architecture patterns to get anidea of how they work. Queuing chain pattern A queuing chain pattern is applied when sequential processing needs to runon multiple systems that are linked together . Let's understand the queuing chain pattern using the example of an image-processing application. In an image-processing pipeline, sequential operations of capturing the image andstoring it on a server , running a job to create dif ferent-resolution copies of the image, watermarking the image, and thumbnail generation are tightlylinked to each other . A failure in one part can cause the entire operation to be disrupted. Y ou can use queues between various systems and jobs to remove a single point of failure and design true loosely coupled systems. The queuing chainpattern helps you to link dif ferent systems together and increase the number of servers that can process the messages in parallel. If there is no image toprocess, you can configure Auto Scaling to terminate the excess servers. The following diagram shows a queuing chain pattern architecture. Here, thequeue provided by A WS is called Amazon Simple Queue Service ( SQS ):   Q u e u i n g c h a i n p a t t e r n a r c h i t e c t u r e The preceding architecture has the following steps: 1 . As soon as the raw image is uploaded to the server , the application needs to watermark all of the images with the company's logo. Here, afleet of Amazon EC2 servers is running batch jobs to watermark all theimages and push the processed image into the Amazon SQS queue. 2 . The second fleet of Amazon EC2 servers pulls the watermarked imagesfrom the Amazon SQS queue. 3 . The second fleet of EC2 workers has a job to process the image andcreate multiple variations with dif ferent resolutions. 4 . After encoding the images, the EC2 workers push the message intoanother Amazon SQS queue. 5 . As the image is processed, the job deletes the message from theprevious queue to make up space. 6 . The final fleet of EC2 servers gets encoded messages from the queueand creates thumbnails along with the copyright. The benefits of this architecture are as follows: Y ou can use loosely coupled asynchronous processing to return responses quickly without waiting for another service acknowledgment.Y ou can structure the system through the loose coupling of Amazon EC2 instances using Amazon SQS.Even if the Amazon EC2 instance fails, a message remains in the queueservice. This enables processing to be continued upon recovery of theserver and creates a system that is robust to failure. Y ou may get fluctuations in application demand that can cause unexpected message loads. Automating your workload as per the queue message loadwill help you to handle any fluctuations. Let's learn more about the job observer pattern to handle such automation next. Job observer pattern In the job observer pattern, you can create an Auto Scaling group, basedupon number of messages in the queue to process. The job observer patternhelps you to maintain performance through increasing or decreasing thenumber of server instances used in job processing. The following diagram depicts the job observer pattern: J o b o b s e r v e r p a t t e r n a r c h i t e c t u r e In the preceding architecture, the first fleet of Amazon EC2 servers is onthe left-hand side, running batch jobs and putting messages in the queue, forexample, image metadata. The second fleet of EC2 servers on the right-hand side is consuming and processing those messages, for example, imageencoding. As the message reaches a certain threshold, Amazon CloudW atch triggers Auto Scaling to add the additional server in the consumer fleet tospeed up the job processing. Auto Scaling also removes additional serverswhen the queue depth goes below the threshold. The job observer pattern computes scale with job size, providing ef ficiency and cost savings. The job observer pattern architecture allows the job to becompleted in a shorter time frame. The process is resilient, which means jobprocessing doesn't stop if a server fails. While queue-based architecture provides loose coupling, it works mostly onthe Asynchr onous Pull method, where the consumer can pull the message from the queue as per their availability . Often, you need to drive communication between various architecture components where one eventshould trigger other events. Let's learn more about event-driven architecturein the next section. Cr eating event-driven ar chitectur e Event-driven architecture helps you to chain a series of events together tocomplete a feature flow . For example, when you are making a payment to buy something on a website, you are expecting to get your order invoicegenerated and to get an email as soon as the payment is complete. Event-driven architecture helps to rope in all of these events so that making apayment can trigger another task to complete the order flow . Often, you will see message queues,  which you learned about in the previous section, as the central point while talking about event-driven architecture. Event-drivenarchitecture can also be based on the publisher/subscriber model or theevent stream model. Publisher/subscriber model In the pub/sub model, when an event is published, a notification is sent to allsubscribers, and each subscriber can take the necessary action as per theirrequirements of data processing. Let's take an example of a photostudio application, which enriches a photo with dif ferent filters and sends a notification to the user . The following architecture depicts a pub/sub model:   P h o t o s t u d i o a p p l i c a t i o n p u b / s u b - e v e n t - d r i v e n a r c h i t e c t u r e In the preceding diagram, you will notice the following things: 1 . The user first uploads the picture to an  Amazon S3  bucket using a web/mobile application. 2 . The Amazon S3 bucket then sends a notification to Amazon Simple Notification Service  ( SNS ). Amazon SNS is a message topic with the following subscribers: 1 . Here, the first subscriber is using the email service, and as soon asthe photo upload is complete, an email is sent to the user . 2 . The second subscriber is using  Amazon SQS  queue, which gets the message from the  Amazon SNS topic and applies various filters in code written in A WS Lambda to improve the image quality . 3 . The third subscriber is using the direct A WS Lambda function, which creates the image thumbnail. In this architecture, Amazon S3 publishes the message to the SNS topic as aproducer which is consumed by multiple subscribers. Additionally , as soon as the message comes to SQS, it triggers an event for the Lambda function toprocess images. Event str eam model In the event stream model, the consumer can read from the continuous flowof events coming from the producer . For example, you can use the event stream to capture the continuous flow of a clickstream log and also send analert if there are any anomalies detected, as shown in the followingarchitecture diagram: C l i c k s t r e a m a n a l y s i s e v e n t s t r e a m a r c h i t e c t u r e Amazon Kinesis is a service that is used to ingest, process, and storecontinuous streaming data. In the preceding diagram, various customersclicking on e-commerce applications from web and mobile applicationsproduce a stream of click events. These clickstreams are sent to analyticsapplications using Amazon API Gateway for real-time analytics. In this analytics application, Kinesis Data Analytics calculates Conversion Rates over a certain period of time, for example, the number of people that endedup making a purchase in the last five minutes. After aggregating data in realtime, Amazon Kinesis Data Analytics sends the results to Amazon Kinesis Data Fir ehose , which stores all the data files in Amazon S3 storage for further processing as needed. A Lambda function reads from  the event stream   and starts examining the data for Anomalies . As anomalies in the conversion rates are detected, the A WS Lambda function sends a notification on email for the campaign team to be notified. In this architecture, the event stream isoccurring continuously , and A WS Lambda is reading from the stream for a specific event.  Y ou should use event-driven architecture to decouple the producer and consumer and keep the architecture extendable so that a new consumer canbe integrated at any time. This provides a highly scalable and distributedsystem with each subsystem having an independent view of events.However , you need to apply a mechanism to avoid duplicate processing and error message handling. T o achieve good application performance, caching is an important factor and it can be applied at every architecture layer and in pretty much anyarchitecture component. Let's learn more about cache-based architecture inthe next section. Building cache-based ar chitectur e Caching is the process of temporarily storing data or files in an intermediarylocation between the requester and the permanent storage, for the purpose ofmaking future requests faster and reducing network throughput. Cachingincreases the application speed and lowers the cost. It allows you to reusepreviously retrieved data. T o increase application performance, caching can be applied at various layers of the architecture such as the web layer , application layer , data layer , and network layer . Normally , the server's random access memory ( RAM ) and in-memory cache engines are utilized to support application caching. However , if caching is coupled to a local server , then the cache will not be persisting data, in case of a server crash. Now , most of the applications are in a distributed environment, so it's better to have a dedicated caching layer thatshould be independent of the application life cycle. When you appliedhorizontal scaling to your application, all servers should be able to access thecentralized caching layer to achieve the best performance. The following diagram depicts the mechanism of caching in various layersof solution architecture: C a c h i n g a t t h e a r c h i t e c t u r e l a y e r s As shown in the preceding diagram, the following are the cachingmechanisms at each layer of architecture: Client side : Client-side caching is applied to user devices such as mobile and desktop. This caches the previously visited web content torespond faster to a subsequent request. Each browser has its owncaching mechanism. HTTP caching makes the application faster bycaching content at the local browser . The c ache-control HTTP header defines  browser caching policies for both client request and server response. These policies define where the content should be cached andhow long it will persist, which is known as T ime to Live  ( TTL ). Cookies are another method used to store information at the clientmachine in order to respond to the browser faster . DNS cache : When a user types the website address over the internet, the public Domain Name System   ( DNS ) server looks up the IP address. Caching this DNS resolution information will reduce thewebsite's load time. DNS can be cached to a local server or browserafter the first request and any further request to that website will befaster . W eb caching : Much of the request involves retrieving web content such as images, video, and HTML pages. Caching these assets near tothe user location can provide a much faster response for a page load.This also eliminates disk read and server load time. A content distribution network ( CDN ) provides a network of edge locations where static content such as high-resolution images and videos can becached. It's very useful for reading heavy applications such as games,blogs, e-commerce product catalog pages, and more. The usersession contains lots of information regarding user preference and theirstate. It provides a great user experience to store the user's session in itsown key-value store and applies to cache for quick user response.Application caching : At the application layer , caching can be applied to store the result of a complex repeated request to avoid business logiccalculations and database hits. Overall, it improves applicationperformance and reduces the load on the database and infrastructure.Database caching : Application performance highly depends upon speed and throughput provided by the database. Database cachingallows you to increase database throughput significantly and lower dataretrieval latency . A database cache can be applied in front of any kind of relational or non-relational database. Some database providersintegrate caching, while applications handle local caching. Redis and Memcached are the most popular  caching   engines. While Memcached is faster (it is good for low-structure data and stores data in akey-value format), Redis is a more persistent caching engine and is capable of handling complex data structures required for an application such as agaming leader board. Let's learn about a few more caching design patterns.Cache distribution pattern in athr ee-tier web ar chitectur e T raditional web hosting architecture implements a standard three-tier web application model that separates the architecture into the presentation,application, and persistence layers. As shown in the following architecturediagram, caching is applied at the presentation, persistence, and applicationlayers: C a c h e d i s t r i b u t i o n p a t t e r n a r c h i t e c t u r e One of the ways you can of fload your web page is through caching. In caching patterns, your goal is to try to hit the backend as little as possible.Y ou can write an application where you can cache images, JavaScript, or even full pages to p rovide a better experience for  your users. As shown in the preceding diagram, caching is applied to the various layers ofarchitecture: Amazon Route 53 provides DNS services to simplify domain management and to help cache DNS-to-IP mapping. Amazon S3 stores all static content such as high-resolution images and videos.Amazon CloudFr ont provides edge caching for high-volume content. It also uses these cache-control headers to determine how frequently itneeds to check the origin for an updated version of that file.Amazon DynamoDB is used for session stores in which web applications cache to handle user sessions.Elastic Load Balancing spreads traf fic to web server Auto Scaling groups in this diagram.Amazon ElastiCache provides caching services for the app, which removes the load from the database tier . In general, you only cache static content; however , dynamic or unique content af fects the performance of your application. Depending on the demand, you might still get some performance gain by caching the dynamicor unique content. Let's take a look at a more specific pattern. Rename distribution pattern When using a content distribution network ( CDN ) such as Amazon CloudFront, you store frequently used data in an edge location near to theuser for fast performance. Often, you set up TTL in CDN for your data,which means the edge location will not query back to the server for updateddata until TTL expires. Y ou may have situations where you need to update CDN cached content immediately , for example, if you need to correct the wrong product description. In such a situation, you can't wait for the file's TTL to expire. The rename distribution pattern helps you to update the cache as soon as new changes arepublished so that the user can get updated information immediately . The following diagram shows the r ename distribution pattern : R e n a m e d i s t r i b u t i o n p a t t e r n a r c h i t e c t u r e As shown in the preceding diagram, using the rename distribution patternwith the cache distribution pattern helps to solve the update issue. W ith this pattern, instead of overwriting the file in the origin server and waiting for theTTL in CloudFront to expire, the server uploads the updated file with a newfilename and then updates the web page with the new URL. When the userrequests original content, CloudFront has to fetch it from the origin and can'tserve the obsolete file that's already cached. Y ou may not always want to use a CDN; instead, you can use the proxy cache server . Let's learn more about it in the next section. Cache pr oxy pattern Y ou can increase your application performance significantly by adding a cache layer . In a cache proxy pattern, static content or dynamic content is cached upstream of the web app server . As shown in the following architectural diagram, you have a caching layer in front of the webapplication cluster: C a c h e p r o x y p a t t e r n a r c h i t e c t u r e In the preceding diagram, for high-performance delivery , cache content is delivered by the cache server . A few benefits of cache proxy patterns are as follows: Cache proxy patterns help you to deliver content using the cache,which means no modification is needed at the web server orapplication server level.They reduce the load of content generation for dynamic content inparticular . Y ou have the flexibility to set up a cache at the browser level such as in HTTP headers, URLs, cookies, and more. Alternatively , you can cache information in the cache layer if you don't want to store it at thebrowser level. In the cache proxy pattern, you need to make sure that you maintainmultiple copies of the cache to avoid the single point of failure. Sometimes,you may want to serve your static content from both the server and CDN,each of which requires a dif ferent approach. Let's deep dive into this hybrid situation in the next section. Rewrite pr oxy pattern Sometimes, you want to change the access destinations of static websitecontent such as images and videos, but don't want to make changes to theexisting systems. Y ou can achieve this by providing a proxy server using rewrite proxy patterns. T o change the destination of static content to other storage such as content service or internet storage, you can use a proxyserver in front of the web server fleet. As shown in the followingarchitecture diagram, you have a proxy server in front of your applicationlayer , which helps to change the content delivery destination without modifying the actual application: R e w r i t e p r o x y p a t t e r n a r c h i t e c t u r e As shown in the preceding diagram, to accomplish a rewrite proxy pattern,place the proxy server in front of the currently running system. Y ou can construct a proxy server using software such as Apache NGINX.  The following are the steps to build a rewrite proxy pattern: 1 . Put a running  proxy server on an EC2 instance, which is able to overwrite the content between the Load balancer and the storage service such as Amazon S3 , which stores the static content.  2 . Add to the proxy server rules for overwriting URLs within the content.These rules will help Elastic Load Balancing ( ELB ) to point to a new location, as shown in the preceding diagram, which redirects the proxyserver rule from https://cdn/test.jpg  to /test.jpg . 3 . As required, apply Auto Scaling to the proxy servers by configuring anumber of minimum and maximum proxy servers needed as perapplication load. In this section, you learned about various ways to handle caching for staticcontent distribution over the network. However , caching at the application layer is very important for improving application performance for overalluser experience. Let's learn more about the app caching pattern to handledynamic user data delivery performance. App caching pattern When it comes to applying caching to applications, you want to add a cacheengine layer between your application servers and the database. The appcaching pattern allows you to reduce the load on the database as the mostfrequent query is served from the caching layer . The app caching pattern improves overall application and database performance. As shown in thefollowing diagram, you can see the caching layer applied between theapplication layer and the database layer: A p p l i c a t i o n c a c h i n g p a t t e r n a r c h i t e c t u r e As shown in the preceding diagram, based on your data access pattern , you can use  either  lazy caching or write-thr ough . In lazy caching, the cache engine checks whether the data is in the cache and, if not, gets it from thedatabase and keeps it in the cache to serve future requests. Lazy caching isalso called the  cache aside pattern . In the write-through method, data is written in the cache and in the data storeat the same time. If the data gets lost from the cache, then it can get it again from the database. W rite-through is used mainly in application-to- application situations where users are writing a product review (whichalways needs to load on the product page). Let's learn more about thepopular caching engines  Redis and Memcached . Memcached versus Redis Redis and Memcached are two popular caching engines used in applicationdesign. Often, the Redis cache engine is required for more complexapplication caching needs such as creating a leader board for a game.However , Memcached is more high-performing and is helpful for handling heavy application  loads. Each caching engine has its own pros and cons. Let's take a look at the major dif ferences between them, which will help you to make a decision of which to use: Memcached Redis Of fers multithreading Single-threaded Able to use more CPU core forfaster processing Unable to utilize multi-coreprocessor , which results in comparatively slow performance Supports key-value style data Supports complex and advancedata structures Lacks data persistence; loses thedata stored in cache memory inthe event of a crash Data can persist using built-in readreplicas with failover Easy maintenance More complexity involved owing to the need to maintain the cluster Good to cache flat strings such asflat HTML pages, serializedJSON, and more Good to create a cache for agaming leader board, a live votingapp, and more   Overall, if you need to decide which engine to use, base it on a use case thatcan justify using Redis or Memcached. Memcached is simple and has lowermaintenance, and it is typically preferred when your cache doesn't need theadvanced features that Redis of fers. However , if you need the advantage of data persistence, advanced data types, or any of the other features listed,then Redis is the best solution. When implementing caching, it's essential to understand the validity of datathat needs to be cached. If the cache hit rate is high, that means the data isavailable in the cache when required. For a higher cache hit ratio, of fload the database by reducing direct queries; this also improves the overallapplication performance. A cache miss occurs when data is not present inthe cache and this increases the load in the database. The cache is not alar ge data store, so you need to set the TTL and evict the cache as per your application need. As you have seen in this section, there are multiple benefits of applyingcaches, including application performance improvement, the ability toprovide predictable performance, and the reduction in database cost. Let'slearn about some more application-based architecture that demonstrates theprinciple of loose coupling and constraint handling. Understanding the cir cuit br eaker pattern It's common for a distributed system to make a call to other downstreamservices, and the call could fail or hang without response. Y ou will often see code that retries the failed call several times. The problem with a remoteservice is that it could take minutes or even hours to correct, and animmediate retry might end up in another failure. As a result, end users waitlonger to get an error response while your code retries several times. Thisretry function would consume the threads, and it could potentially induce acascading failure. The circuit breaker pattern is about understanding the health of downstreamdependencies. It detects when those dependencies are unhealthy andimplements logic to gracefully fail requests until it detects that they arehealthy again. The circuit breaker can be implemented using a persistencelayer to monitor healthy and unhealthy requests over the past requestinterval. If a defined percentage of requests observe an unhealthy behavior over thepast interval or over a total count of exceptions, regardless of percentage,the circuit marks as open. In such a situation, all requests throw exceptionsrather than integrate with the dependency for a defined timeout period.Once the timeout period has subsided, a small percentage of requests try tointegrate with the downstream dependency , to detect when health has returned. Once a suf ficient percentage of requests are healthy again over an interval, or no errors are observed, the circuit closes again, and all therequests are allowed to integrate as they usually would thoroughly .  The implementation decisions involve the state machine to track/share thehealthy/unhealthy request counts. The states of services can be maintainedin DynamoDB, Redis/Memcached, or another low-latency persistence store. Implementing the bulkheadspattern Bulkheads are used in ships to create separate watertight compartments thatserve to limit the ef fect of failure, ideally preventing the ship from sinking. If water breaks through the hull in one compartment, the bulkheads preventit from flowing into other compartments, limiting the scope of the failure. The same concept is useful to limit the scope of failure in the architecture oflar ge systems, where you want to partition your system to decouple dependencies between services. The idea is that one failure should not causethe entire system to fail, as shown in the following diagram: B u l k h e a d p a t t e r n In the bulkhead pattern, it's better to isolate the element of the applicationinto the pool for service, which has a high dependency; so, if one fails,others continue to serve upstream services. In the preceding diagram,Service 3  is partitioned into two pools from a single service. Here, if Service 3 fails, then the impact of either  Service 1 or Service 2  depends on their dependency on the pool, but the entire system does not go down. Thefollowing are the major points to consider when introducing the bulkheadpattern in your design, especially for the shared service model: Save part of the ship, which means your application should not shutdown due to the failure of one service. Decide whether less-ef ficient use of resources is okay . Performance issues in one partition should not impact the overall application.Pick a useful granularity . Don't make the service pools too small; make sure they are able to handle application load.Monitor each service partition performance and adhere to the SLA.Make sure all of the moving parts are working together and test theoverall application when one service pool is down. Y ou should define a service partition for each business or technical requirement. Y ou should use this pattern to prevent the application from cascading failure and isolating critical consumers from the standardconsumer . Often, legacy application servers have a configuration withhardcoded  Internet Pr otocol   ( IP ) addresses or Domain Name Server  ( DNS ) names. Making any server change for modernization and upgrade requires making changes in application and revalidating it. In thesecases, you don't want to change the server address. Let's learn how to handlesuch a situation with a floating IP in the next section. Cr eating a floating IP pattern It's common that monolithic applications have lots of dependencies on theserver where they are deployed. Often, application configuration and codehave hardcoded parameters based on server DNS name and IP address.Hardcoded IP configuration creates challenges if you want to bring up a newserver in case of an issue with the original server . Additionally , you don't want to bring down the entire application for the upgrade, which may causesignificant downtime. T o handle such a situation, you need to create a new server keeping the same server IP address and DNS name. It can be achieved by moving the networkinterface from a problematic instance to the new server . The network interface is generally a Network Interface Card ( NIC ), which facilitates communication between servers over a network. It can be in the form ofhardware or software. Moving network interface means that now your newserver assumes the identity of the old server . W ith that, your application can live with the same DNS and IP address. It also allows easy rollback bymoving the network interface to the original instance. The public cloud (for example, A WS) made it easy by providing Elastic IP ( EIP ) and Elastic Network Interface ( ENI ). If your instance fails and you need to push traf fic to another instance with the same public IP address, then you can move the elastic IP address from one server to another , as shown in the following architecture diagram: F l o a t i n g I P a n d i n t e r f a c e p a t t e r n Since you are moving EIP , the DNS may not need to update. EIP can move your server's public IP across instances. If you need to move both public andprivate IP addresses, then use a more flexible approach such as ENI, asshown on the right of the preceding diagram. ENI can move acrossinstances, and you can use the same public and private address for traf fic routing or application upgrades. So far , you have learned multiple architecture patterns where applications are deployed in the virtual machine. However , in many cases, you may not be able to utilize the virtual machine fully . T o optimize your utilization further , you can choose to deploy your application in containers. Containers are most suitable for microservice deployment. Let's learn more about acontainer -based deployment in the next section. Deploying an application with acontainer As many programming languages are invented and technologies evolve, thiscreates new challenges. There are dif ferent application stacks that require dif ferent hardware and software deployment environments. Often, there is a need to run applications across dif ferent platforms and migrate from one to another platform. Solutions require something that can run anythingeverywhere and is consistent, lightweight, and portable. Just as shipping containers standardized the transport of freight goods,software containers standardize the transport of applications. Docker createsa container that contains everything a software application would need to beable to run all of its files, such as filesystem structure, daemons, libraries,and application dependencies. Containers isolate software from itssurrounding development and staging environments. This helps to reduceconflicts between teams running dif ferent software on the same infrastructure. VMs isolate at the operating system level, and containers isolate at thekernel level. This isolation allows several applications to run on a single-hostoperating system, and yet still have their filesystem, storage, RAM, libraries,and, mostly , their own view  of the system: V i r t u a l m a c h i n e s a n d c o n t a i n e r s f o r a p p l i c a t i o n d e p l o y m e n t   As shown in the preceding diagram, multiple applications are deployed in asingle virtual machine using containers. Each application has its runtimeenvironment, so you can run many individual applications while keeping thesame number of servers. Containers share a machine's operating systemkernel. They start instantly and use less computing time and RAM.Container images are constructed from the filesystem layers and sharestandard files. Shared resourcing minimizes disk usage, and container imagedownloads are much faster . Let's take a look at why containers are becoming more popular along with their benefits. The benefit of containers Customers often ask these questions when it comes to containers : Why do we need containers when we have instances?Don't instances already provide us with a level of isolation from theunderlying hardware? While the preceding questions are valid, several benefits accrue from usinga system such as Docker . One of the key benefits of Docker is that it allows you to fully utilize your virtual machine resources by hosting multipleapplications (on distinct ports) in the same instance. Docker uses certain features of the Linux kernel, namely kernel namespacesand groups, to achieve complete isolation between each Docker process, asindicated in the following architecture diagram: C o n t a i n e r l a y e r i n a p p l i c a t i o n i n f r a s t r u c t u r e As shown in the preceding diagram, it's possible to run two or moreapplications that require dif ferent versions of the Java runtime on the same machine, as each Docker container has its version of Java and theassociated libraries installed. In turn, the container layer in the applicationinfrastructure makes it easier to decompose your applications intomicroservices that can run side by side on the same instance. Containershave the following benefits: Portable runtime application envir onment : Containers provide platform-independent capabilities, where you build your applicationonce and deploy it anywhere regardless of the underlying operatingsystem.Faster development and deployment cycles :  Modify the application and run it anywhere with quick boot time, typically within seconds. Package dependencies and application in a single artifact : Package the code, library , and dependencies together to run the application in any operating system. Run differ ent application versions : Applications with dif ferent dependencies run simultaneously in a single server . Everything can be automated: Container management and deployment is done through scripting, which helps to save cost andhuman error . Better r esour ce utilization : Containers provides ef ficient scaling and high availability with multiple copies of the same microservicecontainer can be deployed across servers for your application.Easy to manage the security aspect : Containers are platform-specific rather than application-specific. Container deployment is becoming very popular due to its benefits. Thereare multiple ways to orchestrate containers. Let's look at containerdeployment in more detail next. Container deployment Complex applications with multiple microservices can be quickly deployedusing container deployment. The container makes it easier to build anddeploy the application more quickly as the environment is the same. Buildthe container in development mode, push to test, and then release toproduction. For hybrid cloud environments, container deployment is veryuseful. Containers make it easier to keep environments consistent acrossmicroservices. As microservices aren't always very resource-consuming,they can be placed together in a single instance to reduce cost. Sometimes, customers have short workflows that require a temporaryenvironment setup. Those environments may be queue systems orcontinuous integration jobs, which don't always utilize server resourcesef ficiently . Container orchestration services such as Docker and Kubernetes can be a workaround, allowing them to push and pop containers onto theinstance. Docker's Lightweight container virtualization platform provides tools tomanage your applications. Its stand-alone application can be installed on anycomputer to run containers. Kubernetes is a container orchestration servicethat works with Docker and another container platform. Kubernetes allowsautomated container provisioning and handles security , networking, and scaling aspects diligently . Containers help the enterprise to create more cloud-native workloads,and public cloud providers such as A WS extend services to manage Docker containers and Kubernetes. The following diagram shows Docker's containermanagement using Amazon Elastic Containers Service ( ECS ), providing a fully managed elastic service to automate the scaling and orchestration ofDocker containers: C o n t a i n e r d e p l o y m e n t a r c h i t e c t u r e In the preceding diagram, multiple containers are deployed in a singleAmazon EC2 virtual machine, which is managed through Amazon ECS andfacilitates Agent Communication Service and cluster management. All userrequests are distributed using a load balancer among the containers.Similarly , A WS provides Amazon Elastic Kubernetes Service ( EKS ) to manage containers using Kubernetes. Containers are a broad topic, and, as a solution architect, you need to befamiliar with all of the available options. This section provides an overviewof containers. However , you will need to deep dive further if you choose to utilize containers for your microservice deployment. As of now , you have learned about various architecture patterns focusing on application development. Everyone has to agree that data is an integral partof any architecture design, and most of the architecture revolves aroundcollecting, storing, and processing the visualization of data. Let's learn moreabout handling data in application architecture in the next section. Database handling in applicationar chitectur e Data is always at the center of any application development, and scalingdata has always been challenging. Handling data ef ficiently improves application latency and performance. In the previous section, Building cache-based ar chitectur e , you learned how to handle frequently queried data by putting a cache in front of your database under the app cachingpattern. Y ou can put either a Memcached or Redis cache in front of your database, which reduces the many hits on the database and results inimproving database latency . In application deployment, as the user base of your application grows, youneed to handle more data by your relational database. Y ou need to add more storage or vertically scale the database server by adding more memory andCPU power . Often, horizontal scaling is not very straightforward when it comes to scaling relational databases. If your application is read-heavy , you can achieve horizontal scaling by creating a read replica. Route all readrequests to database read replicas, while keeping the master database nodeto serve write and update requests. As read replica has asynchronousreplication, it can add some lag time. Y ou should choose to read the replica option if your application can tolerate some milliseconds of latency . Y ou can use read replica to of fload reporting. Y ou can use database sharding to create a multi-master for your relational database and inject the concept of horizontal scaling. The shardingtechnique is used to improve writing performance with multiple databaseservers. Essentially , databases are prepared and divided with identical structures  using appropriate table columns as keys to distribute the writing processes. As demonstrated in the following architecture diagram, thecustomer database can be divided into multiple shards: R e l a t i o n a l d a t a b a s e s h a r d i n g As shown in the preceding diagram, without shar ds , all data resides in one partition; for example, the user's first name starts with A to Z in onedatabase. W ith sharding, data is split into lar ge chunks called shards. For example, user first name A to I is in one database, J to R in anotherdatabase, and S to Z in a third database. In many circumstances, shardinggives you higher performance and better operating ef ficiency . Y ou can use Amazon RDS in shar ding backend databases. Install shar ding softwar e such as MySQL server combined with a Spider Storage Engine on an Amazon EC2 instance. Then, first, pr epar e multiple RDS databases and use them as the shar ding backend databases. However , what if your master database instance goes down? In that case, you need to maintain high availability for your database. Let's take a closerlook at database failover . High-availability database pattern For the high availability of your application, it is critical to keep yourdatabase up and running all of the time. As horizontal scaling is not astraightforward option in the relational database, it creates additionalchallenges. T o achieve high database  availability , you can have a standby replica of the master database instance, as shown in the following diagram: H i g h - a v a i l a b i l i t y d a t a b a s e p a t t e r n As shown in the preceding diagram, if the master instance goes down, yourapplication server switches over to standby instance. Read replica takes theload of f of master to handle latency . Master and standby are located in dif ferent availability zones, so your application will still be up even when an entire availability zone is down. This architecture also helps to achievezero downtime which may cause during the database maintenance window . When a master instance is down for maintenance, the application canfailover to secondary standby instance and continue serving user request. For the purpose of disaster recovery , you will want to define the database backup and archival strategy , d epending on your application's  Recovery point object  ( RPO ) of how frequently you want to take backups. If your RPO is 30 minutes, it means your or ganization can only tolerate data loss worth of 30 minutes. In that case, you should take a backup every half anhour . While storing the backup, you need to determine how long the data can be stored for customer query purposes. Y ou may want to store data for six months as an active backup and then in an archival store as per thecompliance requirement. Consider how quickly you might need to access your backup and determinethe type of network connection needed to meet your backup and recoveryrequirements as per the company r ecovery time objective ( R T O ). For example, if your company's R T O is 60 minutes, it means you should have enough network bandwidth to retrieve and restore your backup within anhour . Also, define whether you are backing up snapshots of complete systems or volumes attached to systems. Y ou may also need to classify your data, for example, if it has customer - sensitive information such as email, addresses, personally identifiableinformation, and more. Y ou need to define the data encryption strategy accordingly . Y ou will learn more about data security in Chapter 8 , Security Considerations . Y ou can also consider migrating from an RDBMS to a NoSQL database depending upon your application's growth and complexity . NoSQL can provide you with greater scalability , management, performance, and reliability than most relational databases. However , the process of migrating to NoSQL from an RDBMS can be time-consuming and labor -intensive. There is lots of data to process in any application, for example, clickstreamdata, application log data, rating and review data, social media data, andmore. Analyzing these datasets and getting insight can help you to growyour or ganization exponentially . Y ou will learn more about these use cases and patterns in Chapter 13 , Data Engineering and Machine Learning . As of now , you have learned about the best practices to design a solution architecture. Let's learn about some anti-patterns,  which should be avoided, in the next section. A voiding anti-patterns in solution ar chitectur e In this chapter , you have learned about a dif ferent way of designing solution architecture with various design patterns. Often, the teams can drift awayfrom best practices due to timeline pressure or the unavailability ofresources. Y ou always need to give special attention to the following architecture design anti-patterns: In an anti-pattern (an example of a poorly designed system), scaling isdone reactively and manually . When application servers reach their full capacity with no more room, users are prevented from accessing theapplication. On user complaints, the admin finds out that the serversare at their full capacity and starts launching a new instance to takesome of the load of f. Unfortunately , there is always a few minutes' lag between the instance launch and its availability . During this period, users are not able to access the application.W ith anti-patterns, automation is missing. When application servers crash, the admin manually launches and configures the new server andnotifies the users manually . Detecting unhealthy resources and launching replacement resources can be automated, and you can evennotify when resources are changed. W ith anti-patterns, the server is kept for a long time with hardcoded IP addresses, which prevent flexibility . Over time, dif ferent servers end up in dif ferent configurations and resources are running when they are not needed. Y ou should keep all of the servers identical and should have the ability to switch to a new IP address. Y ou should automatically terminate any unused resources.W ith anti-patterns, an application is built in a monolithic way , where all layers of architecture including web, application, and data layersare tightly coupled and server dependent. If one server crashes, itbrings down the entire application. Y ou should keep the application and web layer independent by adding a load balancer in between. If one of the app servers goes down, the load balancer automaticallystarts directing all of the traf fic to the other healthy servers. W ith anti-patterns, the application is server bound, and the server communicates directly with each other . User authentication and sessions are stored in the server locally and all static files are servedfrom the local server . Y ou should choose to create an SOA, where the services talk to each other using a standard protocol such as HTTP . User authentication and sessions should be stored in low latency-distributed storage so that the application can be scaled horizontally . The static asset should be stored in centralized object storage that isdecoupled from the server . W ith anti-patterns, a single type of database is used for all kinds of needs. Y ou are using a relational database for all needs, which introduces performance and latency issues. Y ou should use the right storage for the right need, such as the following: NoSQL to store user sessionCache data store for low latency data availabilityData warehouse for reporting needsRelation database for transactional data W ith anti-patterns, you will find a single point of failure by having a single database instance to serve the application. Wherever possible,eliminate single points of failure from your architectures. Create asecondary server (standby) and replicate the data. If the primarydatabase server goes of fline, the secondary server can pick up the load. W ith anti-patterns, static content such as high-resolution images and videos are served directly from the server without any caching. Y ou should consider using a CDN to cache heavy content near the userlocation, which helps to improve page latency and reduce page loadtime. W ith anti-patterns, you can find security loopholes that open server access without a fine-grained security policy . Y ou should always apply the principle of least privilege, which means starting with no accessand only giving access to the required user group.  The preceding points provide some of the most common anti-patterns.Throughout this book, you will learn the best practices of how to adopt them in solution design.Summary In this chapter , you learned about various design patterns by applying the techniques from Chapter 3 , Attributes of the Solution Ar chitectur e, and Chapter 4 , Principles of Solution Ar chitectur e Design . First, you built the architecture design foundation from a multilayer architecture with areference architecture from three-tier web application architecture. Y ou learned how to design a multi-tenant architecture on top of a three-tierarchitecture, which can provide a SaaS kind of of fering. Y ou learned how to isolate multi-tenant architecture at the database label, schema level, andtable level as per customer and or ganization need. User state management is very critical for complex applications such asfinance, e-commerce, travel booking, and more. Y ou learned about the dif ference between stateful and stateless applications and their benefits. Y ou also learned how to create a stateless application with a persistent layer ofthe database for session management. Y ou learned about the two most popular SOA patterns, SOAP-based and RESTful-based patterns, alongwith their benefits. Y ou looked at a reference architecture of an e-commerce website based on SOA and learned how to apply the principle of loosecoupling and scaling. Y ou learned about serverless architecture and how to design a secure survey delivery architecture that is entirely serverless. Y ou also learned about microservice architecture using the example of a serverless real-time votingapplication, which builds on the microservice pattern. For more loosecoupling designs, you learned about queuing chain and job observerpatterns, which provide loosely coupled pipelines to process messages inparallel. Y ou learned about the pub/sub and event stream models to design event-driven architecture.  It's not possible to achieve your desired performance without applyingcaching. Y ou learned about various cache patterns to apply to caches at the client side, content distribution, web layer , application layer , and database layer . Y ou learned architecture patterns to handle failure such as a circuit breaker to handle the downstream service failure scenario and bulkheadpattern to handle complete service failure. Y ou learned about floating IP patterns to change servers without changing their address in failuresituations to minimize downtime. Y ou learned about the various technique of handing data in an application and how to make sure your database is highly available to serve yourapplication. Finally , you learned about various architecture anti-patterns and how to replace them using best practices. While, in this chapter , you learned about various architecture patterns, in the next chapter , you will learn about architecture design principles for performance optimization. Additionally , you will deep dive into technology selection in the area of computing, storage, databases, and networking,which can help to improve your application's performance. Performance Considerations In this era of fast internet, users expect very high-performance applications.There have been experiments that show that every second of applicationloads delay , cause a significant loss in the revenue of an or ganization. Therefore, the performance of the application is one of the most criticalattributes of solution design that can impact your product adoption growth. In the previous chapter , you learned about various solution architecture design patterns that can be used to solve a complex business problem. Inthis chapter , you will gain an understanding of the best practices to optimize your application for optimal performance. Y ou will learn various design principles that you can use to optimize the solution architecture'sperformance. Here, performance needs to be optimized at every layer and inevery component of the architecture. Y ou will gain an understanding of how to choose the right technology at various layers of your architecture for thecontinuous improvement of your application's performance. Y ou will learn how to follow the best practices of performance optimization in thischapter . W e will focus on the following topics in particular: Design principles for architecture performanceT echnology selection for performance optimization Performance monitoring By the end of the chapter , you will have an understanding of important attributes of performance improvement, such as latency , throughput, and concurrency . Y ou will be able to make better decisions regarding your choice of technology , which can help you to improve performance at the various layers of architecture, such as compute, storage, database, andnetworking. Design principles for ar chitectur e performance Architectural performance ef ficiency focuses on ef ficiently using application infrastructure and resources to meet increasing demand andtechnology evaluation. T echnology vendors and open source communities continuously work to improve the performance of applications. Often, lar ge enterprises continue to work on legacy programming languages andtechnologies because of fear of changing and taking risks. As technologyevolves, it often addresses critical performance issues, and the advancementof technology in your application helps to improve applicationperformance. Many lar ge public cloud providers, such as  Amazon W eb Service  ( A WS ), Microsoft Azure, and Google Cloud Platform  ( GCP ), of fer technology as a service. This makes it easier to adopt  enterprise jobs  and complex technologies  more ef ficiently and with minimal ef fort—for example, you might use storage as a service to manage a massive amount of data or aNoSQL database as a managed service to provide high-performancescalability to your application. Now or ganizations can utilize a  content distribution network ( CDN ) to store heavy image and video data near user locations to reduce networklatency and improve performance. W ith infrastructur e as a service ( IaaS ), it becomes easier to deploy workloads closer to your user base, which helpsto optimize application performance by reducing latency over the network. As servers virtualize,  y ou can be more agile and experiment with your application, and you can apply a high degree of automation. Agility helpsyou to experiment and find out what technology and method are best suitedfor your application workload—for example, my server deployment shouldgo for virtual machines or containers, or it can even go completelyserverless with the use of A WS Lambda, which is a  Function as a Service ( FaaS ). Let's look at some vital design principles to consider for your workload performance optimization.Reducing latency Latency can be a major factor in your product adoption, as users are lookingfor faster applications. It doesn't matter where your users are located, youneed to provide a reliable service for your product to grow . Y ou may not be able to achieve zero latency , but the goal should be to reduce response time within the user tolerance limit. Latency is the time delay between the user sending a request and receivingthe desired response. As shown in the following diagram, it takes 600 ms for a client to send a request to the server and 900 ms for the server to respond, which introduces a total latency of 1.5 seconds (1500 ms): R e q u e s t r e s p o n s e l a t e n c y i n a c l i e n t - s e r v e r m o d e l Most of the application needs to access the internet in order to have adiverse set of global users. These users are expecting consistency inperformance, regardless of their geographical location. This is sometimeschallenging, as it takes time to move data over the network from one part  of the world  to another . Network latency can be caused by various factors, such as the  network transmission medium , r outer hops , and network pr opagation . Oftentimes, a request that is sent over the internet hops over multiplerouters, which adds latency . Enterprises commonly use their fiber optics line to set up connectivity between their corporate network or cloud, whichhelps to avoid inconsistency . In addition to the problems caused by the network, latency can also becaused by various components of the architecture. At the infrastructurelevel, your compute server can have latency issues due to memory andprocessor problems, where the data transfer between the CPU and RAM isslow . The disk can have latency due to slow read and write. Latency in a hard disk drive ( HDD ) is dependent on the time it takes to select a disk memory sector to come around and position itself under the head forreading and writing. The disk memory sector is the physical location of data in the memory disk. In an HDD,data is distributed in memory sectors during write operations, as the disk iscontinuously r otating, so data can be written randomly . During the r ead operation, the head needs to wait for the r otation to bring it to the disk memory sector . At the database level, latency can be  caused by slow data reads and writes from the database due to hardware bottlenecks or slow query processing.T aking the database load of f by distributing the data with partitioning and sharding can help to reduce latency . At the application level, there could be an issue with transaction processing from code that needs to be handledusing garbage collection and multithreading. Achieving low latency meanshigher thr oughput , as latency and throughput are directly related, so let's learn more about throughput. Impr oving thr oughput Throughput is the quantity of data that is sent and received at a given time,while latency is defined as the time between when the user initiates a requestin the application and receives the response from the application . When it comes to networks, bandwidth plays an important role. Bandwidth determines the maximum number of data that can get transferr ed over the network. Throughput and latency have a direct relationship as they work together . Lower latency means high throughput as more data can transfer in lesstime. T o understand this better , let's take the analogy of a country's transportation infrastructure. Let's say that highways with lanes are network pipelines and cars are datapackets. Suppose a given highway has 16 lanes between 2 cities, but that notall vehicles can reach the destination at the desired time; they may getdelayed because of traf fic congestion, lanes closing, or accidents. Here, latency determines how fast a car can travel from one city to another , while throughput tells us how many cars can reach their destinations. For anetwork, using full bandwidth is challenging because of error and traf fic congestion. Network throughput transfers a significant amount of data over the networkin bits per second ( bps ). Network bandwidth is the maximum size of the network pipeline, which it can process. The following diagram illustrates theamount of data transferred between client and server: T h r o u g h p u t i n a n e t w o r k In addition to the network, the throughput is applicable at the disk level.Disk throughput is determined by a factor of input/output per second  ( IOPS ) and the amount of data requested (I/O size). Disk throughput is determined in megabytes per second using the following formula: A verage IO size x IOPS = Thr oughput in MB/s So, if your disk IOPS is 20,000 IOPS and the I/O size is 4 KB (4096 bytes),then the throughput will be 81.9 MB/s (20,000 x 4096 and converted frombytes  to megabytes ). Input/output (I/O) requests and disk latency have a direct relationship. I/O means write and read respectively , while d isk latency is the time taken by each I/O request  to receiving the response from the disk . Latency is measured in milliseconds and should be minimal. This is impacted by diskr evolution per minute  ( RPM ). The  IOPS is the number of operations that the disk can serve per second. The throughput, also applicable at CPU and memory level, is determined bythe amount of data transfer between the CPU and RAM per second. At thedatabase level, throughput is determined by the number of transactions adatabase can process per second. At the application level, your code needs tohandle transactions that can be processed every second by managing theapplication memory with the help of garbage collection handling andef ficient use of the memory cache. As you learned when you looked at latency , throughput, and bandwidth, there is another factor called concurr ency , which is applicable to the various components of architecture and helps to improve application performance.Let's learn more about concurrency . Handling concurr ency Concurr ency is a critical factor for solution design as you want  your application  to process multiple tasks simultaneously—for example, your application needs to handle multiple users simultaneously and process theirrequests in the background. Another example is when your web userinterface needs to collect and process web cookie data to understand userinteraction with the product while showing users to their profile informationand product catalog. Concurrency is about doing multiple tasks at the sametime. People often get confused between parallelism and concurrency by thinkingthat they  are   both the same thing; however , concurrency is dif ferent from parallelism. In parallelism, your application divides an enormous task intosmaller subtasks, which it can process in parallel with a dedicated resourcefor each subtask. In concurrency , however , an application processes multiple tasks at the same time by utilizing shared resources among thethreads. The application can switch from one task to another duringprocessing, which means that the critical section of code needs to bemanaged using techniques such as  locks and semaphor es . As illustrated in the following diagram, concurrency is like a traf fic light signal where traf fic flow switches between all four lanes to keep traf fic going. As there is a single thread along which you should pass all traf fic, processing in other lanes has to stop while traf fic in one lane is in the clearing pr ocess . In the case of parallelism, there is a parallel lane available, and all cars can run in parallel without interrupting each other , as shown in the following diagram: C o n c u r r e n c y v e r s u s p a r a l l e l i s m In addition to transaction processing at the application level, concurrencyneeds to apply at the network level where multiple servers share the samenetwork resources. For a web server , there is a need to handle many network connections when users try to connect to it over the network. Itneeds to process the active request and close the connection for thecompleted or timeout request. At the server level, you will see multipleCPUs assigned or a multicore processor . These help in handling concurrency as the server can handle more threads to complete varioustasks simultaneously . At the memory level, the shared memory concurrency model helps toachieve concurrency . In this model, the concurrent modules interact with each other using shared memory . It could be two programs running in the same server and sharing filesystems where they can read and write. Also,there could be two processors or processor cores sharing the same memory . The disk in your server can encounter concurrency situations where twoprograms try to write to the same memory block. Concurrent I/O helps toimprove disk concurrency by allowing the disk to read and write a filesimultaneously . The database is always a central point of architecture design. Concurrencyplays an essential role in data handling as the database should have theability to respond to multiple requests simultaneously . Database concurrency is more complicated as one user might be trying to read arecord while another user is updating it at the same time. The databaseshould only allow data viewing when it gets fully saved. Make sure that thedata is completely committed before another user tries to update it. Cachingcan help to improve performance significantly; let's learn about somedif ferent cache types in architecture. Apply caching In  Chapter 6 , Solution Ar chitectur e Design Patterns , you learned how to apply caching at various levels of architecture in the Cache-based ar chitectur e  section . Caching helps to improve application performance significantly . Although you learned the dif ferent design patterns to apply to the cache by adding an external caching engine and technology , such as a content distribution network ( CDN ) , it's essential to understand that almost every application component and infrastructure have their cachemechanism. Utilizing the build-caching mechanism at each layer can helpto reduce latency and improve the performance of the application. At the server level, the CPU has its hardware cache, which reduces thelatency when accessing data from the main memory . The CPU cache includes the instruction and data cache, where the data cache store copiesfrequently used data. The cache is also applied at the disk level, but it ismanaged by operating system software (known as the  page cache ); however , the CPU cache is entirely managed by hardware. The disk cache is originating from secondary storage, such as the  hard disk drive ( HDD ) or solid-state drive ( SSD ). Frequently used data is stored in an unused portion of the main memory (that is, the RAM as page cache, which resultsin quicker access of content). Oftentimes, the database has a cache mechanism that saves the results fromthe database to respond faster . The database has an internal cache that gets data ready in the cache based on the pattern of your use. They also have aquery cache that saves data in the main server memory (RAM) if you makea query more than once. The query cache gets cleared in case of anychanges in data inside the table. In the case that the server runs out ofmemory , the oldest query result gets deleted to make space. At the network level, you have a DNS cache, which stores the web domainname and corresponding IP address local to the server . DNS caching allows a quick DNS lookup if you revisit the same website domain name. The DNS cache is managed by the operating system and contains a record of allrecent visits to websites. Y ou learned about client-side cache mechanisms such as the  br owser cache and various caching engines like Memcached and Redis in Chapter 6 , Solution Ar chitectur e Design Patterns . In this section, you learned about the original design factors, such aslatency , throughput, concurrent, and caching, which need to be addressed for architecture performance optimization. Each component of thearchitecture (whether it is a network at the server level or an application atthe database level) has a certain degree of latency and a concurrency issuethat needs to be handled. Y ou should design your application for the desired performance, as improving performance comes with a cost. The specifics of performanceoptimization may dif fer from application to application. Solution architecture needs to direct the ef fort accordingly—for example, a stock- trading application cannot tolerate sub-millisecond latency , while an e- commerce website can live with a couple of seconds latency . Let's learn about selecting technology for various architecture levels to overcomeperformance challenges. T echnology selection for performance optimization In Chapter 6 , Solution Ar chitectur e Design Patterns , you learned about various design patterns, including microservice, event-driven, cached-based, and stateless. An or ganization may choose a combination of these design patterns depending on their solution's design needs. Y ou can have multiple approaches to architecture design depending on your workload.Once you finalize your strategy and start solution implementation, the nextstep is to optimize your application. T o optimize your application, you need to collect data by performing load testing and defining benchmarking as peryour application's performance requirements. Performance optimization is a continuous improvement process, one inwhich you need to take cognizance of optimal resource utilization from thebeginning of solution design to after the application's launch. Y ou need to choose the right resources as per the workload or tweak the application andinfrastructure configuration—for example, you may want to select aNoSQL database to store the session state for your application and storetransactions in the relational database. For analytics and reporting purposes, you can of fload your production database by loading data from the application database to data warehousingsolutions and create reports from there. In the case of servers, you may wantto choose a virtual machine or containers. Y ou can take an entirely serverless approach to build and deploy your application code. Regardlessof your approach and application workload, you need to choose the primaryresource type, which includes computing, storage, database, and network.Let's look at more details on how to select these resources type forperformance optimization. Making a computational choice In this section, you will see the use of the term compute instead of the server , as nowadays software deployments are not limited to servers. A public cloud provider such as A WS has serverless of ferings, where you don't need a server to run your application. One of the most popularFaaS of ferings is A WS Lambda. Similar to A WS Lambda, other popular public cloud providers extend their of ferings in FaaS space—for example, Microsoft Azure has Azure Functions and Google Cloud Platform ( GCP ) of fers Google Cloud functions. However , or ganizations  still  make the default choice to go for servers with virtual machines. Now , containers are also becoming popular as the need for automation and resource utilization is increased.  Containers are becoming the preferred choice, e specially in the area of microservice application deployment. The optimal choice of computing— whether you want to choose server instances, containers, or go for serverless— depends upon application use cases. Let's look at the various compute choicesavailable. Selecting the server instance Nowadays, the term instance is getting more popular as virtual servers become norms. These virtual servers provide you flexibility and better useof resources. Particularly for cloud of ferings, all cloud providers of fer you virtual servers, which can be provisioned with a mere click on a webconsole or API call. The server instance helps in automation and providesthe concept of infrastructur e as a code , where everything can be automated everywhere. As your workload varies, there is a dif ferent kind of processing unit choice available. Let's look at some of the most popular options of processingpower: Central pr ocessing unit  ( CPU ): The CPU is one of the most popular computing processing choices. CPUs are easy to program, they enablemultitasking, and, most importantly , are versatile enough to fit anywhere, which makes them a preferred choice for the general-purpose application. The CPU function is measured in GHz (gigahertz), which indicates that the clock rate of the CPU speed is inbillions of cycles per second. CPUs are available at a low cost;however , they are not able to perform well for parallel processing as CPUs have the primary capabilities of sequential processing. Graphical pr ocessing unit  ( GPU ): As the name suggests, the GPU was designed initially to process graphics applications, and is able toprovide massive processing power . As the volume of data grows, it needs to process data by utilizing massive parallel pr ocessing  ( MPP ). For lar ge data processing use cases, such as machine learning, GPUs have become the obvious choice, and are used in a wide range ofcompute-intensive applications. Y ou may have heard of the  tera floating point operation  ( TFLOP ) as a unit of computation power for GPUs. A teraflop refers to the processor capability to calculate onetrillion floating-point operations per second. GPUs consist of thousands of smaller cores compared to CPUs,which have very few lager cores. GPUs have a mechanism to createthousands of threads using CUDA programming, and each threadcan process data in parallel, which makes processing super fast.GPUs are a bit costlier than CPUs. When it comes to processingcapabilities, you will find that GPUs are in the sweet spot of costand performance for an application that requires image analysis,video processing, and signal processing; however , they consume lots of power and may not work with a specific type of algorithm wheremore customized processors are required. Field-pr ogrammable gate array  ( FPGA ): FPGAs are very dif ferent to CPUs or GPUs. They are programmable hardware with a flexiblecollection of logic elements that can be reconfigured for the specificapplication, which can be changed after installation. FPGAs consumemuch less power than GPUs but are also less flexible. They canaccommodate massive parallel processing and also provide a feature toconfigure them as CPUs. Overall, the FPGA cost is higher , as they need to be customized for each individual application and require a longer development cycle.FPGAs may perform poorly for sequential operations and are notvery good for flops (float point operations). Application-specific integrated cir cuit  ( ASIC ): ASICs are purpose built, and custom integrate circuit optimization for a specificapplication—for example, specific to the deep learning T ensorFlow package, which Google provides as a  tensor pr ocessing unit  ( TPU ). They can be custom designed for the applications to achieve anoptimum combination of power consumption and performance. ASICsincur high costs because of the most extended development cycle, andyou have to perform a hardware-level redesign for any changes. The following diagram shows a comparison between the types ofprocessing mentioned in the preceding list. Here, the ASIC is most ef ficient, but takes a longer development cycle to implement. ASICs provide the most optimal performance but have the least flexibility to reutilize, whileCPUs are very flexible and can fit major use cases: C o m p a r i s o n b e t w e e n C P U s , G P U s , F P G A s , a n d A S I C s As shown in the preceding diagram, judging by the prospective cost, CPUsare the cheapest and ASICs are the costliest. T oday , the CPU has become a commodity and is used everywhere for general-purpose devices to keepcosts lower . The GPU has become famous for compute-intensive applications and The FPGA has become the first choice where morecustomized performance is required. Y ou will see these processing choices available over public cloud providers, such as A WS. In addition to CPUs, their Amazon Elastic Cloud Compute ( EC2 ) of fering provides a P-series instance that has a heavy use of the GPU and F-series instance providesFPGAs for custom hardware acceleration. In this section, you learned about the most popular compute choice. Y ou may hear about other types of processor , such as the  accelerated pr ocessing unit  ( APU ). The APU is a combination of the CPU, GPU, and digital signal pr ocessor  ( DSP ), which is optimized to analyze analog signals and then requires high-speed processing of data in real time. Let'slearn more about other popular compute type containers that are gainingpopularity rapidly because of the optimization of the use of resources withinthe virtual machine. W orking with containers In Chapter 6 , Solution Ar chitectur e  Design Patterns , you learned about container deployment and their benefits under the section titled  Deploying an application with a container . The use of containers is becoming the norm to deploy complex microservice applications because of the ease ofautomation and the  ef ficiency of  resource utilization. There are various platforms available for container deployment. Let's learn about some of themost popular choices in the area of the container , their dif ferences, and how they work together . Docker Docker is among one of the most in-demand technologies. It allows you topackage an application and its related dependencies together as a containerand deploy it to any operating system platform. As Docker providesplatform-independent capabilities to a software application, this makes theoverall software development, testing, and deploymentprocess  simplified  and   more accessible. Docker container images are portable from one system to another over alocal network or across the internet using Docker Hub. Y ou can manage and distribute your container using a Docker Hub container repository in caseyou make any changes in the Docker image that cause issues in yourenvironment. It's easy to revert to the working version of the containerimage, which makes overall troubleshooting easier . Docker containers help you to build a more complex multilayer application—for example, if you need to run the application server , database, and message queue together . Y ou can run them side by side using a dif ferent Docker image and then establish communication between them. Each ofthese layers may have a modified version of libraries, and Docker allowsthem to run on the same compute machine without conflict. Public cloud providers, such as A WS, provide containers management platforms, such as  Amazon Elastic Container Service ( ECS ). Container management helps to manage Docker containers on top of the cloud virtualmachine, Amazon EC2. A WS also provides the serverless option of container deployment using Amazon Far gate, where you can deploy containers without provisioning virtual machines. Instead of Docker Hub,you can use Amazon Elastic Container Repository ( ECR ) to manage your Docker imager in the cloud. Complex enterprise applications are built based on microservices that mayspan across multiple containers. Managing various Docker containers as a part of one application can be pretty complicated. Kubernetes helps to solvethe challenges of the multicontainer environment; let's learn more aboutKubernetes.Kubernetes Kubernetes can manage and control multiple containers in productionenvironments compared to Docker , where you can  only  work with a few containers . Y ou can consider Kubernetes as a container orchestration system. Y ou can host a Docker container in bare metal, or a virtual machine node called a Docker host, and Kubernetes can co-ordinate across a cluster ofthese nodes. Kubernetes makes your application self-healing by replacing unresponsivecontainers. It also provides horizontal scaling capabilities and a blue–greendeployment ability to avoid any downtime. Kubernetes distributes incominguser traf fic load between the container and manages the storage shared by various containers. As shown in the following diagram, Kubernetes and Docker work welltogether to orchestrate your software application. Kubernetes handlesnetwork communication between Docker nodes and Docker containers: D o c k e r a n d K u b e r n e t e s Docker works as an individual piece of the application and Kubernetes takescare of the orchestration to make sure all these pieces work together asdesigned. It's easier to automate overall application deployment and scalingwith Kubernetes. In Docker , containers are hosted in nodes and each Docker container in a single node shares the same IP space. In Docker , you need to manage the connections between containers by taking care of any IPconflict. Kubernetes solves this problem by having a master instance thatkeeps track of all nodes hosting containers.  Kubernetes's master node is responsible for assigning an IP address andhosting a key–value store for container configuration along with Kubelet tomanage the containers. Docker containers are groups in the pod, where they share the same IP address. This entire setup is called the  Kubernetes cluster . While Kubernetes is quickly becoming popular , there are other options available, such as Docker Swarm, which is built by Docker itself; however , Swarm doesn't have a web-based interface like Kubernetes, and also doesnot provide autoscaling and external load balancing. Ku bernetes is more complex to learn. A public cloud provider , such as A WS, provides Amazon Elastic Kubernetes   Service ( EKS ) to simplify the management of the Kubernetes cluster . OpenShift is another Kube rnetes distribution managed by Red Hat and is of fered as a  platform as a service ( PaaS ). Overall, containers add a layer of virtualization to the whole applicationinfrastructure. While they are useful in resource utilization, you may want tochoose a bare metal physical machine for your application deployment if itrequires ultra-low latency . Going serverless In recent years, serverless computing has become possible because of thepopularity of public cloud of ferings by cloud providers such as Amazon, Google, and Microsoft. Serverless computing allows the developers tofocus on their code and application development without worrying aboutunderlying infrastructure provisioning, configuration, and scaling. Thisabstracts server management and infrastructure decisions from thedeveloper and lets them focus on their area of expertise and the businessproblem they are trying to solve. Serverless computing brings a relativelynew concept of FaaS . The FaaS of fering is available using services such as A WS Lambda, Microsoft Azure Function, and Google Cloud Function. Y ou can write your code in the cloud editor and A WS Lambda handles the computing infrastructure  underneath   to run and scale your function. Y ou can design event-based architecture or RESTful microservices by adding an APIendpoint using Amazon API Gateway and A WS Lambda functions. Amazon API Gateway is a managed cloud service that adds RESTful APIsand W ebSocket APIs as frontends for the Lambda functions and enables real-time communication between applications. Y ou can further break your microservice into small tasks that can be scaled automatically andindependently .  In addition to focusing on your code, you never have to pay for idleresources in the FaaS model. Rather than scaling your entire service, youcan scale the required functions independently with built-in availability andfault tolerance; however , it could be a pretty daunting task if you have thousands of features to orchestrate, and predicting the autoscaling cost canbe tricky . These are especially good for scheduling jobs, processing web requests, or queuing messages.  In this section, you learned about the various computing choices, looking atserver instances, serverless options, and containers. Y ou need to select these compute services based on your application's requirement. There is no rulethat forces you to choose a particular type of computing; it is all aboutyour  choice of  or ganization's technology , the pace of innovation, and the nature of the software application. However , in general, for the monolithic application, you can stick to a virtual or bare-metal machine, and for complex microservices, you canchoose containers. For simple task scheduling or events-based applications,you can go for function as an obvious choice. Many or ganizations have built complex applications that are entirely serverless, which helped them tosave costs and achieve high availability without managing anyinfrastructure. Let's learn about another important aspect of your infrastructure and how itcan help you to optimize performance. Choosing a storage Storage is one of the critical factors for your application's performance. Anysoftware application needs to interact with storage for installation, logging,and accessing files. The optimal solution for your storage will dif fer based on the following factors: Access methods Block, file, or object Access patterns Sequential or random Access frequency Online (hot), of fline (warm), or archival (cold) Update frequency W rite once r ead many ( WORM ) or dynamic Access availability A vailability of storage when required Access durability Reliability of data store to minimize anydata loss Access throughput Input/output per second  ( IOPS ) and data read/write per second in MBs.   These depend upon your data format and scalability needs. Y ou first need to decide whether your data will be stored in block, file, or object storage.These are the storage formats that store and present data in a dif ferent way . Let's look at this in more detail. W orking with block storage and storage ar ea network (SAN) Block storage divides data into blocks and stores them as chunks of data.Each block has a unique ID that allows the system to place data wherever itis accessible faster . As blocks don't store any metadata about files, so a server -based operating system manages and uses these blocks in the hard drive. Whenever the system requests data, the storage system collects theblocks and gives the result  back   to the user . Block storage deployed in a  storage ar ea network ( SAN ) stores data ef ficiently and reliably . It works well where a lar ge amount of data needs to be stored and accessed frequently—for example, database deployment, email servers, applicationdeployment, and virtual machines. SAN storage is sophisticated and supports complex, mission-critical applications . They are a high-performance storage system that communicates block-level data between the server and storage; however , SAN is significantly costly and should be used for lar ge- scale,  enterprise   applications where low latency is required.   T o configure your block-based storage, you have to choose between a  solid- state drive  ( SSD ) and a  hard-disk drive  ( HDD ). HDDs are the legacy data storage for servers and enterprise storage arrays. HDDs are cheaper , but they are slower and need more power and cooling. SSDs use semiconductorchips and are faster than HDDs. They are much more costly; however , as technology evolves, SSDs will become more af fordable and gain popularity because of their ef ficiency and lower power and cooling requirements. W orking with file storage and network ar ea storage (NAS) File storage has been around for a long time and is widely used. In filestorage, data is stored as a single piece of information and is or ganized inside folders. When you need to access the data, you provide the file pathand get the data files; however , a file path can grow complicated as files become nested under multiple folder hierarchies. Each record has limitedmetadata, including the filename, time of creation, and updated timestamps.Y ou can take the analogy of a book library where you store books in drawers and keep a note of the location where each book is stored. An NAS is a file storage system that is  attached  to the  network and displays to the user where they can store and access their files. NAS storage alsomanage user privilege, file locking, and other security mechanisms thatprotect the data. NAS storage works well as file-sharing systems an d   local archives. When it comes to storing billions of files, NAS might not be theright solution, given that it has limited metadata information and a complexfolder hierarchy . T o store billions of files, you need to use object storage. Let's learn more about object storage and its benefits over file storage. W orking with object storage and the cloud data storage Object storage bundles the data itself with a unique identifier and metadatathat is customizable. Object storage uses a flat address space compared tothe hierarchical addresses in file storage or addresses distributed over achunk of blocks in block storage. Flat address space makes it easier tolocate data and retrieve it faster regardless of the data storage's location.Object storage also helps the user to achieve an unlimited scalability ofstorage. Object storage metadata can have lots of details, and users can customize itto add more details compared to the addition of tagging in file storage. Datacan be accessed by a simple API call and is very cost-ef fective to store. Object storage performs best for high-volume, unstructured data; however , objects cannot be modified but only replaced, which makes it not a gooduse case for a database. Cloud data storage, such as  Amazon Simple Storage Service ( S3 ), provides an unlimited scalable object data store with high availability anddurability . Y ou can access data with a unique global identifier and metadata file prefix. The following diagram shows all three storage systems in anutshell: D a t a s t o r a g e s y s t e m s As shown in the preceding diagram, block storage stores data in blocks.Y ou should use block storage when your application needs very low latency and data storage access by a single instance. File storage stores data in ahierarchical folder structure and has little latency overhead. Y ou should use the file storage system when a separate room needs to access multipleinstances. Object storage stores data in buckets with a unique identifier forthe object. It provides access over the web to reduce latency and increasethroughput. Y ou should use object storage to store and access static content, such as images and videos. Y ou can store a high volume of data in the object store and perform big data processing and analysis. Dir ect-attached storage  ( DAS ) is another kind of data storage where the storage is directly attached to the host server; however , it has a very limited scalability and storage capacity . The magnetic tape drive is another popular storage system for backing up and archiving. Because of its low cost andhigh availability , magnetic tape drives are used for archival purposes, but have high latency , which makes them unsuitable for  use in  direct applications. Oftentimes, you will need to increase throughput and data protection for amission-critical application, such as a transactional database, where data is stored in SAN storage; however , an individual SAN storage may have limited volume and throughput. Y ou can overcome this situation using a r edundant array of independent disks  ( RAID ) configuration. A RAID is a way of storing data on multiple disks. It protects data loss from a drivefailure and increases disk throughput by striping various disks together . RAID uses the technique of disk mirroring or disk striping, but for theoperating system, RAID is a single logical disk. RAID has a dif ferent level to distinguish the configuration type—for example, RAID 0 uses diskstriping and provides the best performance, but has no fault tolerance,whereas RAID 1 is known as disk mirr oring . It duplicates the data storage and provides no performance improvement for write operations but doublesthe read performance. Y ou can combine both RAID 0 and RAID 1 to form RAID 10 and achieve the best of both with high throughput and faulttolerance. Select the storage solution that matches your access pattern to maximizeperformance. There are various options available with a cloud of fering to choose for your block, file, and object storage method—for example, thepublic cloud A WS provides Amazon Elastic Block Stor e  ( EBS ) as a SAN type of storage in the cloud and Amazon Elastic File Storage  ( EFS ) as an NAS type of storage in the cloud. Amazon S3 is very popular for objectstorage. Dif ferent storage solutions provide you the flexibility to choose your storage methods based on the access pattern, whether you are workingin an on-premise environment or wanting to go cloud native. Now that you have learned about the compute and storage choices that arenecessary to achieve optimal performance, let's look at the next criticalcomponent of the application development, which is the database. Choosingthe right database for the right need will help you to maximize yourapplication performance and lower overall application latency . There are dif ferent types of database available, and choosing the correct database is critical. Choosing the database Oftentimes, you will want to standardize a common platform and use adatabase for ease of management; however , you should consider using a dif ferent type of database solution as per your data requirement. Selecting the incorrect database solution can impact system latency and performance.The choice of database can vary based on your application's requirementsfor availability , scalability , data structure, throughput, and durability . There are multiple factors to consider when choosing to use a database—forexample, the access pattern can significantly impact the selection ofdatabase technology . Y ou should optimize your database based on the access pattern. Databases generally have a configuration option for workload optimization.Y ou should consider the configuration for memory , cache, storage optimization, and so on. Y ou should also explore the operational aspect of database technologies in terms of scalability , backup, recovery , and maintenance. Let's look at the dif ferent database technologies that can be used to fulfill the database requirements of applications. Online transactional pr ocessing (OL TP) Most of the traditional relational databases are considered OL TP . The transactional database is the  oldest   and   most popular method of storing and handling application data. Some examples of relational OL TP databases are Oracle, Microsoft SQL Server , MySQL, PostgreSQL, Amazon RDS, as well as others. The data access pattern for OL TP involves fetching a small dataset by looking up their ID. A database transaction means that either allrelated updates of the database table completed, or none of them did. The relational model allows the processing of complex businesstransactions in an application, such as banking, trading, and e-commerce. Itwill enable you to  aggregate data and  create complex queries using multiple joins across tables. While optimizing your relational database, you need toconsider including the following optimizations: Database server that includes computing, memory , storage, and networkOperating system-level settings, such as a RAID configuration of thestorage volume, volume management, and block sizeDatabase engine configuration and partition as requiredDatabase-related options, such as schema, index, and view Scaling can be tricky for the relational database as it can scale vertically andhit the upper limit of system capacity . For horizontal scaling, you have to read the replica for r ead scaling and partition for write scaling . In the previous chapter , you learned how to scale a relational database in the section titled  Database handling in the application ar chitectur e . OL TP databases are suitable for lar ge and complex transactional applications; however , they don't scale well where a massive amount of data needs to aggregate and be queried. Also, with the internet boom, there is a lot of unstructured data that is coming from everywhere, and relationaldatabases are not able to handle unstructured data ef ficiently out of the box. In this case, the NoSQL database comes to the rescue. Let's learn moreabout how to handle a nonrelational database.Nonr elational databases (NoSQL) There is a lot of unstructured and semistructured data produced byapplication such as social media programs, the IoT (internet of things), andlogs, where you have a very dynamic schema. These data types may havedif ferent schemas from each set of records. Storing this data in a relational database could be a very tedious task. Everything has to be filed into fixedschema, which can either cause lots of null values or data loss. Thenonrelational or NoSQL database provides you with the flexibility to storesuch data without worrying about fixed schema. Each record can have avariable number of columns and can be stored in the same table. NoSQL databases can store a lar ge amount of data and provide low-access latency . They are easy to scale by adding more nodes when required and can support horizontal scaling out of the box. They can be an excellentchoice to store user session data and can make your application stateless toachieve horizontal scaling without compromising user experience. Y ou can develop a distributed application on top of the NoSQL database, whichprovides good latency and scaling, but query joining has to be handled atthe application layer . NoSQL databases don't support complex queries such as joining tables and entities. There are various choices available for the NoSQL database—for example,Cassandra, HBase, and MongoDB, which you can install in a cluster ofvirtual machines; however , the public cloud-like A WS provides a managed NoSQL database called Amazon Dynamo DB ,   which provides high throughput and sub-millisecond latency with unlimited scaling. Y ou can use OL TP for a relational database, but it has limited storage capacity . It doesn't respond well to queries for lar ge amounts of data, and those that perform aggregations as required for data warehouses. Datawarehousing needs are more analytical than transactional. The OLAPdatabase satisfies the gap of the OL TP database to query a lar ge dataset. Let's learn more about the OLAP database. Online analytical pr ocessing (OLAP) OL TP and NoSQL databases are useful for application deployment but have limited capabilities for lar ge-scale analysis. A query for a lar ge volume of structured data for analytics purposes is better served by a data warehouseplatform designed for faster access to structured data. Modern datawarehouse technologies adopt the columnar format and use massive parallelprocessing (MPP), which helps to fetch and analyze data faster . The columnar format avoids the need to scan the entire table when you needto aggregate only one column for data—for example, if you want todetermine the sales of your inventory in a given month. There may behundreds of columns in the order table, but you need to aggregate data fromthe purchase column only . W ith a columnar format, you will  only  scan the purchase column, which reduces the amount of data scanned compared tothe row format, and thereby increases the query performance. W ith massive parallel processing, you store data in a distributed manner between child nodes and submit a query to the leader nodes. Based on yourpartition key , the leader node will distribute queries to the child nodes, where each node picks up part of a query to perform parallel processing.The leader node then collects the subquery result from each child node andreturns your aggregated result. This parallel processing helps you to executethe query faster and process a lar ge amount of data quicker . Y ou can use this kind of processing by installing software such as IBM Netezza or Microsoft SQL server on a virtual machine, or you can go for amore cloud-native solution, such as Snowflake. A public cloud, such asAmazon W eb Service, provides the petabyte-scale data warehousing solution Amazon Redshift, which uses the columnar format and massivelyparallel processing. Y ou will learn more about data processing and analytics in Chapter 13 , Data Engineering and Machine Learning . Y ou need to store and search a lar ge amount of data, especially when you want to find a specific error in your logs or build a document search engine.For this kind of capability , your application needs to create a data search. Let's learn more about the data search.Building a data sear ch Oftentimes, you will need to search a lar ge volume of data to solve issues quickly or get business insights. The ability to search your application datawill help you to access detailed information and analyze it from dif ferent views. T o search for data with low latency and high throughput, you need to have search engines as your technology choice. Elasticsearch is one of the most popular search engine platforms and is builton top of the Apache Lucene library . Apache Lucene is a free and open source software library that is the foundation of many popular searchengines.  The  ELK (short for Elasticsear ch , LogStash , and Kibaba ) stack  is easy to use for the automatic discovery of lar ge-scale data and to index it for searching. Because of its property , there are multiple tools that have been developed around  Elasticsearch for visualization and analysis— f or example,  LogStash works with  Elasticsearch to  collect, transform, and analyze a lar ge amount of an application’ s log data. Kibana has an in-built connector with  Elasticsearch that  provides a simple solution for creating dashboards and analysis on indexed data. Elasticsearch can be deployed in virtual machines and scale horizontally toincrease capacity by adding new nodes to the cluster . The public cloud A WS provides the managed service Amazon Elasticsear ch Service ( Amazon ES ), which makes it cost ef fective and simple to scale and manage the Elasticsearch cluster in the cloud. In this section, you learned about the various database technologies andwhat they are used for . Y our applications can use a combination of all database technologies in order for their dif ferent components to achieve optimal performance. For complex transactions, you need to use a relationOL TP database, and to store and process unstructured or semistructured data, you need to use a nonrelational NoSQL database. Y ou should use the NoSQL database where very low latency is required over multiplegeographical regions a nd where you need to handle complex queries at the application layer , such as in a gaming application. If you need to perform any lar ge-scale analytics on structured data, use a data warehouse OLAP database. Let's look at another critical component of your architecture, which isnetworking . Networking is the backbone of the entire application and establishes communication between the servers and the outside world. Let'slearn about networking as regards application performance. Making the networking choice In this era of fast internet availability in almost every corner of the world, itis expected that applications will have a global user reach. Any delay in thesystem's response time depends upon  the request load and  the distance of the end user from the server . If the system is not able to respond to user requests in a timely manner , it can have a ripple ef fect by continuing to engage all system resources and pile up a huge request backlog, which willdegrade overall system performance. T o reduce latency , you should simulate the user's location and environment to identify any gap. As per your findings, you should design the server'sphysical location and caching mechanism to reduce network latency;however , the network solution choice for an application depends upon the networking speed, throughput, and network latency requirements. For anapplication to handle a global user base, it needs to have fast connectivitywith its customers where location plays an important role. Edge locationsprovided by the CDN help to localize the heavy content and reduce overalllatency . In Chapter 6 , Solution Ar chitectur e Design Patterns , you learned how to use a CDN to put data near your user's location in the section titled  cache-based ar chitectur e . There are various CDN solutions available with an extensive network of edge locations. Y ou can use a CDN if your application is static- content-heavy , where you need to deliver lar ge image and video content to your end user . Some of the more popular CDN solutions are Akamai, Cloudflare, and Amazon CloudFront (provided by the A WS cloud). Let's look at some DNS routing strategies to achieve low latency if yourapplication is deployed globally . Defining a DNS r outing strategy In order to have global reach, you may be deploying your application inmultiple geographical regions. When it comes to user request routing, youwant to route their requests to the nearest and fastest available server for aquick response from your application. The DNS router provides themapping between the domain names to the IP addresses and makes sure thatthe requests are served by the right server when the user types in thedomain name—for example, when you type amazon.com in your browser to do some shopping, your request is always routed to the Amazon applicationserver by the DNS service. The public cloud-like A WS provides a DNS service called Amazon Route 53 , where you can define a dif ferent kind of routing policy as per your application's need. Amazon Route 53 provides DNS services to simplifydomain management and zone APEX support. The following are the mostused routing policies: Simple r outing policy : As the name suggests, this is the most straightforward routing policy and doesn't involve any complications.It is useful to route traf fic to a single resource—for example, a web server that serves content for a particular website.Failover r outing policy : This routing policy requires you to achieve high availability by configuring active–passive failover . If your application goes down in one region, then all the traf fic can be routed to another region automatically . Geolocation r outing policy : If the user belongs to a particular location then you can use a geolocation policy . A geolocation routing policy helps to route traf fic to a specific region. Geopr oximity r outing policy : This is like a geolocation policy , but you have the option to shift traf fic to other nearby locations when needed. Latency r outing policy : If your application is running in multiple regions, you can use a latency policy to serve traf fic from the region where the lowest latency can be achieved.W eighted r outing policy : A weighted routing policy is used for A/B testing, where you want to send a certain amount of traf fic to one region and increase this traf fic as your trial proves more and more successful. Additionally , Amazon Route 53 can detect anomalies in the source and volume of DNS queries and prioritize requests from users that are known tobe r eliable . It also protects your application from a DDoS attack. Once traf fic passes through the DNS server , in most cases, the next stop will be a load balancer , which will distribute traf fic among a cluster of servers. Let's learn about some more details regarding the load balancer . Implementing a load balancer The load balancer distributes network traf fic across the servers to improve use concurrency , reliability , and application latency . The load balancer can be physical or virtual . Y ou need to choose a load balancer based on your application's need. Commonly , two types of load balancer can be utilized by the application: 1 . Layer 4 or network load balancer : Layer 4 load balancing routes packets based on information in the packet header—for example,source/destination IP addresses and ports. Layer 4 load balancing doesnot inspect the contents of a packet, which makes it less computeintensive and therefore faster . A network load balancer can handle millions of requests per second. 2 . Layer 7 or application load balancer : Layer 7 load balancing inspects, and routes packets based on the full contents of the packet.Layer 7 is used in conjunction with HTTP requests. The materials thatinform routing decisions are factors such as HTTP headers, URI path,and content type. This allows for more robust routing rules butrequires more compute time to route packets. The application loadbalancer can route the request to containers in your cluster based ontheir distinctive port number . Depending on the environment, you can choose hardware-based loadbalancers, such as an F5 load balancer or a Cisco load balancer . Y ou can also choose a software-based load balancer , such as  Nginx . The public cloud provider A WS facilitates a managed virtual load balancer called Amazon Elastic load balancer   ( ELB ). ELB can be applied at layer 7 as an application load balancer and tier 4 as a network load balancer . A load balancer is a great way of securing your application, making ithighly available by sending a request to healthy instances. It works togetherwith autoscaling to add or remove instances as required. Let's look at autoscaling and learn how it helps to improve overall performance and thehigh availability of your application. Applying autoscaling Y ou learned about autoscaling in Chapter 4 , Principles of Solution Ar chitectur e Design . Y ou learned about predictive autoscaling and reactive autoscaling in the section titled  Design for scale . The concept of autoscaling became popular with the agility provided by the cloud computing platform . Cloud infrastructure allows you to easily scale up or scale down your serverfleet based on user or resource demand. W ith a public cloud platform such as A WS, you can apply autoscaling at every layer of your architecture. In the presentation layer , you can scale the web server fleet based on your requests, and at the application layer based onthe server's memory and CPU utilization. Y ou can also perform scheduled scaling if you know the traf fic pattern when the server load is going to increase. At the database level, autoscaling is available for relationaldatabases such as Amazon Aurora Serverless and the Microsoft Azure SQLdatabase. A NoSQL database such as Amazon DynamoDB can beautoscaled based on throughput capacity . When autoscaling, you need to define the number of desired serverinstances. Y ou need to define the maximum and minimum server capacity as per your application's scaling needs. The following screenshot illustrates theautoscaling configuration from A WS Cloud: A u t o s c a l i n g c o n f i g u r a t i o n In the preceding autoscaling configuration setting, if three web serverinstances are running, it can scale up to 5 instances if the CPU utilization of servers goes above 50% and scale down to 2 instances if the CPU utilization goes below 20%. In the case of an unhealthy instance, the count will gobelow the desired capacity in a standard scenario. In such a case, the loadbalancer will monitor the instance health and will use autoscaling to provision new instances. The load balancer monitors instance health and willtrigger autoscaling to provision new instances as required. Autoscaling is a good feature to have, but make sure you set up your desiredconfigurations to limit the cost of a change in CPU usage. In the case ofunforeseen traf fic due to events such as a distributed denial of service (DDoS) attack, autoscaling can increase cost significantly . Y ou should plan to protect your system for such kinds of events. Y ou will learn more about this in Chapter 8 , Security Considerations . At the instance level, you need high-performance computing ( HPC ) to perform manufacturing simulation or gnome analysis. HPC performs wellwhen you put all instances in the same network close to each other for lowlatency of data transfer between the node of a cluster . Between your data centers or cloud, you can choose to use your private network, which canprovide an additional performance benefit. For example, to connect yourdata center to A WS cloud, you can use Amazon Direct Connect. Direct Connects provides 10 Gbps private fiber optics lines, where Network latencyis much lower than sending data over the internet. In this section, you have learned about various networking components thatcan help to improve application performance. Y ou can optimize your application network traf fic according to your user location and application demand. Performance monitoring is an essential part of your application, andyou should do proactive monitoring to improve customer experience. Let'slearn more about performance monitoring Managing performance monitoring Performance monitoring is important when you are trying to proactivelyunderstand any performance issue and reduce end user impact. Y ou should define your performance baseline and raise the alarm to the team in the caseof a threshold breach—for example, an application's mobile app open timeshould not be more than three seconds. Y our alarm should be able to trigger an automated action to handle poorly performing components—forexample, adding more nodes in a web application cluster to reduce therequest load. There are multiple monitoring tools available to measure applicationperformance and overall infrastructure. Y ou can use a third-party tool, such as Splunk, or A WS provided Amazon CloudW atch to monitor any application. Monitoring solutions can be categorized into active monitoring and passive monitoring solutions: W ith active monitoring, you need to simulate user activity and identify any performance gap upfront. Application data and workloadsituations are always changing, which requires continuous proactivemonitoring. Active monitoring works alongside passive monitoring asyou run the known possible scenarios to replicate user experience. Y ou should run active monitoring across all dev , test, and prod environments to catch any issue before it reaches the user .  Passive monitoring tries to identify an unknown pattern in real time.For a web-based application, passive monitoring needs to collectimportant metrics from the browser that can cause performance issues.Y ou can gather metrics from users regarding their geolocation, browser types, and device types to understand user experience and thegeographic performance of your application. Monitoring is all aboutdata, and it includes the ingestion, processing, and visualization of lotsof data. Y ou will continue to learn about the various monitoring methods and tools in upcoming chapters and dive deep into monitoring and alerts in Chapter 9 , Ar chitectural Reliability Considerations . Performance always comes with a cost, and, as a solution architect, youneed to think about the trade-of fs to take the right approach—for example, an or ganization's internal applications, such as the timesheet and HR programs, may not need very high performance compared to externalproducts, such as e-commerce applications. An application that deals withtrading (for example) needs very high performance, which requires moreinvestment. As per your application's needs, you can balance betweendurability , consistency , cost, and performance  T racking and improving performance are complex tasks, where you need to collect lots of data and analyze patterns. An access pattern helps you tomake the right choice for performance optimization. Load testing is one ofthe methods that allows you to tweak your application configuration bysimulating user load and provides you with data to make the right decisionsfor your application architecture. Applying continuous active monitoring incombination with passive monitoring helps you to maintain consistentperformance for your application. Summary In this chapter , you learned about the various architecture design principles that impact the performance of applications. Y ou learned about latency and throughput at dif ferent layers of architecture and how they relate to each other . For high-performant applications, you need to have low latency and high throughput at every layer of the architecture. Concurrency helps toprocess a lar ge number of requests. Y ou also learned the dif ference between parallelism and concurrency and got an insight into how caching can help toimprove overall application performance. Then you learned about how to choose your technology and their workingmodels, which can help to achieve your desired application performance.While looking at the compute option, you learned about the variousprocessor types and their dif ferences to help you make the right choice when selecting server instances. Y ou learned about containers and how they can help you to utilize the resources ef ficiently and at the same time help to improve performance. Y ou also learned how Docker and Kubernetes work well with each other and fit into your architecture. In the section on choosing storage, you learned about dif ferent kinds of storage, such as block, file, and object storage and their dif ferences. Y ou also learned about the available storage choices in on-premise and cloudenvironments. Storage choice depends on multiple factors. Y ou can enhance disk storage durability and throughput by putting multiple volumes in aRAID configuration. In the section on choosing a database, you learned about the variousdatabase types, including relational, nonrelational, data warehouse, and datasearch. While looking at choosing your network, you learned about thevarious request routing strategies that can help you to improve networklatency for your globally distributed user . Y ou learned how load balancers and autoscaling can help you to manage a lar ge number of user requests without compromising application performance. In the next chapter , you will learn about how to secure your application by applying authentication and authorization. This will ensure that your data atrest and in transit and your application is protected from various kinds ofthreats and attacks. Y ou will also learn about compliance requirements and how to satisfy them when designing your application. Y ou will learn the details about security audits, alerts, monitoring, and automation. Security Considerations Security is always at the center of architecture design. Many lar ge enterprises suf fer financial loss due to security breaches when their customer data gets leaked. Or ganizations not only lose customer trust but also lose the entire business. There are many industry-standard compliancesand regulations out there to make sure your application is secur e and protects customer -sensitive data. Security isn't just about getting through the outer boundary of yourinfrastructure. It's also about ensuring that your environments and theircomponents are secured from each other . In the previous chapter , you learned about various performance-improvement aspects and technologychoices for your architecture. In this chapter , you will gain an understanding of best practices to secure your application and make sure it is compliantwith industry-standard regulations. For example, in the server , you can set up a firewall that allows you to determine which ports on your instances can send and receive traf fic, and where that traf fic can come from. Y ou can use firewall protection to reduce the probability that a security threat on one instance will not spread to otherinstances in your environment. Similar precautions are required for otherservices. Specific ways to implement security best practices are discussedthroughout this chapter . Y ou will learn the following best security practices in this chapter: Designing principles for architectural securitySelecting technology for architectural security Security and compliance certificationsThe cloud's shared security responsibility model Y ou will learn various design principles applicable to secure your solution architecture. Security needs to be applied at every layer and in every component of the architecture. Y ou will get an understanding of the right technology to select to ensure your architecture is secure at every layer .  Designing principles forar chitectural security Security is all about the ability to protect your system and informationwhile delivering business value for your customers. Y ou need to conduct an in-depth risk assessment and plan a mitigation strategy for the continuousoperation of your business. The following sections talk about the standarddesign principles that help you to strengthen your architectural security . Implementing authentication andauthorization contr ol The purpose of authentication is to determine if a user can access thesystem with the provided credentials of user ID and password. Whileauthorization determines what a user can do once they are inside thesystem, you should create a centralized system to manage your user'sauthentication and authorization. This centralized user management system helps you to keep track of theuser's activity so you can deactivate them if they are no longer a part of thesystem. Y ou can define standard rules to onboard a new user and remove access for inactive users. The centralized system eliminates reliance onlong-term credentials and allows you to configure other security methodssuch as  passwor d r otation and str ength . For authorization, you should start with the principle of least privilege—itmeans users should not have any access to begin with, and then beginassigning them only the required access according to their job role. Creatingan access group according to job role helps to manage the authorizationpolicy in one place and apply  authorization restrictions  across a lar ge number of users. For example, you can restrict the development team tohave full access to the dev environment and read-only access to theproduction environment. If any new developer joins, they should be addedto this dev group, where all authorization policies are managed centrally . Enabling single sign-on with a centralized users' repository helps to reducethe hassle of remembering multiple passwords for your user base andeliminates any risk of password leakage. Lar ge or ganizations use centralized user management tools such as Active Dir ectory ( AD ) for employee authentication and authorization, to provide them access toi nternal enterprise applications such as the HR system, the expense system, the timesheet application, and so on. In a customer -facing application, such as e-commerce and social media websites, you can use an OpenID authentication system to maintain acentralized system. Y ou will learn about lar ge-scale user management tools in more detail in the  OAuth section of this chapter . Applying security everywher e Often, or ganizations have a main focus of ensuring the physical safety of their data center and protecting the outer networking layer from any attack.Instead of just focusing on a single outer layer , ensure that security is applied at every layer of the application. Apply the defense-in-depth ( DiD ) approach, and put security at various layers of the application; for example, a web application needs to besecured from an external internet traf fic attack by protecting the Enhanced Data Rates for Global Evolution ( EDGE ) network and Domain Name System ( DNS ) routing. Apply security at the load balancer and network layers to block any malicious traf fic. Secure every instance of your application by allowing only requiredincoming and outgoing traf fic in the web application and database layer . Protect operating systems with antivirus software to safeguard against anymalware attack. Apply both proactive and reactive measures of protectionby putting intrusion detection and intrusion prevention systems in front ofyour traf fic flow and W eb Application Fir ewall ( W AF ) to protect your application from various kinds of attacks. Y ou will learn more details about the various security tools to use in the Selecting technology for ar chitectural security section of this chapter . Reducing blast radius While applying security measures at every layer , you should always keep your system isolated in a small pocket to reduce the blast radius. If attackersget access to one part of the system, you should be able to limit a securitybreach to the smallest possible area of the application. For example, in aweb application, keep your load balancer in a separate network from otherlayers of the architecture, as that will be internet-facing. Further , apply network separation at the web, application, and database layers. In any case,if an attack happens in one layer , it will not expand to other layers of the architecture. The same rules are applied to your authorization system to give the leastprivilege to users and provide only the minimum required access. Makesure to implement multi-factor authentication ( MF A ) so that even if there's a breach in user access, it always needs a second level ofauthentication to get into the system. Provide minimal access to ensure that you are not exposing the entiresystem and provide temporary credentials to make sure access is not openfor a long time. T ake particular caution when providing programmatic access by putting a secure token in place, with frequent key rotation. Monitoring and auditingeverything all the time Put the logging mechanism for every activity in your system and conduct aregular audit. Audit capabilities are often also required from variousindustry-compliance regulations. Collect logs from everycomponent,  including all transactions and each API call,  to put centralized monitoring in place. It is a good practice to add a level of security andaccess limitation for a centralized logging account so that no one is able totamper with it. T ake a proactive approach and have the alert capability to take care of any incident before the user gets impacted. Alert capabilities with centralizedmonitoring help you to take quick action and mitigate any incident. Monitorall user activity and application accounts to limit the security breach. Automating everything Automation is an essential way to apply quick mitigation for any security-rule violation. Y ou can use automation to revert changes  against desired configurations  and alert the security team—for example, if someone added admin users in your system and an open firewall to an unauthorized port orIP address. Applying automation in security systems has become popularwith the concept of DevSecOps. DevSecOps is about adding security atevery part of application development and operation. Y ou will learn more about DevSecOps in  Chapter 12 , DevOps and Solution Ar chitectur e Framework . Create secure architectures and implement security control that is definedand managed as code. Y ou can version-control your security as a code template, and analyze changes as required. Automated security mechanismsas software code help you scale security operations more rapidly , in a cost- ef fective way . Pr otecting data Data is at the center of your architecture, and it is essential to secure andprotect it. Most of the compliance and regulation in place are there toprotect customer data and identity . Most of the time, any attack has the intention of stealing the user's data. Y ou should categorize your data as per its sensitivity level and protect it accordingly . For example, customer credit card information should be the most sensitive data and needs to be handledwith the utmost care. However , a customer's first name may not be that sensitive compared to their password. Create mechanisms and tools that should minimize the need for directaccess to data. A void  manual processing of data by applying tool-based automation that eliminates human error , especially when handling sensitive data.  Apply access restrictions to the data wherever possible to reduce the risk of data loss or data modification. Once you categorize data sensitivity , you can use the appropriate encryption, tokenization, and access control to protect the data. Data needsto be protected not only at rest but when transmitting over the network aswell. Y ou will learn about various mechanisms to protect data in the Data security section of this chapter . Pr eparing a r esponse Keep yourself ready for any security events. Create an incidentmanagement process as per your or ganizational policy requirements. Incident management can dif fer from one or ganization to another and from one application to another . For example, if your application is handling Personal Identifiable Information ( PII ) of your customers, you need a tighter security measure in your incident response. However , if the application is handling small amounts of sensitive data, such as aninventory management application, then it will have a dif ferent approach. Make sure to simulate the incident response to see how your security teamis recovering from the situation. Y our team should use automation tools for speed of detection, investigation, and response to any security event. Y ou need to set up the alert, monitor , and audit mechanisms to do Root Cause Analysis ( RCA ) to prevent such events occurring again. In this section, you learned about the general security  principles  to apply in your architecture for application security . In the next section, you will learn how to apply these principles using dif ferent tools and techniques. Selecting technology forar chitectural security The previous section was more focused on the general rules of applicationsecurity to consider while creating an architecture design, but the questionis: How do we apply these rules to make the application secur e during implementation? There are various tools and technologies available for each layer of your application to make it secure. In this section, you will learn in detail about the multiple technologychoices to apply in the area of user management and protection of the web,infrastructure, and data of your application. Let's start with the firstarea, user identity and access management. User identity and accessmanagement User identity and access management are vital parts of information security . Y ou need to make sure only authenticated and authorized users are able to access your system resources in a defined manner . User management could be a daunting task as your or ganization and product adoption grows. User access management should dif ferentiate and manage access to an  or ganization's employees , vendors, and customers. Enterprise or corporate users could be the or ganization's employees, contractors, or vendors. Those are specialist users who have a specialprivilege to develop, test, and deploy the application. In addition to that, theyrequire access to another corporate system to do their daily job—forexample, an  Enterprise Resour ce System  ( ERP ), a payroll system, an HR system, a timesheet application, and so on. As your or ganization grows, the number of users can grow from hundreds to thousands. The end users are the customers who use your applications and haveminimal access to explore and utilize the desired feature of the application—for example, players of a gaming application, users of social mediaapplications, or customers of an e-commerce website. The count of theseusers could be from hundreds to thousands to millions (or even more) as thepopularity of your product or application grows. The other factor is that usercount can grow exponentially , which can add challenges. Y ou need to take special care of security when exposing the application to external-facinginternet traf fic to protect it from various threats. Let's talk about corporate user management first. Y ou need to have a centralized repository where you can enforce security policies such as strongpassword creation, password rotation, and multi-factor authentication ( MF A ) for better user management. The use of MF A provides another means of validating someone's identity , if a password may have already compromised. Popular MF A providers include Google Authenticator , Gemalto, Y ubiKey , RSA SecureID, Duo,  and Microsoft Authenticator . From a user -a ccess prospective,   r ole-based authentication ( RBA ) simplifies user management; you can create user groups as per the user's roleand assign an appropriate access policy . As illustrated in the following diagram, you have three groups—admin, developer , and tester—with the corresponding access policy applied to the individual group. Here, admincan access any system, including production, while developer access islimited to the dev environment, and so the tester can only access the testenvironment: U s e r g r o u p o r g a n i z a t i o n As shown in the preceding diagram, when any new user joins the team, theyget assigned to the appropriate group as per their role. In this way , each user has a defined set of standard access. The user group also helps to updateaccess in case a new development environment gets introduced, and alldevelopers need to have access to that. Single Sign-On ( SSO ) is the standard process to reduce any security lapses and help to automate the system. SSO provides users with a login to thedif ferent corporate systems, with a single user ID and password. Federated Identity Management ( FIM ) allows users to access the system without a password with a pre-authenticated mechanism. Let's look at some moredetails. FIM and SSO FIM provides a way to connect the identity management system when userinformation is stored in the third-party identity pr ovider ( IdP ). W ith   FIM, the user only provides authentication information to the IdP , which in turn already has a trusted relationship with the  service .  As illustrated in the following diagram, when a user logs in to access aservice , the servic e pr ovider ( SP ) gets credentials from the IdP , rather than getting them directly from the user:    F I M SSO allows the use of a single sign-on, with which the user can accessmultiple services. Here, an SP could tar get an environment where you want to log in—for example, a Customer Relationship Management ( CRM ) application or your cloud application. An IdP could be a corporate AD.Federation allows something similar to an SSO without a password, as thefederation server knows users to access information. There are various techniques available to implement FIM and SSO. Let'slook at some of the popular Identify and Access Management  ( IAM ) choices available.Kerber os Kerberos is an authentication protocol that allows two systems to identifyeach other in a secure way and helps to implement SSO. It works in theclient-server model and uses a ticket system for user identity . Kerberos has the Key Distribution Center ( KDC ), which facilitates authentication between two systems. The KDC consists of two logical parts—the  Authentication Server ( AS ) and the  T icket-Granting Server ( TGS ). Kerberos stores and maintains the secret keys of each client and server in thedatastore. It establishes a secure session between two systems during theircommunication and identifies them with the stored secret key . The following diagram illustrates the architecture of Kerberos authentication: K e r b e r o s a u t h e n t i c a t i o n As shown in the preceding diagram, when you want to access a service, thefollowing steps are involved: 1 . The client sends an access ticket request to the AS as a plaintextrequest. This request contains the client ID, TGS ID, IP address, and authentication time. 2 . The AS checks if your information is available in the KDC database.Once AS has found your information, it establishes a session betweenthe client request and the TGS. The AS then replies to the client withthe  T icket-Granting T icket ( TGT ) and the TGS session key . 3 . Now , the TGS session key asks for a password, and, given the correct password, a client can decrypt the TGS session key . However , it cannot decrypt the TGT since the TGS secret key is not available. 4 . Now , the client sends the current TGT to the TGS with the authenticator . The TGS contains the session key along with the client ID and Service Principal Name ( SPN ) of the resource the client wants to access. 5 . Now , the TGS again checks if the requested service address exists in the KDC database. If yes, the TSG will then encrypt the TGT and senda valid session key for the service to the client. 6 . The client forwards the session key to the service to prove that the userhas access, and the service grants access. While Kerberos is an open source protocol, lar ge enterprises like to use more managed software with robust support, such as AD. Let's look at theworking mechanism of one of the most popular user management tools,Microsoft AD, which is based on the Lightweight Dir ectory Access Pr otocol ( LDAP ). AD AD is an identity service developed by Microsoft for users and machines.AD has a domain controller , also known as Active Dir ectory Domain Service ( AD DS ), which stores the user's and the system's information, their access credentials, and their identity . The following diagram illustrates a simple flow of the necessary authentication process:   A D a u t h e n t i c a t i o n f l o w As shown in the preceding diagram, the user login is managed by AD orany resource on the domain networks. Users first send the request to thedomain controller with their credentials and communicate with the Active Dir ectory Authentication Library ( ADAL ). The ADAL verifies the user credentials and sends back an access token with a continuous session for therequested service. LDAP is the standard protocol that handles the tree-like hierarchicalstructure of information stored in directories. Active Dir ectory Lightweight Dir ectory Services ( AD LDS ) provides an LDAP interface to the directory of users and systems. For file encryption and network traf fic encryption, Active Dir ectory Certificate Services ( AD CS ) provides the key infrastructure functionality . Active Dir ectory Federation Service  ( ADFS ) provides access mechanisms for external resources such as web app logins for a lar ge number of users. Amazon W eb Services (A WS) Dir ectory Service A WS Directory Service helps to connect A WS resources in your account with an existing on-premises user management tool such as AD. It helps toset up a new user management directory in the A WS cloud. A WS Directory Service facilitates a secure connection to the on-premises directory . After establishing the connection, all users can access cloud resources and on-premises applications with their already existing credentials. A WS AD Connector is another service that helps you to connect the existing Microsoft AD to the A WS cloud. Y ou don't need any specific directory synchronization tool. After setting up an AD connection, userscan utilize their existing credentials to log on to A WS applications. Admin users can manage A WS resources, using A WS IAM.  AD  Connector helps to enable MF A by integrating with your existing MF A infrastructure, such as Y ubiKey , Gemalto token, RSA token, and so on. For a smaller user base (fewer than 5,000 users), A WS provides Simple AD, which is a managed directory powered by Samba 4 Active Dir ectory Compatible Server . Simple AD has common features such as user accounts management, user group management, SSO based on  Kerberos , and user group policies. In this section, you have learned a high-level overview of AD and managedAD services provided by Microsoft and Amazon. The other directoryservices provided by major technology companies include Google CloudIdentity , Okta, Centrify , Ping Identity , and Oracle Identity Cloud Service ( IDCS ). Security Assertion MarkupLanguage (SAML) Earlier in this section, under FIM and SSO, you learned about IdPs and SPs (short for service pr oviders ). T o access a service, the user gets validated from the IdP , which in-turn has a trusted relationship with the SP . SAML is one of the mechanisms to establish a trusted relationship between an IdP andan SP . SAML uses  extensible markup language ( XML ) to standardize communication between an IdP and an SP . SAML enables SSO, so users can use a single credential to access multiple applications. A SAML assertion is an XML document that the IdP sends to the SP withuser authorization. The following diagram illustrates the flow of the SAMLassertion:   U s e r a u t h e n t i c a t i o n u s i n g S A M L As mentioned in the preceding diagram, the following steps are taken toimplement user authentication using SAML: 1 . A user sends a request to access the service—for example, theSalesforce CRM application— as a service provider . 2 . The service provider (CRM application) sends a SAML request withthe user information to the SAML IdP . 3 . The SAML IdP pops up the SSO login page, where users enterauthentication information. 4 . The user access credential goes to the identity store for validation. Inthis case, user identify store is an AD . 5 . The user identity store sends user validation status to the SAML IdP , with whom the identity store has a trusted relationship. 6 . The SAML IdP sends a SAML assertion to the service provider (aCRM application) with information pertaining to user verification. 7 . After receiving the SAML response, the service provider allowsapplication access to the user . Sometimes, service providers can act as an identity provider as well. SAMLis very popular for establishing a relation between any identity store andservice provider . All modern identity store applications are SAML 2.0- compatible, which allows them to communicate with each other seamlessly . SAML allows user identity to be federated and enables SSO for enterpriseusers. However , for lar ge user bases such as social media and e-commerce websites, OAuth ( short for Open Authorization ) and  OpenID are more suitable. Let's learn more about OAuth and OpenID Connect. OAuth and OpenID Connect(OIDC) OAuth is an open standard authorization protocol that provides secureaccess to an application. OAuth provides secure access delegation. OAuthdoesn't share password data but uses the authorization token to establish theidentity between service providers and consumers. Users of an applicationprovide access to their information without giving login credentials. WhileOAuth is mainly for authorization, many or ganizations have started adding their own mechanisms for authentication. OpenID Connect defines theauthentication standard on top of OAuth authorization . Lar ge technology companies such as Amazon, Facebook, Google, and T witter allow the user to share information in their account with third-party applications. For example, you can log in to a new photo app using yourFacebook login and authorize the new app to access only your Facebookphoto information. The following diagram illustrates an OAuth accessdelegation flow: U s e r a c c e s s d e l e g a t i o n w i t h O A u t h 2 . 0 As shown in the preceding diagram, the authentication flow follows thesesteps: 1 . Y ou want a photo app to get your profile photo from Facebook. 2 . The photo app requests authorization to access Facebook profilephotos. 3 . The authorization server (which is your Facebook account in this case)creates and displays a consent screen to you. 4 . Y ou provide your consent to the request for the photo app to access only your Facebook profile photos. 5 . After getting your approval, the authorization Facebook server sendsan authorization code back to the requesting photo app. 6 . The photo app then requests an access token from the authorizationserver (Facebook account) using the authorization code. 7 . The authorization server identifies the photo app and checks thevalidity of the authentication code. 8 . If the access token is validated, the server issues an access token to thephoto app. 9 . The photo app can now access resources such as Facebook profilephotos using the access token. OAuth 2.0, which is faster than OAuth 1.0 and more comfortable toimplement,  is now most commonly used . JSON W eb T oken  ( JWT ) is a simple and accessible token format that can be used with OAuth and ispopular with OpenID. JWT tokens have a JSON structure that hasinformation about expiration time, issuer , subject, and so on. It is more robust than Simple W eb T oken ( SWT ) and simpler than SAML 2.0. In this section, you learned about the most common user management toolsand services. However , there are various other protocols and services available for user authentication and authorization. Implementation of theprotocols mentioned previously can be complicated, and there is a lar ge amount of packaged software available that makes the job easier .  Amazon Cognito is a user access management service provided by A WS that includes standard-based authorization such as SAML 2.0, OpenIDConnect, and OAuth 2.0, along with an enterprise user directory thatprovides the ability to connect with AD. Okta and Ping Identity provideenterprise user management and the ability to communicate with variousservice provider tools in one place. Once your application is exposed to the internet, there are always variouskinds of attacks bound to happen. Let's learn about some most commonattacks, and how to set up the first layer of defense for web-layer protection. Handling web security As user demand is changing to require 24/7 availability of services,businesses are evolving to go into online mode and adopting webapplication models. W eb applications also help a company to gain a global customer base. Businesses such as online banking and e-commercewebsites are always available, and they deal with customers' sensitive datasuch as payment information and payer identity . Now , web applications are central to any business, and these applications are exposed to the world. W eb applications can have vulnerabilities, which makes them exposed to cyber -attacks and data breaches. Let's explore some common web vulnerabilities and how to mitigate them. W eb app security vulnerabilities A web application is prone to security breaches as hackers orchestratecyber -attacks from dif ferent locations and by various methods. A web application is more vulnerable to theft than a physical store location. Just asyou lock and protect your physical shop, in the same way , your web app needs to protect itself from unwanted activity . Let's explore some standard methods of attack that can cause security vulnerabilities in your webapplication. Denial of Service (DoS) andDistributed Denial of Service(DDoS) attacks A DoS attack attempts to make your website unreachable to your users. T o achieve a successful DoS attack, the attacker uses a variety of technologiesthat consume network and system resources, thus interrupting access forlegitimate users. The attacker uses multiple hosts to orchestrate the attackagainst a single tar get. A DDoS attack is a type of DoS attack where multiple compromisedsystems (typically infected with T r ojans ) are used to tar get a single system. V ictims of a DDoS attack find that all their systems are maliciously used and controlled by the hacker in the distributed attack. As illustrated in thefollowing diagram, a DDoS attack happens when multiple systems exhaustthe bandwidth of resources of a tar geted system: D D o S a t t a c k The general concept of a DDoS attack is to leverage additional hosts toamplify the requests made to the tar get, rendering them overprovisioned and unavailable. A DDoS attack is often the result of multiple compromisedsystems, whereby a botnet puts a flood of traf fic in the tar geted system. The most common DDoS attack happens at the application layer , using either a DNS flood or a Secur e Sockets Layer ( SSL ) negotiation attack. In DNS floods, attackers exhaust the resources of a DNS server with too manyrequests. During SSL negotiations, attackers send a lar ge amount of unintelligible data for computationally expensive SSL decryption. Theattacker can perform other SSL-based attacks to the server fleet andoverburden it with unnecessary task  processing . At the infrastructure layer , a typical DDoS attack happens in the form of the following: User Datagram Pr otocol (UDP) r eflection : W ith  UDP reflection, attackers spoof the tar get server's IP address and make a request that returns amplified significant responses from a hacked reflector server . SYN floods : W ith SYN floods, attackers exhaust the tar get server's T ransmission Contr ol Pr otocol ( TCP ) service by creating and abandoning high numbers of connections, blocking legitimate usersfrom accessing the server . Often, attackers try to get sensitive customer data, and for that purpose, theyuse a dif ferent kind of attack called SQL injection (SQLi)  attacks. Let's learn more about them. SQLi attacks As the name suggests, in an SQLi attack, attackers inject maliciousStructur e Query Language  ( SQL ) to get control of a SQL database and fetch sensitive user data. The attacker uses SQLi to gain access tounauthorized information, take control of an application, add new users, andso on. T ake an example of a loan-processing web application. Y ou have loanId  as a field that customers can use to get all information related to their loanfinance. The typical query will look like this:  SELECT * FROM loans WHERE loanId = 117 . If proper care is not taken, attackers can execute a query such as  SELECT * FROM loans WHERE loanId = 117 or ‘1=1'  and get access to the entire customer database, as this query will always return the true result.  The other common method to hack user data through script injection iscr oss-site scripting ( XSS ) where a hacker impersonate himself as legitimate user . Let's explore more about it. XSS attacks Y ou must have encountered  phishing  emails that have links impersonating a website known to you. Clicking on these links may lead to compromiseddata through XSS. W ith XSS, the attacker attaches their code to a legitimate website and executes it when the victim loads the web page. The maliciouscode can be inserted in several ways, such as in a URL string or by puttinga small JavaScript code on the web page. In an XSS attack, the attacker adds a small code snippet at the end of theURL or client-side code. When you load the web page, this client-sideJavaScript code gets executed and steals your browser cookies. Thesecookies often contain sensitive information, such as the access token andauthentication to your banking or e-commerce websites. Using these stolencookies, the hacker can get into your bank account and take your hard-earned money .  Cr oss-Site Request Forgery (CSRF) attacks A CSRF attack takes advantage of user identity by creating confusion. Ittypically  tricks the user with a transaction activity in which the state gets changed—for example, changing the password of a shopping website orrequesting a money transfer to your bank. It is slightly dif ferent than an XSS attack as, with CSRF , the attacker tries to for ge the request rather than insert a code script. For example, the attacker can for ge a request to transfer a certain amount of money from the user's bank and send that link in an email to the user . As soon as users click on that link, the bank gets a request and transfers the money to the attacker'saccount. CSRF has minimal impact on the individual user account, but itcan be very harmful if attackers are able to get into the admin account. Buffer overflow and memorycorruption attacks A software program writes data in temporary memory area for fastprocessing, which is called a  buffer . W ith a buf fer overflow attack, an attacker can overwrite a portion of memory connected with the buf fer . An attacker can deliberately cause a buf fer overflow and access connected memory , where an application executable may be stored. The attacker can replace the executable with the actual program and take control of the entiresystem. Buf fer overflow attacks can cause memory corruption with unintentional memory modification, which the hacker can use to injectcode. Looking at the overall application, there are more security threats that existat the infrastructure layer , network layer , and data layer . Let's explore some standard methods to mitigate and prevent security risks at the web layer . W eb security mitigation Security needs to be applied to every layer , and special attention is required for the web layer due to its exposure to the world. For web protection,important steps include keeping up with the latest security patches,following the best software development practices, and making sure properauthentication and authorization are carried out. There are several methodsto protect and secure web applications; let's explore the most commonmethods. W eb Application Fir ewall (W AF) W AFs are necessary firewalls that apply specific rules to HTTP and HTTPS traf fic (that is, port 80 and 443). W AFs are software firewalls that inspect your web traf fic and verify that it conforms to the norms of expected behavior .  W AFs provide an additional layer of protection from web attacks. W AF rate limiting is the ability to look at the amount or type of requests sent to your service and define a threshold that caps how many requests areallowed per user , session, or IP address. Whitelists and blacklists allow you to allow or block users explicitly . A WS W AF helps you to secure your web layer by creating and applying rules to filter web traf fic. These rules are based on conditions that include  HTTP headers, user geolocation, malicious  IP addresses, or custom Uniform Resour ce Identifiers ( URIs ), and so on. A WS W AF rules block common web exploits such as  XSS and  SQLi. A WS W AF provides a centralized mechanism in the form of rules that can be deployed across multiple websites. This means that you can create asingle set of rules for an environment that has various websites and webapplications running. Y ou can reuse rules across applications instead of recreating them. Overall, W AF is a tool that applies a set of rules to HTTP traf fic. It helps to filter web requests based on data such as IP addresses, HTTP headers,HTTP body , or URI strings. It can be useful for mitigating DDoS attacks by of floading illegitimate traf fic. Let's learn more about DDoS mitigation. DDoS mitigation R esilient architecture can help to prevent or mitigate DDoS attacks. A fundamental principle in keeping your infrastructure secure is reducing thepotential number of tar gets that an attacker can hit. In short, if an instance doesn't need to be public, then don't make it public. An application-layerattack can spike monitoring metrics such as network utilization for yourcontent distribution network ( CDN ), load balancer , and server metrics due to  HTTP flood . Y ou can apply various strategies to minimize the attack surface area: Wherever possible, try to reduce the number of necessary internet entrypoints. For example, open incoming internet access to your loadbalancer , not web servers. Hide any required internet entry points from  untrusted end users so  that they cannot access them.Identify and remove any non-critical internet entry points—forexample, expose file-share storage for vendors to upload data withlimited access, rather than exposing it to worldwide internet traf fic. Isolate the access point and apply specific restrictions policy for enduser traf fic compared to application management traf fic. Create a decoupled internet entry point to minimize the attack surface. Y our primary goal is to mitigate DDoS attacks at the Edge location of the CDN. It's more challenging and costly to handle DDoS attacks if they getthrough to your application servers. The following diagram illustrates aDDoS mitigation example for an A WS cloud workload: D D o S W A F s a n d w i c h m i t i g a t i o n s t r a t e g y The preceding diagram illustrates a  W AF sandwich ar chitectur e , where the W AF appliance is staged between a load balancer to handle a DDoS attack. Frequent DDoS attacks come from attacking strategies such as SYN floodsand UDP reflection, which  Amazon CloudFront prevents by only accepting well-formed connections before  the attacking strategy can reach your application servers. CDNs such as Amazon CloudFront help to tackle  DDoS attacks by isolating them at  a geographically isolated location and preventing the traf fic from af fecting other locations. Network firewall security helps you to control incoming and outgoing traf fic at an individual server level. As mentioned in the previous section, W AFs are used to protect web applications against exploit attacks such as XSS and SQLi attacks. Inaddition to this, W AFs also help to detect and prevent DDoS attacks at the web application layer . T o handle a DDoS attack, you can apply either horizontal or vertical scaling. Y ou can take advantage of scaling in the following way: 1 . First, select the right server size and configuration for your webapplication. 2 . Second, apply a load balancer to distribute traf fic among the fleet of servers and add autoscaling to add/remove servers as required. 3 . Finally , use the CDN and DNS server , as they are built to handle traf fic at scale. Scaling for DDoS attacks is an excellent example of why it's essential to setreasonable maximum counts for your servers. A DDoS attack could scaleyour servers out to a count that would be extremely costly , while still potentially not being able to avoid becoming unavailable. Having reasonablemaximum limits for expectations of regular traf fic spikes would prevent a DDoS attack costing your company too much money . In this section, you learned about various security risks and vulnerabilities atthe web layer and some standard methods to protect them. As security needsto be applied to every layer , let's explore more about the protection of the infrastructure layer . Securing an application and itsinfrastructur e Security isn't just about getting through the outer boundary of yourinfrastructure. It's also about ensuring that your environments and theircomponents are secured from each other . Y ou should apply security at each layer of the application, such as the web, application, and data layer . Y ou should add  security controls to all of the system's resources such as load balancers, network topology , database, and all servers.  For example, in a server instance, you can set up a firewall that allows youto determine which ports on your instances can send and receive traf fic. Y ou can use this to reduce the probability that a security threat on one instance will spread to every other instance in your environment. Similarprecautions should be taken with other services. Specific ways toimplement this best practice are discussed throughout this section. Application and operating systemhardening It is not possible to entirely eliminate vulnerabilities in your application, butyou can limit system attacks by hardening your application's operatingsystem, filesystem, and directory . Once attackers can get into your application, they can get root access and orchestrate an attack on the entireinfrastructure. It is essential to limit attacks to the application level byrestricting the directory by har dening permission . At the process level, restrict memory and CPU utilization to prevent a DOS attack. Set the right permission at the file, folder , and file partition levels, which is the only requirement for the application to execute. A void giving root privilege to the application or their user . Y ou should create a separate user and directory , with only required access for each application. Don't use common access for all applications. Automate application restart by using tools and avoid a manual approach,whereby users need to log in to the server to start. Y ou can use process control tools such as  DAEMON T ools and Supervisord to automate an application restart. For a Linux operating system, a utility such as systemd or System V init scripts help to start/stop the application. Softwar e vulnerabilities and secur e code It is always recommended to apply the latest security patch to youroperating system provided by your operating system vendor . This helps to fill any security holes in the sy stem and protect your system from vulnerabilities where attackers are able to steal your security certificate orrun arbitrary code. Make sure to   integrate secure coding best practice to your software development process, as recommended by the Open W eb Application Security Pr oject ( OW ASP :  https://owasp.org/www-project-top- ten/ ). Keeping your system up to date with the latest security patch is veryimportant. It is better to automate the process of the most recent patchinstallation as soon as it becomes available. However , sometimes, running a security patch may break your working software, so it's better to set upa  Continuous Integration and Continuous Deployment ( CI/CD ) pipeline with automated test and deployment. Y ou will learn more about the CI/CD process in Chapter 12 , DevOps and Solution Ar chitectur e Framework . A WS cloud provides a system manager tool that allows you to apply security patches and monitoring of your server fleet in the cloud. Y ou can use a tool such as auto-updates  or u nattended-upgrades to automate security patch installation.  Network, fir ewall, and trusted boundary When it comes to protecting your infrastructure, securing the network comesinto consideration first. The physical security of your IT infrastructure in thedata center is to be taken care of by providers. In the case of cloud-like A WS providers, they take the utmost care of the physical security of yourinfrastructure. Let's talk about ensuring network security , which is your responsibility as an application owner . T o understand it better , let's take an example from a public cloud provider such as A WS and apply the same example to your on-premises or private cloud network infrastructure as well. As illustrated in the following diagram,you should apply security at every layer and define trusted boundariesaround each layer , with minimal access: N e t w o r k c o n f i g u r a t i o n f o r i n f r a s t r u c t u r e s e c u r i t y   In the preceding diagram, the load balancer is in a public subnet, which canaccept internet traf fic and distribute it to the application server fleet. W AF filter traf fic is based on set rules and protects your application from various attacks, as you learned in the previous section. The application server fleetand database servers are in a private subnet, which means direct internetaccess is not reachable to  expose  them to the public internet. Let's dive deep into the preceding architecture diagram and walk through each layer , as follows: Amazon V irtual Private Cloud ( VPC ) provides you with logical network isolation of your infrastructure. Amazon VPC is your networkenvironment in the cloud. It's where you will launch many of your resources. It's designed to provide greater control over the isolation ofyour environments and their resources from each other . Y ou can have multiple VPCs in each account or region. When you create a VPC, you specify its set of IP addresses withClassless Inter -Domain Routing ( CIDR ) notation. CIDR notation is a simplified way of showing a specific range of IP addresses. Forexample, 10.0.0.0/16 covers all IPs from 10.0.0.0 to 10.0.255.255,providing 65,535 IP addresses to use.Subnets are segments or partitions of a network divided by the CIDRrange. They create trusted boundaries between private and publicresources. Rather than defining your subnets based on the application orfunctional tier (web/app/data), you should or ganize your subnets based on internet accessibility . A subnet allows you to define clear , subnet- level isolation between public and private resources.In this environment, all of your resources that require direct access tothe internet (public-facing load balancers, Network Addr ess T ranslation ( NA T ) instances, bastion hosts, and so on) would go into the public subnet, while all other instances (such as database andapplication resources) would go into your private subnet. Use subnetsto create layers of separation between tiers  of resources, such as putting your application instances and your data resources into separate privatesubnets.The majority of resources on A WS can be hosted in private subnets, using public subnets for controlled access to and from the internet asnecessary . Because of this, you should plan your subnets so that your private subnets have substantially more IPs available compared to yourpublic subnets.While subnets can provide a fundamental element of segregationbetween resources using network Access Contr ol List ( ACL ) rules, security groups can give an even more fine-grained level of traf fic control between your resources, without the risk of overcomplicatingyour infrastructure and wasting or running out of IPs.A routing table contains a set of rules, called r outes.  Routes determine which application servers are to receive network traf fic. For better security , use the custom route table for each subnet. Security groups are the virtual firewalls that control inbound andoutbound traf fic for one or more instances from the CIDR block range, or another security group, as designated resources. As per the principleof least privilege, deny all incoming traf fic by default and create rules that can filter traf fic based on TCP , UDP , and Internet Contr ol Message Pr otocol ( ICMP ) protocols. A Network Access Contr ol List ( NACL ) is an optional virtual firewall that controls inbound and outbound traf fic at the subnet level. An NACL is a stateless firewall that is compared to the security group,which is stateful. This means that if your incoming request is allowed,then the outbound request does not have to be inspected or tracked.While stateless, you have to define both inbound and outbound traf fic rules explicitly . Internet traf fic routed through an internet gateway ( IGW ) to make a subnet public. By default, internet accessibility is denied for internettraf fic in your environment. An IGW needs to be attached to your VPC, and the subnet's route table should define the rules to the IGW . A private subnet blocks all incoming and outgoing internet traf fic, but servers may need outgoing internet traf fic for software and security patch installation. A NA T   gateway enables instances in a private subnet to initiate outbound traf fic to the internet and protects resources from incoming internet traf fic. A bastion host acts like a jump server , which allows access to other resources in the private subnet. A bastion host needs to be hardenedwith tighter security so that only appropriate people can access it. T o log in to the server , always use public-key cryptography for authentication rather than a regular user ID and password method. Many or ganizations typically collect, store, monitor , and analyze network flow logs for various purposes, including troubleshooting connectivity andsecurity issues and testing network access rules. Y ou need to monitor traf fic flow to your system VPC, which includes recording incoming and outgoingtraf fic information from your network. VPC Flow Logs enables you to capture that information, along with accepted and rejected traf fic information for the designated resource to understand traf fic patterns better . Flow Logs can also be used as a security tool for monitoring traf fic that is reaching your instance. Y ou can create alarms to notify if certain types of traf fic are detected. Y ou can also create metrics to help you to identify trends and patterns. Y ou can create a flow log for a VPC, a subnet, or a network interface. If you cre ate a flow log for a subnet or VPC , each network interface in the VPC or subnet is monitored. As you can see, there are multiple layers for security available at the networklayer that can help to protect your infrastructure. Keeping resources in theirisolated subnet helps to reduce the blast radius. If an attacker can penetrateone component, you should be able to restrict them to limited resources. Y ou can use an  Intrusion Detection System ( IDS ) and an  Intrusion Pr evention System ( IPS ) in front of your infrastructure to detect and prevent any malicious traf fic. Let's learn more about them. IDS/IPS An IDS detects any cyber -attack happening through  network traf fic by recognizing an attack pattern . An IPS goes a step further and helps to stop malicious traf fic proactively . Y ou need to determine the applicability of the IDS/IPS system as per your application's requirements. An IDS can be host-based or network-based.  Host-based IDS In an IDS, a host- or agent-based IDS is running on each host of yourenvironment. It can review the activity within that host to determine if anattack has occurred and has been successful. It can do this by inspectinglogs, monitoring the filesystem, monitoring network connections to thehost, and so on. The software or agent then communicates with acentral/command application about the health or security of the host it ismonitoring. Pros for host-based solutions include that they can deeply inspect theactivity inside each host. They can horizontally scale as far as required(each host gets its own agent), and do not need to impact the performanceof running applications. The cons include the additional configurationmanagement overheads that can be introduced if managing agents on manyservers, which are burdensome for an or ganization. As each agent is operating in isolation, widespread/coordinated attacks canbe harder to detect. T o  handle  coordinated attacks , the system should respond immediately across all hosts, which requires the host-basedsolution to play well with the other components, such as the operatingsystem and the application interface,  deployed on the host. Network-based IDS A network-based IDS inserts an appliance into the network, through whichall traf fic is routed and inspected for attacks. The pros include a simple/single component that needs to be deployed and managed awayfrom the application hosts. Also, it is hardened or monitored in a way thatmight be burdensome across all hosts. An individual/shared view ofsecurity exists in a single place so that the big picture can be inspected foranomalies/attacks. However , a network-based IDS includes the performance hit of adding a network hop to applications. The need to decrypt/re-encrypt traf fic to inspect it is both a massive performance hit and a security risk that makesthe network appliance an attractive tar get. Any traf fic that IDS unable to decrypt cannot inspect/detect anything. An IDS is a detection and monitoring tool and does not act on its own. AnIPS detects, accepts, and denies traf fic based on set rules. IDS/IPS solutions help to prevent DDoS attacks due to their anomaly-detection capabilitiesthat make them able to recognize when valid protocols are used as an attackvehicle. An IDS and an IPS read network packets and compare contents to adatabase of known threats. Continuous auditing and scanning are requiredfor your infrastructure to proactively secure it from any attack, so let's learnmore about this. In this section, you learned all about securing your infrastructure fromvarious types of attacks. The goal of these attacks is to get hold of yourdata. Y ou should secure your data in such a way that an attacker is not able to acquire sensitive information even after getting hold of the data. Let'slearn about data protection using security at the data layer , encryption, and backup. Data security In today's digital world, every system revolves around data. Sometimes, thisdata may contain sensitive information such as customer health records,payment information, and government identity . Securing customer data to prevent any unauthorized access is most important. There are manyindustries that place stress on data protection and security . Before architecting any solution, you should define basic security practicesas per the application objective,  such as complying with regulatory requirements.  There are several dif ferent approaches used when addressing data protection. The following section describes how to use theseapproaches. Data classification One of the best practices is to classify your data, which provides a way tocategorize and handle or ganizational data based on levels of sensitivity . According to data sensitivity , you can plan data protection, data encryption, and data access requirements. By managing data classification as per your system's workloadrequirements, you can create the data controls and level of access neededfor the data. For example, content such as a user rating and review is oftenpublic, and it's fine to provide public access, but user credit cardinformation is highly sensitive data that needs to be encrypted and putunder very restricted access. At a high level, you can classify data into the following categories: Restricted data : This contains information that could harm the customer directly if it got compromised. Mishandling of restricted datacan damage a company's reputation and impact a business adversely . Restricted data may include customer  Personally Identifiable Information ( PII ) data such as social security numbers, passport details, credit card numbers, and payment information.Private data : Data can be categorized as confidential if it contains customer -sensitive information that an attacker can use to plan to obtain their restricted data. Confidential data may include customeremail IDs, phone numbers, full names, and addresses. Public data : This is available and accessible to everyone, and requires minimal protection—for example, customer ratings and reviews,customer location, and customer username if the user made it public. Y ou can have a more granular category depending on the type of industry and the nature of the user data. Data classification needs to balance betweendata usability versus data access. Putting dif ferent levels of access, as mentioned previously , helps to restrict only the necessary data and make sure sensitive data is not exposed. Always avoid giving direct human accessto data and add some tools that can generate a read-only report for users toconsume in a restrictive manner .Data encryption Data encryption is a method to protect your data whereby you convert yourdata from plaintext to encoded ciphertext format using the encryption key . T o read these ciphertexts, they first need to be decrypted using the encryption key , and only authorized users will have access to those decryption keys. Commonly used key-based encryption falls into one oftwo categories of cryptography: Symmetric-key encryption : W ith symmetric encryption algorithms, the same key is used to encrypt and decrypt the data. Each data packetis self-encrypted with a secret key . Data is encrypted while saving and decrypted during retrieval. Earlier , symmetric encryption used to be applied as per the Data Encryption Standard ( DES ), which used a 56-bit key . Now , the  Advanced Encryption Standard  ( AES ) is heavily used for symmetric encryption, which is more reliable as ituses a 128-bit, 192-bit, or 256-bit key . Asymmetric-key encryption : W ith the help of asymmetric algorithms, two dif ferent keys can be used, one to encrypt and one to decrypt. In most cases, the encryption key is a public key and thedecryption key is a private key . Asymmetric key encryption is also known as public-key encryption . Both public and private keys are unidentical , but they are paired together . The private key is only available to one user , while the public key can be distributed across multiple resources. Only the user who has a private key can decryptthe data. Rivest–Shamir–Adleman  ( RSA ) is one of the first and most popular public key-encryption algorithms used to secure datatransmissions over the network. If you are encrypting your data with an AES 256-bit security key , it's become almost impossible to break the encryption. The only way to decryptis by getting hands-on encryption, which means you need to secure yourcode and keep it in a safe place. Let's learn about some essentialmanagement methods to safeguard your encryption key . Encryption key management Key management involves controlling and maintaining your encryption key . Y ou need to make sure that only authorized users can create and access the encryption key . Any encryption key management system handles storage, rotation, and destruction of the key in addition to access management andkey generation. Key management dif fers depending on whether you are using a symmetric or asymmetric algorithm. The following methods arepopular for key management. Envelope encryption Envelope encryption is a technique to secure your data encryption key . Data encryption keys are symmetric keys to increase the performance of dataencryption.  Symmetric e ncryption keys work with an encryption algorithm such as AES and produce ciphertext that you can store safely , as it is not readable by a human. However , you need to save the symmetric encryption data key along with data to use it for data decryption, as needed. Now , you need to further protect the data key in isolation, which is where e nvelope encryption helps you to protect it. Let's understand it in more detail with thehelp of the  following diagram:  E n v e l o p e e n c r y p t i o n  The preceding diagram illustrates the following flow to explain envelopeencryption: 1 . The symmetric key is generated from software or hardware. 2 . The generated  symmetric data key is used to encrypt  plaintext data. 3 . The key encrypts data using an algorithm such as AES and generatesencrypted ciphertext data. 4 . The encrypted data is stored in tar geted storage. 5 . As the data key needs to be stored with ciphered data, the data keyneeds to be encrypted further . The user gets the customer master key stored in the key management systems to encrypt the data key . 6 . The data key is encrypted using the master key . Y our master key is the key encryption key as it encrypts the data encryption key . Only the master key can encrypt multiple data keys, and it is securely stored inthe key management systems, with restricted access. 7 . The master key encrypts data key , and encrypted data key store along withciphered data in storage, where master key securely stores in keymanagement system with restricted access. If a user wants to decrypt data, then they first need a master key that has, inturn, an encrypted data encryption key . This master key can be stored in a separate access system such as a hardwar e security module ( HSM ) or software-based key management service provided by cloud providers suchas A WS. Let's look into this in more detail. A WS Key Management Service (KMS) A WS KMS uses envelope encryption whereby a unique data key encrypts customer data, and KMS master keys encrypt data keys. Y ou can bring your key material to A WS KMS and manage user access, key distribution, and rotation from a centralized place. Y ou can also disable unused keys, and a low number of keys helps to improve the performance of the applicationand encourage better key management. A WS KMS is designed to limit access and protect master keys. KMS helps you to implement key security best practices by never storing plaintextmaster keys on disk or in memory . KMS also gracefully rotates the master key to secure it further . As A WS KMS is a multitenancy key management module, a customer wants to have a dedicated key management module due to compliance.Also, the customer may have an old HSM, and they want to follow thesame model. The HSM is single-tenant hardware in A WS and is called A WS CloudHSM. Y ou can choose your own HSM vendor as well. Let's explore HSM in more depth. Hardwar e security module (HSM) An HSM is a device that is designed to secure encryption keys andassociated cryptographic operations. An HSM is designed with physicalmechanisms to protect the keys, which include tamper detection andresponse. In case of any key tampering, the HSM destroys the keys andprevents any security compromise. An HSM includes logical protections to limit access controls. Logicalseparation helps the HSM appliance administrator to manage the devicesecurely . Access restriction applies rules for users who can connect it to the network and provision the IP address. Y ou can create a separate role for everyone, including security of ficers, appliance admin, and users. As the loss of a key can make your data useless, you need to make sure ofhigh availability for your HSM by maintaining at least two HSMs indif ferent geographic locations. Y ou can use other HSM solutions, such as SafeNet or V oltage. T o protect your key , choose a managed HSM provided by cloud services such as A WS CloudHSM or CipherCloud. Data encryption at r est and in transit Data at rest means it is stored somewhere such as a storage ar ea network ( SAN ) or network-attached storage ( NAS ) drive, or in cloud storage. All sensitive data needs to be protected by applying symmetric or asymmetricencryption, explained in the previous section, with proper key management. Data in transit means data in motion and transferred over the network. Y ou may encrypt data at rest from source and destination, but your data transferpipeline needs to be secure when transferring data. When transferring dataover an unencrypted protocol such as HTTP , it can get leaked by an attack such as an eavesdropping or  man-in-the-middle ( MITM ) attack. In an eavesdropping attack, the attacker captures a small packet from anetwork and uses it to search for any other type of information. A MITMattack is a tampering-based attack, where the attacker secretly alters thecommunication to start communication on behalf of the receiver . These kinds of attacks can be prevented by transferring data over SSL, using astrong protocol such as T ransport Security Layer ( TSL ). Y ou will observe that most websites now use HTTPS protocol for communication, which encrypts data using SSL. By default, HTTP traf fic is unprotected. SSL/TLS protection for HTTP traf fic (HTTPS) is supported by all web servers and browsers. HTTP traf fic is also applicable to service- oriented architectures such as Repr esentational State T ransfer ( REST )- and Simple Object Access Pr otocol ( SOAP )-based architectures.  SSL/TSL handshakes use certificates to exchange a public key usingasymmetric encryption, and then use the public key to exchange a privatekey using symmetric encryption. A security certificate is issued by anacceptable Certification Authority ( CA ) such as V erisign. Procured security certificates need to be secured using a  Public Key Infrastructur e ( PKI ). The public cloud, such as A WS, provides an  A WS Certificate Manager ( ACM ) managed Non-web transmission of data over the network should also be encrypted,and this includes Secur e Shell ( SSH ) and Internet Pr otocol Security ( IPsec ) encryption. SSH  is most prevalent while connecting to servers, and IPsec is applicable to securing corporate traf fic transferred over a virtual private network ( VPN ). File transfer should be secured using SSH File T ransfer Pr otocol ( SFTPS ) or FTP Secur e ( FTPS ), and email server communication needs to be secured by Simple Mail T ransfer Pr otocol Secur e ( SMTPS ) or  Internet Message Access Pr otocol  ( IAMPS ). In this section, you learned about various methods to secure data at rest andin motion with dif ferent cryptographic techniques. Data backup and recovery is an important aspect of protecting your data in the case of anyunforeseen incidents. Y ou will learn more about data backup in Chapter 9 , Ar chitectural Reliability Considerations , in the Disaster r ecovery planning section. There are many governing bodies available that publish compliance, whichis a set of checklists to ensure customers' data security . Compliance also makes sure that or ganizations comply with industry and local government rules. Let's learn more about various compliance measures in the nextsection. Security and compliancecertifications There are many compliance certifications depending on your industry andgeographical location to protect customer privacy and secure data. For anysolution design, compliance requirements are among the critical criteria thatneed to be evaluated. The following are some of the most popular industry-standard compliances: Global compliance includes certifications that all or ganizations need to adhere to, regardless of their region. These include ISO 9001, ISO27001, ISO 27017, ISO 27018, SOC 1, SOC 2, SOC 3, and CSAST AR for cloud security . The US government requires various kinds of compliance to handlepublic sector workload. These include FedRAMP , DoD SRG Level-2, 4, and 5, FIPS 140, NIST SP 800, IRS 1075, IT AR, VP A T , and CJIS. Industry-level compliance of application apply to a particular industry . These include PCI DSS, CDSA, MP AA, FERP A, CMS MARS-E, NHS IG T oolkit (in the UK), HIP AA, FDA, FISC (in Japan), F ACT (in the UK), Shared Assessment, and GLBA.Regional compliance certification applies to a particular country orregion. These include EU GDPR, EU Model Clauses, UK G-Cloud,China DJCP , Singapore MTCS, Ar gentina PDP A, Australia IRAP , India MeitY , New Zealand GCIO, Japan CS Mark Gold, Spain ENS and DP A, Canada Privacy Law , and US Privacy Shield. As you can see, there are many compliance certifications available fromdif ferent regulatory bodies as per industry , region, and government policy . W e are not going into details of compliance, but you need to evaluate your application with compliance requirements before starting your solutiondesign. Compliance requirements influence the overall solution designheavily . Y ou need to decide what kind of encryption is required, as well as logging, auditing, and location of your workload based on your complianceneeds. Logging and monitoring help to ensure robust security and compliance.Logging and monitoring are essential. If an incident occurs, your teamshould be notified immediately and be ready to respond to incidents. Y ou are going to learn more about monitoring and alert methods in Chapter 10 ,  Operational Excellence Considerations . There are several compliance industries depending on your applicationgeolocation, industry , and government rules. Y ou learned about the various categories of compliance and some common compliance standardsappropriate for each group. Many or ganizations are moving to the cloud, so it's vital to understand security in the cloud. The cloud's shar ed security r esponsibility model As the cloud is becoming regularly used and many or ganizations are moving their workload to a public cloud such as A WS, Google Cloud Platform ( GCP ), and Azure, the customer needs to understand the cloud security model. Security in the cloud is a joint ef fort between the customer and the cloud provider . Customers are responsible for what they implement using cloud services and for the applications connected to the cloud. In the cloud,customer responsibility for application security needs depends upon thecloud of ferings they are using and the complexity of their system. The following diagram illustrates a cloud security model from one of thelar gest public cloud providers (A WS), and it's pretty much applicable to any public cloud provider: A W S c l o u d s h a r e d s e c u r i t y r e s p o n s i b i l i t y m o d e l As shown in the preceding diagram, A WS handles the security of the cloud , specifically the physical infrastructures that host your resources. Thisincludes the following: Data centers : Non-descript facilities, 24/7 security guards, two-factor authentication, access logging and review , video surveillance, and disk degaussing and destruction.Hardwar e infrastructur e : Servers, storage devices, and other appliances that rely on A WS services. Softwar e infrastructur e : Host operating systems, service applications, and virtualization software.Network infrastructur e : Routers, switches, load balancers, firewalls, cabling, and so on. Also includes continuous network monitoring atexternal boundaries, secure access points, and redundant infrastructure. The customer handles the security in the cloud , which includes the following: Server's operating system : The operating system installed on the server could be vulnerable to attacks. Patching and maintenance of theoperating system is the customer's responsibility , as software applications depend heavily upon it. Application : Every application and its environments, such as dev , test, and prod, are maintained by the customer . So, handling password policies and access management is the customer's responsibility . Operating system-/host-based fir ewalls : Customers need to protect their entire system from external attacks. However , the cloud provides security in that area, but customers should consider an IDS or an IPS toadd an extra layer of security . Network configuration and security gr oup : The cloud provides tools to create a network firewall, but it's up to application requirements as towhich traf fic needs to be stopped or allowed to go through. Customers are responsible for setting up firewall rules to secure their systems fromexternal and internal network traf fic. Customer data and encryption : Data handling is the customer's responsibility , as they are more aware of the data protection that's needed. The cloud provides tools to apply for data protection by using various encryption mechanisms, but it's the customer'sresponsibility to apply those tools and secure their data. The public cloud also provides various compliance certifications that applyto the hardware portions managed by them. T o make your application compliant, you need to handle and complete audits for application-levelcomplaints. As a customer , you get an additional advantage by inheriting security and compliance provided by the cloud provider . T ry to automate security best practices wherever possible. Software-based security mechanisms improve your ability to scale more rapidly , cost- ef fectively , and securely . Create and save a custom baseline image of a virtual server , and then use that image automatically on each new server you launch. Create an entire infrastructure that is defined and managed in atemplate to replicate best practices for the new environment.  The cloud provides all kinds of tools and services to secure your applicationin the cloud, along with in-built security at the IT infrastructure level.However , it's up to the customer how they want to utilize those services and make their application secure in the cloud. The overall cloud provides bettervisibility and centralized management for your IT inventory , which helps to manage and secure your system. Security is the priority for any solution, and a solution architect needs tomake sure their application is secure and protected from any attack. Securityis a continuous ef fort. Each security incident should be treated as an improvement opportunity for the application. A robust security mechanismshould have authentication and authorization controls. Every or ganization and application should automate responses to security events and protectinfrastructure at multiple levels.  Summary In this chapter , you learned about various design principles to apply security best practices for your solution design. These principles include keyconsiderations during solution design to protect your application by puttingin the appropriate access control, data protection, and monitoring. Y ou need to apply security at every layer . Starting with user authentication and authorization, you learned about applying security at the web layer , application layer , infrastructure layer , and database layer . Each layer has a dif ferent kind of attack, and you learned various methods to protect your application with the available technology choices. For user management, you learned about using FIM and SSO to handlecorporate users, and various methods for implementation of userauthentication and authorization. These choices include enterprisemanagement services such as Microsoft's AD and A WS Directory Service. Y ou also have options to handle millions of users, using OAuth 2.0. At the web layer , you learned about various attack types such as  DDoS , SQLi , and XSS . Y ou learned about how to protect those attacks, using dif ferent DDoS prevention techniques and network firewalls. Y ou learned various techniques to protect code at the application layer and ensure thesecurity of your infrastructure. Y ou dived deep into dif ferent network components and methods to build trusted boundaries to limit the attackradius. Y ou learned about data protection by putting proper data classification in place and tagged your data as confidential, private, or public data. Y ou learned about symmetric and asymmetric algorithms and how they dif fer from each other . Y ou learned about using key management to protect the public/private encryption key . Data can be in motion or sitting in storage. Y ou learned about how to protect data in both modes. In the end, you learned about various compliance and shared security responsibility modelsapplicable to a cloud workload. While this chapter is about applying security best practices, reliability isanother essential aspect of any solution design. T o make your business successful, you want to create a reliable solution that should always beavailable and able to handle workload fluctuation. In the next chapter , you will learn about the best practices to make your application reliable with theavailable technology choices. Y ou will learn various disaster recovery and data replication strategies to make your application more reliable. Ar chitectural Reliability Considerations Application reliability is one of the essential aspects of architecture design.A reliable application helps to win customer trust by making it availablewhenever the customer needs it. High availability is one ofthe  mandatory  criteria for online applications. Users want to browse your application anytime and complete tasks such as shopping and banking asper their convenience. Reliability is one of the essential recipes for anybusiness to be successful. Reliability means the ability of the system to recover from failure. It's about making your application fault-tolerant in such a way that it canrecover without impacting the customer experience. A reliable systemshould be able to recover from any infrastructure failure or server failure.Y our system should be prepared to handle any situation that could cause disruption.  In this chapter , you will learn various design principles applicable to making your solution reliable. Reliability needs to consider everycomponent of the architecture. Y ou will get an understanding of the right selection of technology to ensure your architecture's reliability at everylayer . Y ou will learn the following best practices for reliability in this chapter: Design principles for architectural reliabilityT echnology selection for architectural reliability Improving reliability with the cloud By the end of this chapter , you will have learned about various disaster recovery techniques to ensure the high availability of your application anddata replication methods for business process continuations. Design principles for ar chitectural r eliability The goal of reliability is to keep the impact of any failure to the smallestarea possible. By preparing your system for the worst, you can implement avariety of mitigation strategies for the dif ferent components of your infrastructure and applications. Before a failure occurs, you want to test your recovery procedures. It isessential to try to automate recovery as much as possible to reduce thepossibility of human error . The following are the standard design principles that help you to strengthenyour system reliability . Y ou will find that all reliability design principles are closely related and complement each other . Making systems self-healing System failure needs to be predicted in advance, and in the case of failureincidence, you should have an automated response for system recovery , which is called system self-healing. Self-healing is the ability of thesolution to automatically recover from failure. A self-sealing system detectsfailure proactively and responds to it gracefully with minimal customerimpact. Failure can happen in any layer of your entire system, whichincludes hardware failure, network failure, or software failure. Usually , data center failure is not an everyday event, and more granular monitoring isrequired for frequent failures such as  database connection and network connection  failures. The system needs to monitor the failure and act to recover . T o handle failure response, first, you need to identify Key Performance Indicators ( KPIs ) for your application and business. At the user level, these KPIs may include the number of requests served per second or pageload latency for your website. At the infrastructure level, you can definemaximum CPU utilization, such as it should not go above 60%. Memoryutilization should not go beyond 50% of the total available Random-Access Memory  ( RAM ) and so on. As you defined your KPIs, you should put the monitoring system in place totrack failures and notify you as your KPIs reach the threshold. Y ou should apply automation around monitoring so that the system can self-heal in theevent of any incidents. For example, add more servers when CPUutilization reaches near 50%—proactive monitoring helps to preventfailures. Applying automation Automation is the key to improving your application's reliability . T ry to automate everything from application deployment and configuration to theoverall infrastructure. Automation provides you with agility where yourteam can move fast and experiment more often. Y ou can replicate the entire system infrastructure and the environment with a single click to try a newfeature. Y ou can plan the Auto Scaling of your application based on a schedule, for example, an e-commerce website has more user traf fic on weekends. Y ou can also automate scaling based on the user request volume to handle theunpredictable workload. Use automation to launch independent and parallelsimulation jobs that will provide greater accuracy when combined with theresults from the first simulation job. Repeating the same steps to configureeach environment manually can be err or pr one . There is always a chance of human error , such as a typo, for example, in a database name. Frequently , you need to apply the same configuration that you have on your development environment to Quality Assurance ( QA ) environments. There may be multiple QA environments for each stage of testing, whichincludes functional testing, UA T , and stress testing environments. Often, a QA tester discovers a defect caused by wrongly configured resources,which could introduce a further delay in the test schedule. Mostimportantly , you cannot af ford to have a configuration error in production servers. T o reproduce precisely the same configuration, you may need to document a step-by-step configuration instruction. The solution to this challenge is toautomate these steps by creating a script . The automation script itself can be the documentation. As long as the script is correct, it is more reliable than manualconfiguration. It is certainly reproducible. Detecting unhealthy resources and launching replacement resources can be automated, and you can notifythe IT operation team when resources are changed. Automation is a keydesign principle that needs to apply everywhere in your system.Cr eating a distributed system Monolithic applications have low reliability when it comes to systemuptime, as one small issue in a particular module can bring down the entiresystem. Dividing your application into multiple small services reduces theimpact ar ea , so that issue is one part of the application shouldn't  impact the whole system, and the application can continue to serve criticalfunctionality . For example, in an e-commerce website, an issue with the payment service should not af fect customer's ability to place orders, as payment can be processed later . At the service level, scale your application horizontally to increase systemavailability . Design a system so that it can use multiple smaller components working together rather than a single monolithic system to reduce theimpact area. In a distributed design, requests are handled by dif ferent components of the system, and the failure of one component doesn't impactthe functioning of other parts of the system. For example, on an e-commerce website, the failure of warehouse management components willnot impact the customer placing the order . However , the communication mechanism can be complicated in a distributed system. Y ou need to take care of system dependencies by utilizing the circuit breaker pattern. As you learned about the circuit breakerpattern in Chapter 6 , Solution Ar chitectur e Design Patterns , the basic idea behind the circuit breaker is simple. Y ou wrap a protected function call in a circuit breaker object, which monitors for failures. Monitoring capacity Resource saturation is the most common reason for application failure.Often, you will encounter the issue where your applications start rejectingrequests due to CPU, memory , or hard disk overload. Adding more resources is not always a straightforward task as you should have additionalcapacity available when needed. In a traditional on-premises environment, you need to calculate servercapacity based on the assumption  in advance. W orkload capacity prediction becomes more challenging for a business such as a shopping website andany online business. Online traf fic is very unpredictable and fluctuates heavily driven by global trends. Usually , procuring hardware can take anywhere between 3 to 6 months , and it's tough to guess capacity in advance. Ordering excess hardware will incur an extra cost as a resource issitting idle, and a lack of resources will cause the loss of business due toapplication unreliability . Y ou need an environment where you don't need to guess capacity , and your application can scale on-demand. A public cloud provider such as A WS provides Infrastructur e-as-a-Service ( IaaS ), which facilitates the on- demand availability of resources. In the cloud, you can monitor systemsupply and demand. Y ou can automate the addition or removal of resources as needed. It allows you to maintain the level of resources that will satisfydemand without over -provisioning or under -provisioning.  Performing r ecovery validation When it comes to infrastructure validation, most of the time, or ganizations focus on validating a happy path where everything is working. Instead, youshould validate how your system fails and how well your recoveryprocedures work. V alidate your application, assuming everything fails all the time. Don't just expect that your recovery and failover strategies willwork. Make sure to test them regularly , so you're not surprised if something does go wrong. A simulation-based validation helps you to uncover any potential risks. Y ou can automate a possible scenario that could cause your system failure andprepare an incident response accordingly . Y our validation should improve application reliability in such a way that nothing will fail in production. Recoverability is sometimes overlooked as a component of availability . T o improve the system's  Recovery Point Objective  ( RPO ) and Recovery T ime Objective ( R T O ), you should back up data and applications along with their configuration as a machine image. Y ou will learn more about R T O and RPO in the next section. In the event that a natural disaster makes one or more of your components unavailable or destroys your primary datasource, you should be able to restore the service quickly and without lostdata. Let's talk more about specific disaster recovery strategies to improveapplication reliability and associated technology choices. T echnology selection for ar chitectur e r eliability Application reliability often looks at the availability of the application toserve users. Several factors go into making your application highlyavailable. Fault tolerance refers to the built-in redundancy of anapplication's components. Scalability is how your application'sinfrastructure can respond to increased capacity needs to ensure yourapplication is available and performing within your required standards. T o make your application reliable, you should able to restore service quickly and without lost data. Going forward, we are going to address therecovery process as disaster recovery . Before going into various disaster recovery scenarios, let's learn more about the  Recovery T ime Objective ( R T O )/ Recovery Point Objective  ( RPO ) and data replication. Planning the R T O and RPO Any application needs to define service availability in an aspect ofa  Service-Level Agr eement ( SLA ). Or ganizations define SLAs to ensure application availability and reliability for their users. Y ou may want to define an SLA, saying my application should be 99.9% available in a given year or the or ganization can tolerate it if the application is down for 43 minutes per month, and so on. The RPO and R T O for an application is mostly driven by the defined SLA. The RPO is the amount of data loss an or ganization can tolerate in the aspect of time. For example, my application is fine if it loses 15 minutes'worth of data. The RPO helps to define a data backup strategy . The R T O is about application downtime and how much time my application should taketo recover and function normally after failure incidence. The followingdiagram illustrates the dif ference between the R T O and RPO:  R T O a n d R P O In the preceding diagram, suppose the failure occurs at 10 A.M. and youtook the last backup at 9 A.M.; in the event of a system crash, you wouldlose 1 hour of data. When you restore your system, there is an hour's worthof data loss, as you were taking data backups every hour . In this case, your system RPO is 1 hour , as it can tolerate living with 1 hour's worth of data loss. In this case, the RPO indicates that the maximum data loss that can betolerated is 1. If your system takes 30 minutes to restore to the backup and bring up thesystem, then it defines your R T O as half an hour . It means the maximum downtime that can be tolerated is 30 minutes. The R T O is the time it takes to restore the entire system after a failure that causes downtime. An or ganization typically decides on an acceptable RPO and R T O based on the user experience and financial impact on the business in the event ofsystem unavailability . Or ganizations consider various factors when determining the R T O/RPO, which includes the loss of business revenue and damage to their reputation due to downtime. IT or ganizations plan solutions to provide ef fective system recovery as per the defined R T O and RPO. Replicating data Data replication and snapshots are the key to disaster recovery and makingyour system reliable. Replication creates a copy of the primary data site onthe secondary site, and in the event of primary system failure, the systemcan fail over to the secondary system and keep working reliably . This data could be your file data stored in a  NAS drive , database snapshot , or machine image snapshot . Sites could be two geo-separated on-premises systems, two separate devices on the same premises, or a physicallyseparated public cloud. Data replication is not only helpful for disaster recovery , but it can speed up an or ganization's agility by quickly creating a new environment for testing and development. Data replication can be synchronous or asynchronous. Synchr onous versus asynchr onous r eplication Synchronous replication creates a data copy in real time. Real-time datareplication helps to reduce the RPO and increase reliability in the event of adisaster . However , it is expensive as it requires additional resources in the primary system for continuous data replication.  Asynchronous replication creates copies of data with some lag or as per thedefined schedule. However , asynchronous replication is less expensive as it uses fewer resources compared to synchronous replication. Y ou may choose asynchronous replication if your system can work with a longer RPO. In terms of database technology such as Amazon RDS, synchronousreplication is applied if we create RDS with multiple availability zonefailover . For read replicas, there is asynchronous replication, and you can use that to serve report and read requests. As illustrated in the following architecture diagram, in synchronousreplication, there is no lag of data replication between the master andstandby instance of the database while, in the case of asynchronousreplication, there could be some lag while replicating the data between themaster and reapplication instance: S y n c h r o n o u s a n d a s y n c h r o n o u s d a t a r e p l i c a t i o n Let's explore some methods of data reapplication for the synchronous andasynchronous approaches. Replication methods The replication method is an approach to extract data from the sourcesystem and create a copy for data recovery purposes. There are dif ferent replication methods available to store a copy of data as per the storage typefor business process continuation. Replications can be implemented usingthe following methods: Array-based r eplication : In this, built-in software automatically replicates data. However , both the source and destination storage arrays should be compatible and homogenous to replicate data. Astorage array contains multiple storage disks in a rack. Lar ge enterprises use array-based replication due to the ease of deployment and the reduction in the compute power host system.Y ou can choose array-based replication products such as HP Storage, EMC SAN Copy , and NetApp SnapMirror . Network-based r eplication : This can copy data between a dif ferent kind of heterogeneous storage array . It uses an additional switch or appliance between incompatible storage arrays to replicate data. In network-based replication, the cost of replication could be higheras multiple players come into the picture. Y ou can choose from networked-based replication products such as NetApp Replication Xand EMC RecoverPoint. Host-based r eplication : In this, you install a software agent on your host that can replicate data to any storage system such as NAS, SAN,or DAS. Y ou can use a host-based software vendor , for example, Symantec, Commvault, CA, or V ision Solution. It's highly popular in Small and Medium-Sized Businesses ( SMBs ) due to fewer upfront costs and heterogeneous device compatibility . However , it consumes more compute power as the agent needs to install on host operating system. Hypervisor -based r eplication : This is VM-aware, which means it can copy the entire virtual machine from one host to another . As or ganizations mostly use a virtual machine, it provides a very ef ficient disaster recovery approach to reduce the R T O. Hypervisor -based replication is highly scalable and consumes fewer resources than host-based replication. It can be carried out by nativesystems built into VMware and Microsoft W indows. Y ou can choose a product such as Zerto to perform hypervisor -based replication or another product from one of the various vendors. Previously , in Chapter 3 , Attributes of the Solution Ar chitectur e , you learned about scalability and fault tolerance. In Chapter 6 , Solution Ar chitectur e Design Patterns , you learned about various design patterns to make your architecture highly available. Now , you will discover multiple ways to recover your system from failure and make it highly reliable. Planning disaster r ecovery Disaster Recovery ( DR ) is about maintaining business continuation in the event of system failure. It's about preparing the or ganization for any possible system failure and the ability to recover from it. DR planningincludes multiple dimensions, which include hardware or software failure.While planning for disaster recovery , always ensure you consider other operational failures, which include a power outage, network outage, heatingand cooling system failure, physical security breach, and dif ferent incidents such as fire, flood, or human error . Or ganizations invest ef fort and money in disaster recovery planning as per system criticality and impact. A revenue-generating application needs to beup all of the time as it has a significant impact on company image andprofitability . Such an or ganization invests lots of ef fort in creating their infrastructure and training their employees for a disaster recovery situation.Disaster recovery is like an insurance policy that you have to invest andmaintain even when you are not going to utilize it. Similarly , during normal operations, the infrastructure typically is under -utilized and over - provisioned. Applications can be placed on a spectrum of complexity . There are four DR scenarios, sorted from highest to lowest R T O/RPO as follows: Backup and restorePilot lightW arm standby Multi-site active-active As shown in the following diagram, in DR planning, as you progress witheach option, your R T O and RPO will reduce while the cost of implementation increases. Y ou need to make the right trade-of f between R T O/RPO requirements and cost as per your application reliability requirement: D i s a s t e r r e c o v e r y o p t i o n s s p e c t r u m Let's explore each of the aforementioned options in detail with technologychoices. Now , the public cloud, such as A WS, enables you to operate each of the preceding DR strategies cost-ef fectively and ef ficiently . Business continuity is about ensuring critical business functions continue tooperate or function quickly in the event of disasters. As or ganizations are opting to use the cloud for disaster recovery plans, let's learn about variousdisaster recovery strategies between an on-premise environment and theA WS cloud. Backup and r estor e Backup is the lowest cost option but with higher RPO and R T O. This method is simple to get started and extremely cost-ef fective as you need backup storage. This backup storage could be a tape drive, hard disk drive,or network access drive. As your storage needs increase, adding andmaintaining more hardware across regions could be a daunting task. One ofthe simple and cost-ef fective options is to use the cloud as backup storage. Amazon S3 provides unlimited storage capacity at a low cost and with apay-as-you-go model. The following diagram shows a basic disaster recovery system. In thisdiagram, the data is in a traditional data center , with backups stored in A WS. A WS Import/Export or Snowball is used to get the data into A WS, and the information is later stored in Amazon S3: D a t a b a c k u p t o A m a z o n S 3 f r o m o n - p r e m i s e s i n f r a s t r u c t u r e Y ou can use other third-party solutions available for backup and recovery . Some of the most popular choices are NetApp, VMware, T ivoli, Commvault, and CloudEndure. Y ou need to take backups of the current system and store them in Amazon S3 using a backup software solution.Make sure to list the procedure to restore the system from a backup on thecloud, which includes the following: 1 . Understand which Amazon Machine Image  ( AMI ) to use and build your own as required with pre-installed software and security patches. 2 . Document the steps to restore your system from a backup. 3 . Document the steps to route traf fic from the primary site to the new site in the cloud. 4 . Create a run book for deployment configuration and possible issueswith their resolutions. If the primary site located on-premises goes down, you need to start therecovery process. As shown in the following diagram, in the preparationphase, create a custom Amazon Machine Image ( AMI ), which is pre- configured with the operating system along with the required software, andstore it as a backup in Amazon S3. Store any other data such as databasesnapshots, storage volume snapshots, and files in Amazon S3: R e s t o r i n g s y s t e m s f r o m A m a z o n S 3 b a c k u p s i n t h e c l o u d If the primary site goes down, you can retrieve backups from Amazon S3and bring up the required infrastructure. Y ou can spin up Amazon EC2 server instances using golden machine image and put it behind loadbalancer with Auto Scaling configuration as needed. It would be a betterapproach to automate your infrastructure such as networking deploymentand bring it up by deploying the A WS CloudFormation template . Once your servers are up and running, you need to restore data from backup. Thelast task is to switch over traf fic to the new system by adjusting DNS records to point to A WS. This disaster recovery pattern is easy to set up and rather inexpensive . However , in this scenario, both RPO and R T O will be high, R T O will be downtime until system gets restored from backup and start functioning,while RPO will have an amount of data loss which depends upon thebackup frequency . Let's explore the next approach, pilot light, which provides improvements in your R T Os and RPOs. Pilot light The pilot light is the next lowest cost DR method after backup and restore.As the name suggests, you need to keep the minimum amount of coreservices up and running in a dif ferent region. Y ou can spin up additional resources quickly in the event of a disaster . Y ou would probably actively replicate the database tier , then spin up instances from a VM image or build out infrastructure using infra as code such as CloudFormation . Just like the pilot in your gas heater , a tiny flame that is always on can quickly light up the entire furnace to heat the house. The following diagram shows a pilot light disaster r ecovery pattern . In this case, the database is replicated into A WS, with Amazon EC2 instances of the web servers and application servers ready to go, but currently not running: T h e p i l o t l i g h t d a t a r e p l i c a t i o n t o D R s i t e s c e n a r i o A pilot light scenario is pretty much similar to back up and r estor e , where you take the backup of most of the components and store them passively . However , you maintain active instances with a lower capacity for critical components such as a database or authentication server , which can take a significant time to come up. Y ou need to be prepared to start all required resources automatically , which include network settings, load balancers, and virtual machine images as required. As core pieces are already running,recovery time is faster than the backup and restore method. The pilot light is very cost-ef fective as you are not running all of the resources 24/7. Y ou need to enable the replication of all critical data to the disaster recovery site—in this case, the A WS cloud. Y ou can use the A WS Data Migration Service to replicate data between on-premise and cloud databases. For file-based data, you can use Amazon File Gateway . There are many third-party managed tools available that provide data reapplicationsolutions  ef ficiently , such as Attunity , Quest, Syncsort, Alooma, and JumpMind. Now , if the primary system fails, as shown in the following diagram, you start up the Amazon EC2 instances with the latest copy of the data. Then,you redirect Amazon Route 53 to point to the new web server: R e c o v e r y i n t h e p i l o t l i g h t m e t h o d For the pilot light method, in the case of a disaster environment, you need tobring up the resources around the replicated core dataset automatically andscale the system as required to handle the current traf fic. Y ou can choose to vertically scale up the database instance and scale out application servers with horizontal scaling using a load balancer . Finally , update the DNS record in your router to point to the new site.  A pilot light disaster recovery pattern is relatively easy to set up andinexpensive. However , in this scenario, the R T O takes a bit longer to automatically bring up a replacement system, while the RPO lar gely depends on the replication type. Let's explore the next approach,  warm standby , which provides further improvements in your R T Os and RPOs. W arm standby W arm standby , also known as fully working low capacity standby , is like the next level of the pilot light. It is the option where you use the agility ofthe cloud to provide low-cost DR. It increases the cost but also allows datato recover more quickly by having a small subset of services alreadyrunning. Y ou can decide whether your disaster recovery environment should be enough to accommodate 30% or 50% of production traf fic. Alternatively , you can also use this for non-production testing. As shown in the following diagram, in the warm standby method, twosystems are running—the central system and a low-capacity system—on acloud such as A WS. Y ou can use a router such as Amazon Route 53 to distribute requests between the central system and the cloud system: W a r m s t a n d b y s c e n a r i o r u n n i n g a n a c t i v e - a c t i v e w o r k l o a d w i t h a l o w c a p a c i t y When it comes to a database, warm standby has a similar approach to pilotlight, where data is continuously replicating from the main site to thedisaster recovery site. However , in warm standby , you are running all necessary components 24/7; however , they do not scale up for production traf fic. Often, the or ganization chooses a warm standby strategy for more critical workloads, so you need to make sure there are no issues in the disasterrecovery site with continuous testing. The best approach is to take the A/Btesting approach, where major traf fic will be handled by the main site but a small amount of traf fic, approximately 1% to 5%, is routed to the disaster recovery site. It will make the disaster recovery site able to serve traf fic when the primary site is down. Also, make sure to patch and updatesoftware regularly in the disaster recovery site. As shown in the following diagram, during the unavailability of the primaryenvironment, your router switches over to the secondary system, which isdesigned to automatically scale its capacity up in the event of a failoverfrom the primary system: R e c o v e r y p h a s e i n t h e w a r m s t a n d b y s c e n a r i o In the event of a failure in the primary site, you can take a step approach,where an immediate transfer of the critical production workload to thedisaster recovery site, and other non-critical workloads can be transferredlater as you scale up your environment. For example, in an e-commercebusiness, you first need to bring up your website, which is customer -facing, and later you can bring up other systems such as warehouse managementand shipping, which is working in the background to fulfill the order . Y our disaster recovery process becomes more ef ficient if your application is an all-in cloud, where entire infrastructures and applications are hosted inthe public cloud, such as A WS. The A WS cloud allows you to use cloud- native tools ef ficiently; for example, you can enable a multi-AZ failover feature in the Amazon RDS database to create a standby instance in anotheravailability zone with continuous replication. In the case of the primarydatabase, an instance goes down, an in-built automatic failover takes care ofswitching the application to the standby database without any application configuration changes. Similarly , you can use automatic backup and replication options for all kinds of data protection. A warm standby disaster recovery pattern is relatively complex to set upand expensive. The R T O is much quicker than pilot light for critical workload, however for non-critical workload it's depends upon how quicklyyou can scale up system, while the RPO lar gely depends upon the replication type. Let's explore the next approach, multi-site, which providesnear -zero R T Os and RPOs.  Multi-site Lastly , the multi-site strategy , also known as hot standby , helps you to achieve near -zero R T O and RPO. In multi-site, your disaster recovery site is an exact replica of the primary site with continuous data replication andtraf fic flow . It is known as multi-site architecture due to the automated load balancing of traf fic across regions or between on premise and the cloud.  As shown in the following diagram, multi-site is the next level of disasterrecovery to have a fully functional system running in the cloud at the sametime as on-premises systems: M u l t i - s i t e s c e n a r i o r u n n i n g a n a c t i v e - a c t i v e w o r k l o a d w i t h a f u l l c a p a c i t y The advantage of the multi-site approach is that it is ready to take a fullproduction load at any moment. It's similar to warm standby but runningfull capacity in the disaster recovery site. If the primary site goes down, alltraf fic can be immediately failed over to the disaster recovery site. A multi-site disaster recovery pattern is most expensive as it requires aduplicate of everything. However , in this scenario, the R T O objective is much quicker for all workloads, while the RPO objective lar gely depends upon the replication type. Let's explore some best practices around disasterrecovery to make sure your system is running reliably . Applying best practices for disasterr ecovery As you start thinking about disaster recovery , here are some important considerations: Start small and build as needed : Make sure to streamline the first step of taking a backup. Most of the time, or ganizations lose data as they didn't have an ef ficient backup strategy . T ake a backup of everything, whether it is your file server , machine image, or databases.  Keeping lots of active backups could increase costs, so make sure toapply a lifecycle policy to archive and delete data as per businessneeds. For example, you can choose to keep a 90-day active backupand after that store that in low-cost archive storage such as a tapedrive or Amazon Glacier . After 1 or 2 years, you may want to set a lifecycle policy to delete the data. Compliance such as PCI-DSSmay require users to store data for 7 years, and in that case, youmust choose archival data storage to reduce costs. Check your softwar e licenses : Managing software licenses can be a daunting task, especially in the current microservice architectureenvironment, where you have several services running independentlyon their instances of virtual machines and databases. Software licensescould be associated with several installations, a number of CPUs, andseveral users. It becomes tricky when you go for scaling. Y ou need to have enough licenses to support your scaling needs.  Horizontal scaling needs to add more instances with softwareinstalled, and in vertical scaling, you need to add more CPU ormemory . Y ou need to understand your software licensing agreement and make sure you have the appropriate license to fulfill systemscaling. Also, you don't have to buy an excessive license, which you may not be able to utilize and costs more money . Overall, make sure to manage your license inventory like your infrastructure orsoftware. T est your solutions often : Disaster recovery sites are created for rare disaster recovery events and are often overlooked. Y ou need to make sure your disaster recovery solution is working as expected in case ofan event to achieve higher reliability . Compromising a defined SLA can violate contractual obligations and result in the loss of money andcustomer trust.  One way to test your solution often is by playing gameday . T o play gameday , you can choose a day when the production workload is smaller and gather all of the team responsible for maintaining theproduction environment. Y ou can simulate a disaster event by bringing down a portion of the production environment and let theteam handle the situation to keep the environment up and running.These events make sure you have working backups, snapshots, andmachine images to handle disaster events. Always put a monitoring system in place to make sure automated failoverto the disaster recovery site takes place if an event occurs. Monitoring helpsyou to take a proactive approach and improves system reliability byapplying automation.  Monitoring capacity saves you resource saturation issues, which can impact your application's reliability . Creating a disaster recovery plan and performing regular recovery validation helps to achievethe desired application reliability . Impr oving r eliability with the cloud In previous sections, you have seen examples of a cloud workload for thedisaster recovery site. Many or ganizations have started to choose the cloud for disaster recovery sites to improve application reliability , as the cloud provides various building blocks. Also, cloud providers such as A WS have a marketplace where you can purchase a variety of ready-to-use solutionsfrom providers. The cloud provides data centers that are available across the geographiclocation at your fingertips. Y ou can choose to create a reliability site on another continent without any hassle. W ith the cloud, you can easily create and track the availability of your infrastructures such as backups andmachine images. In the cloud, easy monitoring and tracking help to make sure yourapplication is highly available as per the  SLA . The cloud enables you to have fine control over IT resources, cost, and handling trade-of fs for RPO/R T O requirements. Data recovery is critical for application reliability . Data resources and locations must align with R T Os and RPOs. The cloud provides easy and ef fective testing of your disaster recovery plan. Y ou inherit features available in the cloud, such as the logs and metrics mechanisms for various cloud services. Built-in metrics are a powerful toolfor gaining insight into the health of your system. W ith all available monitoring capabilities, you can notify the team in case of any thresholdbreach or trigger automation for system self-healing. For example, A WS provides CloudW atch, which can collect logs and generate metrics while monitoring dif ferent applications and infrastructure components. It can trigger various automation to scale your application. The cloud provides a built-in change management mechanism that helps totrack provisioned resources. Cloud providers extend out-of-the-boxcapabilities to ensure applications and operating environments are running known software and can be patched or replaced in a controlled manner . For example, A WS provides A WS System Manager , which has the capability of patching and updating cloud servers in bulk. The cloud has tools to back updata, applications, and operating environments to meet requirements forR T Os and RPOs. Customers can leverage cloud support or a cloud partner for their workload handling needs. W ith the cloud, you can design a scalable system, which can provide flexibility to add and remove resources automatically to match the currentdemand. Data is one of the essential aspects of any application's reliability . The cloud provides out-of-the-box data backup and replication tools,including machine images, databases, and files. In the event of a disaster , all of your data is backed up and appropriately saved in the cloud, which helpsthe system to recover quickly . Regular interaction across the application development and operation teamwill help to address and prevent known issues and design gaps, which willreduce the risk of failures and outages. Always architect your applicationsto achieve resiliency and distribute them to handle any outages. Distributionshould span dif ferent physical locations to achieve high levels of availability . Summary In this chapter , you learned about various principles to make your system reliable. These principles include making your system self-healing byapplying rules of automation and to reduce the impact in the event of failureby designing a distributed system where the workload spans multipleresources.  Overall system reliability heavily depends on your system's availability andits ability to recover from disaster events. Y ou learned about synchronous and asynchronous data replication types and how they af fect your system reliability .  Y ou learned about various data replication methods, including array-based, network-based, host-based, and hypervisor -based. Each replication method has its pros and cons. There are multiple vendors'products available to achieve the desired data replication. Y ou learned about various disaster planning methods as per the or ganization's needs and the R T O and RPO. Y ou learned about backup & restore method that has high R T O and RPO, and it is easy to implement. Pilot light improves your R T O/RPO by keeping critical resources such as a database active in the disaster recovery site. W arm standby and multi-site maintain an active copy of a workload on a disaster recovery site and helpto achieve a better R T O/RPO. As you increase application reliability by lowering the system's R T O/R T O, the system's complexity and cost increase. Y ou learned about utilizing the cloud's built-in capability to endure application reliability . Solution design and launch may not happen too often, but operationalmaintenance is an everyday task. In the next chapter , you will learn about the alert and monitoring aspects of solution architecture. Y ou will learn about various design principles and technology choices to make yourapplication operationally ef ficient and apply operational excellence. Operational ExcellenceConsiderations Application maintainability is one of the main aspects that a solutionarchitect should consider during architectural design. Every new projectstarts with lots of planning and resources at the beginning. Y ou may spend a few months creating and launching your application but change frequencywill get reduced as your application matures. After the production launch,the application needs several things to be taken care of to keep operating.Y ou need to continually monitor your application to find and resolve any issues on a day-to-day basis. The operations team needs to handle application infrastructure, security , and any software issues to make sure your application is running reliablywithout any problems or issues. Often, an enterprise application is complexin nature, with a defined Service Level Agr eement ( SLA ) regarding application availability . Y our operations team needs to understand business requirements and prepare themself accordingly to respond to any event. Operation excellence should be implemented across various componentsand layers of architecture. In modern microservice applications, there are somany moving parts involved that this makes system operations andmaintenance a complicated task. Y our operations team needs to put proper monitoring and alert mechanisms in place to tackle any issues. Operationalissues involve coordination from several teams for pr eparation and r esolution . Operation expenditures are one of the significant costs that an or ganization puts aside to run a business. In this chapter , you will learn various design principles applicable to achieve operational excellence for your solution. The operational aspectneeds to consider every component of the architecture. Y ou will get an understanding of the right selection of technologies to ensure operational maintainability at every layer . Y ou will learn the following best practices of operational excellence in this chapter: Designing principles for operational excellenceSelecting technologies for operational excellenceAchieving operational excellence in the public cloud By the end of this chapter , you will learn about various processes and methods to achieve operational excellence. Y ou will learn about best practices that you can apply throughout application design, implementation,and post-production to improve application operability . Designing principles foroperational excellence Operational excellence is about running your application with the minimalpossible interruption to gain maximum business value. It is about applyingcontinuous improvements to make the system ef ficient. The following sections talk about the standard design principles that canhelp you strengthen your system's maintainability . Y ou will find that all operational excellence design principles are closely related and complementeach other . Automating the operation T echnology has been  moving fast in recent times, and so IT operations need to keep up with that, where hardware and software inventories areprocured from multiple vendors. Enterprises are building a hybrid cloud, soyou need to handle both on-premises and cloud operations. All modernsystems have a decidedly more extensive user base, with variousmicroservices working together and millions of devices connected in thenetwork. There are many moving parts in an IT operation, so this makes itdif ficult to run things manually . Or ganizations maintain agility , and operations have to be faster to avail the required infrastructure for new service development and deployment. Theoperations team has a more significant responsibility to keep services upand to run and recover quickly in case of an event. Now , it is required to take a proactive approach in IT operations, rather than waiting for anincident to happen and then reacting. Y our operations team can work very ef ficiently by applying automation. Manual jobs need to be automated so that the team can focus on morestrategic initiatives rather than getting overworked with tactical work.Spinning up a new server or starting and stopping services should beautomated by taking an infrastructur e as code ( IaC ) approach. Automating active discovery and response for any security threat is mostimportant, to free up the operations team. Automation allows the team todevote more time to innovation. For your web-facing application, you can detect anomalies in advancebefore they impact your system, using machine learning prediction. Y ou can raise an automated security ticket if someone exposes your server to theworld with HTTP port 80. Y ou can pretty much automate the entire infrastructure and redeploy it multiple times  as a one-click solution . Automation also helps to avoid human error , which can occur even if a person is doing the same job repetitively . Automation is now a must-have friend for IT operations.Making incr emental and r eversible changes Operational optimization is an ongoing process, whereby continuous ef fort is required to identify the gap and improve upon it. Achieving operationalexcellence is a journey . There are always changes required in all parts of your workload to maintain it—for example, often, operating systems ofyour server need to be updated with the security patch provided by yourvendor . V arious software that your application is using needs a version upgrade. Y ou need to make changes in the system to adhere to new compliance. Y ou should design your workload in such a way that it allows all system components to get updated regularly , so the system will benefit from the latest and most significant updates available. Automate your flow so thatyou can apply small changes to avoid any significant impact. Any changesshould be reversible, to restore system working conditions in the case ofany issue. Incremental changes help to do thorough testing and improveoverall system reliability . Automate any change management to avoid human error and achieve ef ficiency . Pr edicting failur es and r esponding Preventing failures is vital to achieving operational excellence. Failures arebound to happen, and it's more critical to identify them as far in advance aspossible. During architecture design,  anticipate failure to  make sure you design for failure so that nothing will fail. Assume that everything will failall the time and have a backup plan ready . Perform regular pre-mortem exercises to identify any potential source of failure. T ry to remove or mitigate any resource that could cause a failure during system operation. Create a test scenario based on your SLA that potentially includes a systemRecovery T ime Objective  ( R T O ) and Recovery Point Objective ( RPO ). T est your scenario, and make sure you understand their impact. Make your team ready to respond to any incident by simulating in a production-likescenario. T est your response procedure to make sure it is resolving issues ef fectively and create a confident team that is familiar with response execution. Learning fr om the mistake and r efining As operational failures occur in your system, you should learn from themistake and identify the gap. Make sure those same events do not occuragain, and you should have a solution ready in case a failure gets repeated.One way to improve is by running r oot cause analysis , also called RCA . During RCA, you need to gather the team and ask five whys . W ith each why , you peel of f one layer of the problem, and, after asking subsequent why , you get to the bottom of the issue. After identifying the actual cause, you can prepare a solution by removing or mitigating the resources andupdate the operational runbook with the ready-to-use solution. As your workload evolves with time, you need to make sure the operationprocedure gets updated accordingly . Make sure to validate and test all methods regularly , and that the team is familiar with the latest updates in order to execute them.  Keeping operation's runbookupdated Often, a team overlooks documentation, which results in an outdatedrunbook. A runbook provides a guide to executing a set of actions in orderto resolve issues arising due to external or internal events. A lack ofdocumentation can make your operation people-dependent, which can berisky due to team attrition. Always establish processes to keep your systemoperation people-independent, and document all the aspects. Y our runbook should include the defined SLA in the aspect of R T O/RPO, latency and performance, and so on. The system admin should maintain arunbook with steps to start, stop, patch, and update the system. Theoperations team should include the system testing and validation result,along with the procedure to respond to the event. In the runbook, you want to keep track of all previous events and actionstaken by team members to resolve them, so that any new team members canprovide a quick resolution of similar incidents during operation support.Automate your runbook through the script so that it can get updatedautomatically as new changes roll out to the system. Automate processes to annotate documents as a team applies changes to thesystem, and also after every build. Y ou can use annotation to automate your operation, and it is easily readable by code. Business priorities andcustomer needs continue to change, and it's essential to design operations tosupport evolution over time. Selecting technologies foroperational excellence The operations team needs to create procedures and steps to handle anyoperational incidents and validate the ef fectiveness of their actions. They need to understand the business need to provide ef ficient support. The operations team needs to collect systems and business metrics to measurethe achievement of business outcomes. The operational procedure can be categorized into three phases—planning,functioning, and improving. Let's explore technologies that can help in eachphase. Planning for operational excellence The first step in the operational excellence process is to define operationalpriorities to focus on the high business impact area. Those areas could beapplying automation, streamlining monitoring, developing team skills asworkload evolves, and focusing on improving overall workloadperformance. These are tools and services available that crawl through yoursystem by scanning logs and system activity . These tools provide a core set of checks that recommend optimizations for the system environment andhelp to shape priorities. After understating priorities, you need to design the operation, whichincludes the understanding of workloads to design and building theprocedures to support them. The design of a workload should consist ofhow it will be implemented, deployed, updated, and operated. An entireworkload can be viewed as various application components, infrastructurecomponents, security , data governance, and operations automation. While designing for an operation, consider the following best practice: Automate your runbook with scripting to reduce human error , which creates an operating workload.Use resource identification mechanisms to execute operations based ondefined criteria such as environment, various versions, applicationowner , and roles. Make incident responses automated so that, in the case of an event, thesystem should start self-healing without much human intervention. Use various tools and capabilities to automate the management ofserver instances and overall systems.Create script procedures on your instances to automate the installationof required software and security patches when the server gets started.These scripts are also known as bootstrap scripts. After the operation design, create a checklist for operational readiness.These checklists should be comprehensive to make sure the system is readyfor operation support when going live in production. This includes loggingand monitoring, a communication plan, an alert mechanism, a team skillset,a team support charter , a vendor support mechanism, and so on. For operational excellence planning, the following are the areas where you needappropriate tools for preparation: IT Asset ManagementConfiguration management Let's explore each area in more detail, to understand the available tools andprocesses. IT Asset Management (IT AM) Operational excellence planning requires the list of IT inventories andtracks their use. These inventories include infrastructure hardware such asphysical servers, network devices, storage, end-user devices, and so on. Y ou also need to keep track of software licenses, operational data, legalcontracts, compliance, and so on. IT assets include any system, hardware,or information that a company is using to perform a business activity . Keeping track of IT assets helps an or ganization to make strategic and tactical decisions regarding operational support and planning. However , managing IT assets in a lar ge or ganization could be a very daunting task. V arious IT AM tools are available for the operations team to help in the asset management process. Some of the most popular IT AM tools are SolarW inds , Fr eshservice , ServiceDesk Plus , Asset Panda , PagerDuty ,  Jira Service Desk , and so on. IT management is more than tracking IT assets. It also involves monitoringand collecting asset data continuously , to optimize usages and operation costs. IT AM makes the or ganization more agile by the provided end-to-end visibility and the ability to apply patches and upgrades quickly . The following diagram illustrates IT asset life cycle management ( IT ALM ): I T A L M p r o c e s s As shown in the preceding diagram, the IT ALM process includes the following phases: Plan : An asset life cycle starts with planning, which is a more strategic focus to determine needs for overall IT assets and procurementmethods. It includes cost-benefit analysis and total cost of ownership. Pr ocur e : In the procurement phase, or ganizations acquire the asset based on the outcome of planning. They may also decide to developsome holdings as required—for example, in-house software forlogging and monitoring.Integrate : In this phase, an asset is installed in the IT ecosystem. It includes operation and support of the asset, and defines user access—for example, installing a log agent to collect logs from all the serversin a centralized dashboard, and restricting monitoring dashboardmetrics to the IT operations team. Maintain : In the maintenance phase, the IT operations team keeps track of assets, and acts to upgrade or migrate based on the asset lifecycle—for example, applying a security patch provided by thesoftware vendor . The other example is keeping track of end of life for licensed software, such as a plan to migrate from W indows 2008 to W indows 2016, as the old operating system is getting to the end of its life.Retir e : In the retirement phase, the operations team disposes of the end-of-life asset. For example, if an old database server is getting tothe end of its life, then the team takes action to upgrade it and migratesthe required user and support to the new server . IT AM helps or ganizations adhere to ISO 19770 compliance requirements. It includes software procurement, deployment, upgrade, and support. IT AM provides better data security and helps to improve software compliance. Itprovides better communication between business units such as operation,finance, and marketing teams, and frontline staf f. Configuration management Configuration management maintains configuration items ( CI ) to manage and deliver an IT service. CIs are tracked in the  Configuration Management Database  ( CMDB ). The CMDB stores and manages system component records with their attributes such as their type, owner , version, and dependency with other components. The CMDB  keeps track of whether the server is physical or virtual, if the operating system is installed with itsversion (W indows 2012 or Red Hat Enterprise Linux ( RHEL ) 7.0),the  owner of the server (support, marketing, or HR), and whether it has dependency on other servers such as order management, and so on. Configuration management is dif ferent from asset management. As you learned in the previous section, asset management is much faster . It manages the entire life cycle of an asset, from planning to retirement, while CMDB isa component of asset management that stores configuration records of anindividual asset. As shown in the following diagram, configurationmanagement implements the integration and maintenance part of assetmanagement: I T a s s e t l i f e c y c l e v e r s u s c o n f i g u r a t i o n m a n a g e m e n t   Zooming in on configuration management, as shown in the precedingdiagram, configuration management implements the deployment,installation, and support part of asset management. The configurationmanagement tool can help the operations team to reduce downtime byproviding readily available information on asset configuration. Implementing ef fective change management helps us to understand the impact of any changes in the environment. The most popular configurationmanagement tools are Chef, Puppet, Ansible, Bamboo, and so on. Y ou will learn more details about them in Chapter 12 , DevOps and Solution Ar chitectur e Framework . IT management becomes easier if your workload is in a public cloud such asAmazon W eb Services ( A WS ), Azure, or Google Cloud Platform ( GCP ). Cloud vendors provide inbu ilt tools to track a nd manage IT inventories and configuration in one place. For example, A WS provides services such as A WS Config, which tracks all IT inventories that spin up as a part of your A WS cloud workload, and services such as A WS T rusted Advisor , which recommends cost, performance, and security , which you can use to decide to manage your workload. The enterprise creates a framework such as an  Information T echnology Infrastructur e Library  ( ITIL ), which implements an  Information T echnology Service Management ( ITSM ) best practice. An ITIL provides a view on how to implement ITSM. In this section, you learned about asset management and configurationmanagement, which is part of the ITIL framework and more relevant tooperational excellence. ITSM helps or ganizations to run their IT operations daily . Y ou can learn more about ITIL from its governing body AXELOS by visiting their website ( https://www.axelos.com/best-practice-solutions/itil ). AXELOS of fers ITIL certification to develop skills in the IT service management process. As you have learned about planning, let's explore thefunctioning of IT operations in the next section.  The functioning of operationalexcellence Operational excellence is determined by proactive monitoring and quicklyresponding, to recover in the case of an event. By understanding theoperational health of a workload, it is possible to identify when events andresponses impact it. Use tools that help understand the operational health ofthe system using metrics and dashboards . Y ou should send log data to centralized storage and define metrics to establish a benchmark. By defining and knowing what a workload is, it is possible to respondquickly and accurately to operational issues. Use tools to automateresponses to operational events supporting various aspects of yourworkload. These tools allow you to automate  responses  for operational events and initiate their execution in response to alerts. Make your workload components replaceable, so that rather than fixing theissue you can improve recovery time by replacing failed components withknown good versions. Then, analyze the failed resources without impactinga production environment. For the functioning of operational excellence,the following are the areas where appropriate tools are needed: Monitoring system healthHandling alerts and incident response Let's understand each area in more detail with information on the availabletools and processes. Monitoring system health Keeping track of system health is essential to understand workload behavior . The operations team uses system health monitoring to record any anomalies in the system component, and acts accordingly . T raditionally , monitoring is limited to the infrastructure layer keeping track of the server'sCPU and memory utilization. However , monitoring needs to be applied to every layer of the architecture. The following are the significantcomponents where monitoring is applied. Infrastructur e monitoring Infrastructure monitoring is essential and is the most popular form ofmonitoring. Infrastructure includes components required for hostingapplications. These are the core services such as storage, servers, networktraf fic, load balancer , and so on. Infrastructure monitoring may consist of metrics, such as the following: CPU usage : Percentage of CPU utilized by the server in a given period Memory usage : Percentage of random-access memory ( RAM ) utilized by the server in a given periodNetwork utilization : Network packet, in and out over the given period Disk utilization : Disk read/write throughput and input/output operations per second ( IOPS ) Load balancer : Number of request counts in a given period There are many more metrics available, and or ganizations need to customize those monitoring metrics as per their application monitoringrequirement. The following screenshot shows a sample monitoringdashboard for network traf fic: I n f r a s t r u c t u r e m o n i t o r i n g d a s h b o a r d   Y ou can see the preceding system dashboard showing a spike in one day , with color -coding applied for dif ferent servers. The operations team can dive deep in each graph and resources to get a more granular view . Application monitoring Sometimes, your infrastructure is all healthy except applications having anissue due to some bug in your code or any third-party software issues. Y ou may have applied some vendor -provided operating system security patch that messed up your application. Application monitoring may includemetrics, such as following: Endpoint invocation : Number of request counts in a given period Response time : A verage response time to fulfill the request Thr ottle : Number of valid requests spilled out as the system runs out of capacity to handle the additional requestsErr or : Application throw an error while responding to a request The following screenshot shows a sample application endpoint-monitoringdashboard: A p p l i c a t i o n m o n i t o r i n g d a s h b o a r d   There could be many more metrics based on application and technology—for example, a memory garbage collection amount for a Java application, anumber of HTTP POST and GET requests for a RESTful service, a count of 4XX client errors, a count of 5XX server errors for a web application, and so on. Platform monitoring Y our application may be utilizing several third-party platforms and tools that need to be monitored. These may include the following: Memory caching : Redis and Memcached Relational database : Oracle Database,  Microsoft SQL Server , Amazon Relational Database Service ( RDS ), PostgreSQL NoSQL database : Amazon DynamoDB, Apache Cassandra, MongoDBBig data platform : Apache Hadoop, Apache Spark, Apache Hive, Apache Impala, Amazon Elastic MapReduce ( EMR ) Containers : Docker , Kubernetes, OpenShift Business intelligence tool : T ableau, MicroStrategy , Kibana, Amazon QuickSightMessaging system : MQSeries, Java Message Service ( JMS ), RabbitMQ, Simple Queue Service ( SQS ) Sear ch : Elasticsearch, Solr search-based application Each of the aforementioned tools has its own set of metrics that you need tomonitor to make sure your application is healthy as a whole. The followingscreenshot shows the monitoring dashboard of a relational databaseplatform: P l a t f o r m m o n i t o r i n g d a s h b o a r d f o r a r e l a t i o n a l d a t a b a s e m a n a g e m e n t s y s t e m ( R D B M S ) In the preceding dashboard, you can see the database has lots of writeactivity , which is showing that the application is continuously writing data. On the other hand, read events are relatively consistent except for somespikes. Log monitoring T raditionally , log monitoring was a manual process, and or ganizations took a reactive approach to analyze logs when issues were encountered. However , with more competition and increasing expectations from users, it hasbecome essential to take quick action before the user notices the issue. For aproactive approach, you should have the ability to stream logs in acentralized place and run queries to monitor and identify the issue. For example, if some product page is throwing the error out, you need toknow the error immediately and fix the problem before the user complains,else you will suf fer a revenue loss. In the case of any network attack, you need to analyze your network log and block suspicious IP addresses. ThoseIPs may be sending an erroneous number of data packets to bring down yourapplication. Monitoring systems such as A WS CloudW atch, Logstash, Splunk, Google Stackdriver , and so on provide an agent to install in your application server . The agent will stream logs to a centralized storage location. Y ou can directly query to central log storage and set up alerts for any anomalies. The following screenshot shows a sample network log collected in acentralized place: R a w n e t w o r k l o g s t r e a m e d i n a c e n t r a l i z e d d a t a s t o r e Y ou can run a query in these logs and find out the top 10 source IP addresses with the highest number of reject requests, as shown in the followingscreenshot: I n s i g h t f r o m r a w n e t w o r k l o g b y r u n n i n g q u e r y As shown in the preceding query editor , you can create a graph and put an alarm in, if the number of rejections detected crosses a certain threshold,such as more than 5,000. Security monitoring Security is a critical aspect of any application. Security monitoring shouldbe considered during solution design. As you learned when we looked atsecurity in the various architectural components in Chapter 8 , Security Considerations , security needs to be applied at all layers. Y ou need to implement security monitoring to act and respond to any event. Thefollowing significant components show where monitoring needs to beapplied: Network security : Monitor any unauthorized port opening, suspicious IP address, and activity . User access : Monitor any unauthorized user access and suspicious user activity . Application security : Monitor any malware or virus attack. W eb security : Monitor a distributed denial-of-service ( DDoS ) attack, SQL injection, or cr oss-site scripting ( XSS ). Server security : Monitor any gap in security patches. Compliance : Monitor any compliance lapses such as violations of payment card industry ( PCI ) compliance checks for payment applications or the  Health Insurance Portability and Accountability Act ( HIP AA ) for healthcare applications. Data security : Monitor unauthorized data access, data masking, and data encryption at rest and in transit. For monitoring, you can use various third-party tools such as Imperva,McAfee, Qualys, Palo Alto Networks, Sophos, Splunk, Sumo Logic,Symantec, T urbot, and so on. While you are putting application monitoring tools in place to monitor allcomponents of your system, it is essential to monitor the monitoringsystem. Make sure to monitor the host of your monitoring system. Forexample, if you're hosting your monitoring tool in Amazon Elastic Compute Cloud ( EC2 ), then A WS CloudW atch can monitor the health of EC2. Handling alerts and incidentr esponse Monitoring is one part of operational excellence functioning; the other partinvolves handing alerts and acting upon them. Using alerts, you can definethe system threshold and when you want to work. For example, if the serverCPU utilization reaches 70% for 5 minutes, then the monitoring tool recordshigh server utilization and sends an alert to the operations team to takeaction to bring down CPU utilization before a system crash. Responding tothis incident, the operations team can add the server manually . When automation is in place, autoscaling triggers the alert to add more servers asper demand. It also sends a notification to the operations team, which can beaddressed later . Often, you need to define the alert category , and the operations team prepares for the response as per the alert severity . The following levels of severity provide an example of how to categorize alert priority: Severity 1 : Sev1 is a critical priority issue. A Sev1 issue should only be raised when there is a significant customer impact, for which immediatehuman intervention is needed. A Sev1 alert could be that the entireapplication is down. The typical team needs to respond to these kinds ofalerts within 15 minutes and requires 24/7 support to fix the issue.Severity 2 : Sev2 is a high-priority alert that should be addressed in business hours. For example, the application is up, but the rating andreview system is not working for a specific product category . The typical team needs to respond to these kinds of alerts within 24 hoursand requires regular of fice hours' support to fix the issue. Severity 3 : Sev3 is a medium-priority alert that can be addressed during business hours over days—for example, the server disk is goingto fill up in 2 days. The typical team needs to respond to these kinds ofalerts within 72 hours and requires regular of fice hours' support to fix the issue. Severity 4 : Sev4 is a low-priority alert that can be addressed during business hours over the week— for example, Secur e Sockets Layer ( SSL ) certification is going to expire in 2 weeks. The typical team needs to respond to these kinds of alerts within the week and requiresregular of fice hours' support to fix the issue. Severity 5 : Sev5 falls into the notification category , where no escalation needed, and it can be simple information—for example,sending a notification that deployment is complete. Here, no response isrequired in return since it is only for information purposes. Each or ganization can have dif ferent alert severity levels as per their application needs. Some or ganizations may want to set four levels for severity , and others may go for six. Also, alert response times may dif fer . Maybe some or ganization wants to address Sev2 alerts within 6 hours on a 24/7 basis, rather than waiting for them to be addressed during of fice hours. While setting up an alert, make sure the title and summary are  descriptive and concise . Often, an alert is sent to a mobile (as an SMS) or a pager (as a message) and needs to be short and informative enough to take immediateaction. Make sure to include proper metrics data in the message body . In the message body , include information such as  The disk is 90% full in pr oduction-web-1 server  rather than just saying The disk is full . The following screenshot shows an example alarm dashboard: A l a r m d a s h b o a r d As shown in the preceding alarm dashboard, there is one alarm in progresswhere my billing char ges went above 1,000 USD. The bottom three alarms have an OK status as data is collected during monitoring that is well within the threshold. Four alarms are showing Insuf ficient data , which means there are not enough data points to determine the state of resources you aremonitoring. Y ou should only consider this alarm good if it can collect data and move into the OK state. T esting of incident response in the case of critical alerts is important to make sure you are ready to respond as per the defined SLA. Make sure your threshold is set up correctly so that you have enough room to address theissue, and, also, don't send too many alerts. Make sure that as soon as theissue resolved, your alert gets reset to the original setting and is ready tocapture event data again. An incident is any unplanned disruption that impacts the system andcustomer negatively . The first response during an incident is to recover the system and restore the customer experience. Fixing the issue can beaddressed later as the system gets restored and starts functioning. Theautomated alert helps to discover the incident actively and minimizes userimpact. This can act as a failover to a disaster recovery site if the entiresystem is down, and the primary system can be fixed and restored later . For example, Netflix uses the Simian Army ( https://netflixtechblog.com/the-netflix -simian-army-16e57fbab116 ), which has Chaos Monkey to test system reliability .  Chaos Monkey orchestrates random termination of a production server to test if the system can respond to disaster events without any impacton end users. Similarly , Netflix has other monkeys to test various dimensions of system architecture, such as Security Monkey , Latency Monkey , and even  Chaos  Gorilla , which can simulate outage of the entire availability zone. Monitoring and alerts are critical components to achieving operationalexcellence. All monitoring systems typically have an alert feature integratedwith them. A fully automated alert and monitoring system improves theoperations team's ability to maintain the health of the system, provideexpertise to take quick action, and excel in the user experience. Impr oving operational excellence Continuous improvement is required for any process, product, orapplication to excel. Operational excellence needs continuous improvementto attain maturity over time. Y ou should keep implementing small incremental changes as you perform RCA and learn a lesson from variousoperations' activities. Learning from failure will help you to anticipate any operational event thatmay be planned (such as deployments) or unplanned (such as utilizationsur ge). Y ou should record all lessons learned and update remedies in your operation runbook. For operational improvement, the following are theareas where you need appropriate tools: IT Operation Analytics ( IT OA ) Root cause analysis ( RCA ) Auditing and reporting IT OA IT OA is the practice of gathering data from various resources to make a decision and predict any potential issue that you may encounter . It's essential to analyze all events and operational activities in order to improve.Analyzing failures will help to predict any future event and keep the teamready to provide the appropriate response. Implement a mechanism tocollect the logs of operations events, various activities across workloads,and infrastructure changes. Y ou should create a detailed activity trail and maintain an activity history for audit purposes.  A lar ge or ganization could have hundreds of systems generating a massive amount of data. Y ou need a mechanism to ingest and store all logs and event data for a length of time, such as 90 or 180 days, to get insight. IT OA uses big data architecture to store and analyze multiple terabytes of datafrom all over the place. IT OA helps to discover any issue that you could not find by looking at individual tools and helps to determine dependenciesbetween various systems, providing a holistic view . As shown in the following diagram, each system has its own monitoringtool that helps to get insights and maintains individual system components.For operation analytics, you need to ingest this data in a centralized place.All operation data collection in one place gives a single source of truth,where you can query required data and run analytics to get a meaningfulinsight: B i g d a t a a p p r o a c h f o r I T O A   T o create an operation analytics system, you can use scalable big data storage such as  Amazon Simple Storage Service ( S3 ). Y ou can also store data in an on-premises Hadoop cluster . For data extraction, the agent can be installed in each server , which can send all monitoring data to a centralized storage system. Y ou can use the Amazon CloudW atch agent to collect data from each server and store it in S3. Third-party tools such as  ExtraHop and Splunk can help to extract data from various systems. Once data is collected in centralized storage, you can perform atransformation to make data ready for search and analysis. Datatransformation and cleaning can be achieved using a big data applicationsuch as Spark, MapReduce, A WS Glue, and so on. T o visualize the data, you can use any business intelligence tool such as T ableau, MicroStrategy , Amazon QuickSight, and so on. Here, we are talking to about building anextract, transform, and load ( ETL ) pipeline. Y ou will learn more details in Chapter 13 , Data Engineering and Machine Learning . Y ou can further perform machine learning to do predictive analysis on a future event. RCA For continuous improvement, it is essential to prevent any error happeningagain. If you can identify problems correctly , then an ef ficient solution can be developed and applied. It's important to get to the root cause of theproblem to fix the problem. Five whys  is a simple, yet most ef fective, technique to identify the root cause of a problem. In the five whys technique , you gather the team for a retrospective look at an event and ask five consecutive questions to identify actual issues. T ake an example where data is not showing up in your application monitoringdashboard. Y ou will ask five whys to get to the root cause. Pr oblem : Application dashboard not showing any data. 1 . Why: Because the application is unable to connect with the database. 2 . Why: Because the application is getting a database connectivity error . 3 . Why: Because the network firewall is not configured to the databaseport. 4 . Why: Because the configuring port is a manual check andinfrastructure team missed that. 5 . Why: Because the team don't have the skills and tools for automation. Root Cause : Manual configuration error during infrastructure creation. Solution : Implement a tool for automated infrastructure creation. In the preceding example, a first-instance issue looks like it is related to theapplication. After the five whys  analysis, it turns out to be a bigger problem and there is a need to introduce automation to prevent similar incidents.RCA helps the team to document lessons learned and continuously buildupon it for operational excellence. Make sure to update and maintain yourrunbook-like code and share best practices across the team.  Auditing and r eporting An audit is one of the essential activities to create recommendations andidentify any malicious activity in the system by internal or externalinterference. An audit becomes especially important if your applicationneeds to be compliant as per regulatory body requirements—for example,PCI, HIPP A, Federal Risk and Authorization Management Pr ogram ( FedRAMP ), International Organization for Standardization ( ISO ), and so on. Most of the compliant regulatory bodies need to conduct regularaudits and verify each activity going into the system to prepare acompliance report and grant a certificate. An audit is essential to prevent and detect security events. A hacker maysilently get into your system and systematically steal information withoutanyone noticing. Regular security audits can uncover a hidden threat. Y ou may want to conduct a regular audit for cost optimization to identify ifresources are running idle when not required. Also, determine resourcedemand and available capacity so that you can plan. In addition to alert and monitoring, the operations team is also responsiblefor saving the system from any threat by enabling and conducting the audit.An IT audit makes sure you safeguard IT assets and license protection andthat you ensure data integrity and operations adequately to achieve youror ganizational goal.  Auditing steps include planning, preparing, evaluation, and reporting. Anyrisk item needs to be highlighted in the report, and follow-ups will beconducted to address open issues. For operational excellence, the team canperform internal audit checks to make sure all systems are healthy and thatproper alerts are in place to detect any incidents. Achieving operational excellence inthe public cloud A public cloud provider such as A WS, GCP , and Azure provides many inbuilt capabilities and guidance to achieve operational excellence in thecloud. Cloud providers advocate automation, which is one of the mostessential factors for operational excellence. T aking the example of the A WS cloud, the following services can help to achieve operational excellence: Planning : Operational excellence planning includes identification of gaps and recommendation, automating via scripting, and managingyour fleet of servers for patching and updates. The following A WS services help you in the planning phase: A WS T rusted Advisor : A WS T rusted Advisor checks your workload based on prebuilt best practice and providesrecommendations to implement them.A WS CloudFormation : W ith A WS CloudFormation, the entire workload can be viewed as code, including applications,infrastructure, policy , governance, and operations. A WS Systems Manager : A WS Systems Manager provides the ability to manage cloud servers in bulk for patching, update, andoverall maintenance. Functioning : Once you have created o perational excellence best practice and applied automation, you need continuous monitoring ofyour system to be able to respond to an event. The following A WS services help you in system monitoring, alerts, and automatedresponse: Amazon CloudW atch : CloudW atch provides hundreds of inbuilt metrics to monitor workload operation and trigger alerts as perthe defined threshold. It provides a central log managementsystem and triggers an automated incident response.A WS Lambda : The A WS service used to automate responses to operational events is A WS Lambda. Impr oving : As incidents come into your system, you need to identify their pattern and root cause for continuous improvement. Y ou should apply the best practice to maintain the version of your scripts. Thefollowing A WS services will help you to identify and apply system improvements: Amazon Elasticsear ch : Elasticsearch helps to learn from experience. Use Elasticsearch to analyze log data to gain insightand use analytics to learn from experience.A WS CodeCommit : Share learning with libraries, scripts, and documentation by maintaining them in the central repository as acode. A WS provides various capabilities to run your workload and operations as code. These capabilities help you to automate operations and incidentresponse. W ith A WS, you can easily replace failed components with a good version and analyze the failed resources without impacting the productionenvironment. On A WS, aggregate the logs of all system operation and workload activities, and infrastructure, to create an activity history , such as A WS CloudT rail. Y ou can use A WS tools to query and analyze operations over time and identify a gap for improvement. In the cloud, resource discovery iseasy , as all assets are located under the API- and web-based interfaces within the same hierarchy . Y ou can also monitor your on-premises workload from the cloud. Operational excellence is a continuous ef fort. Every operational failure should be analyzed to improve the operations of your application. Byunderstanding the needs of your application load, documenting regularactivities as a runbook, and following steps to guide issue handling, usingautomation, and creating awareness, your operations will be ready to dealwith any failure event. Summary Operational excellence can be achieved by working on continuousimprovement as per operational needs, and lessons learned from past eventsusing RCA. Y ou can achieve business success by increasing the excellence of your operations. Build and operate applications that increase ef ficiency while building highly responsive deployments. Use best practices to makeyour workloads operationally excellent. In this chapter , you learned about the design principles to achieve operational excellence. These principles advocate operation automation,continuous improvement, taking an incremental approach, predictingfailure, and being ready to respond. Y ou learned about various phases of operational excellence and corresponding to technology choices. In theplanning phase, you learned about IT asset management to track theinventory of IT resources and identify dependencies between them usingconfiguration management. Y ou learned about alerts and monitoring in the functioning phase of operational excellence. Y ou got an idea about various kinds of monitoring, with examples such as infrastructure, application, log, security , and platform monitoring. Y ou learned about the importance of alerts, and how to define alert severity and respond to it. During the improvement phase of operational excellence, you learned aboutanalytics in IT operations by building a big data pipeline, methods toperform RCA using the five whys, and the importance of auditing to save the system from any malicious behaviors and unnoticed threats. Y ou learned about operational excellence in the cloud and dif ferent inbuilt tools that can be utilized for operational excellence in the A WS cloud. As of now , you have learned best practices in the areas of performance, security , reliability , and operational excellence. In the next chapter , you will learn about best practices for cost optimization. Y ou will also learn about various tools and techniques to optimize overall system costs and how toleverage multiple tools in the cloud to manage IT expenditure.Cost Considerations One of the primary goals of any business is to increase profitability whileserving its customers. Cost is the key parameter of discussion when aproject is initiated. Having an application upgrade and adding new productfeatures heavily depends upon the amount of funding it can get. Theproduct's cost is everyone's responsibility and needs to be considered inevery phase of the product life cycle (from planning to post-production).This chapter will help you understand the best practices for optimizing costsfor your IT solutions and operations. Cost optimization is a continuous process and needs to be managed carefully without sacrificing customer experience. Cost optimizationdoesn't mean cost reduction but rather reducing the business risk by maximizing r eturn on investment ( ROI ). Y ou will need to understand your customer needs before planning any cost optimization and actaccordingly . Often, if customers are looking for quality , they are ready to pay a higher price. In this chapter , you will learn various design principles for the cost optimization of your solution. The cost aspect needs to be considered atevery phase and component of the architecture. Y ou will get an understanding of the right selection of technology to ensure costoptimization at every layer . Y ou will learn the following best practices of cost optimization in this chapter: Design principles for cost optimizationT echniques for cost optimization Cost optimization in the public cloud By the end of the chapter , you will have learned about various techniques to optimize cost without risking business agility and outcome. Y ou will have learned various methods to monitor the cost and apply governance for costcontrol.  Design principles for costoptimization Cost optimization includes the ability to increase business value andminimize risk while reducing the cost of running a business. Y ou need to plan by estimating budget and forecasting expenditure. T o realize cost gain, you need to implement a cost-saving plan and closely monitor yourexpenditure. Y ou can follow several principles that help you achieve cost optimization. The following sections talk about the common design principles that helpyou to optimize cost. Y ou will find that all cost optimization design principles are closely related and complement each other . Calculating the total cost ofownership Often, or ganizations tend to overlook the total cost of ownership ( TCO ) and take decisions based on the upfront cost to acquire software andservices, known as  capital expenditur e   (CapEx) . While the  upfront cost  determination is essential, in the long run, the TCO matters the most. The TCO includes both CapEx and operational expenditur e ( OpEx ), covering all the dimensions of the application life cycle. The CapEx costincludes the price or ganizations pay upfront to acquire services and software, while OpEx includes the cost of operation, maintenance, training,and retirement of software application. Y ou should consider all associated costs to help to make more strategic decisions while calculating your ROI inthe long run. For example, when you buy a refrigerator , which is going to run 24/7, you look for an ener gy-saving rating to keep your electricity bill  low . Y ou are ready to pay a higher price upfront as you know the total cost over time willbe low due to the saving in the ener gy bill. Now let's take an example of a data center . There is an upfront hardware acquisition cost involved, the CapEx. However , the data center setup requires additional ongoing costs, the OpEx, which includes heating,cooling, rack maintenance, infrastructure administration, security , and so on. For a typical use case, when you are purchasing and implementing software,consider the following costs to calculate the TCO: T C O f o r s o f t w a r e Let's look at this at a more granular level. Each TCO component has thefollowing common costs involved for off-the-shelf software such as Oracle or MS SQL database: Pur chase and setup costs:   These are  the upfront cost to acquiring the software and the services to deploy software. This includes thefollowing: Software price includes software with user licensesHardware cost includes purchasing a server and storage to deploysoftwareImplantation cost includes the time and ef fort to get it ready for productionMigration cost includes moving data to the new system Operational and maintenance costs : This continues the cost of service to keep the software running for the business use case. This costincludes the following: Software maintenance and supportPatching and updateEnhancementData center cost to maintain hardware serverSecurity Human r esour ces and training costs: This is the overhead cost to train staf f so that they can use the software to address business activities. This cost includes the following: Application admin staf f IT support staf f Functional and technical consultantT raining cost and training tools When looking for a solution, you will have multiple choices (such as takingout a subscription for a  softwar e as a service ( SaaS ) product such as Salesfor ce CRM ). The SaaS model is mostly subscription-based, so you need to determine whether you are getting the desired ROI for a moresignificant number of uses. Y ou can take a hybrid approach and use the cloud to handle your hardware by choosing  the Infrastructur e as a Service ( IaaS ) option and installing of f-the-shelf software. Overall, if the available software doesn't meet your requirements, you can choose to build it byyourself. In any scenario, you should calculate the TCO to make a decisionwhere you can make the maximum ROI. Let's look at budget and forecastplanning, which can help to control TCO and achieve ROI. Planning the budget and for ecast Every business needs to plan its expenditure and calculate ROI. Planningthe budget gives guidance to or ganizations and teams on cost control. Or ganizations plan a long-term budget, for 1-5 years, which helps them to run the business based on the funding required. These budgets then comedown to the individual project and application levels. During solutiondesign and development, the team needs to consider the available budgetand plan accordingly . The budget helps to quantify what the business wants to achieve. The forecast provides an estimate of what the company ismaking. Y ou can consider budget planning as important strategic planning in the long run, and the forecast provides an estimate at a more tactical level todecide the business direction. In application development and operation,you can quickly lose track of the budget and overrun costs in the absence ofa budget and a forecast. These two terms may be confusing, so let'sunderstand the clear dif ference between a budget and a forecast: Budget For ecast Represents future result and cashflow for business objectives that youwant to achieve Represents revenue and currentsituation of the business Plans for the long term, for example1-5 years Plans month to month orquarterly Is adjusted less frequently , maybe Is updated more regularly based once in a year , based on business drivers on actual business progress Helps to decide business directionssuch as or ganization restructuring based on actual cost versus budgetedcost Helps to adjust short-termoperational costs such asadditional staf fing Helps to determine performance bycomparing planned cost versusactual cost Isn't used for performancevariation but for streamliningprogress   Forecast information helps you to take immediate action, while the budgetmay become unachievable due to changes in the market. As shown in thefollowing diagram, while you are working on day-to-day solutions,developments based on historic expenditure forecasts can prompt you toadjust the next month's cost: B i l l i n g a n d f o r e c a s t r e p o r t In the preceding billing and forecast report, your monthly budget is $3,000and the forecast is showing that, by the end of the month, you will overrunyour budget. Here, the forecast helps you act and control cost to stay withinbudget. Let's look at the mechanism to improve cost-ef ficiency by managing demand and service in the next section. Managing demand and servicecatalogs Almost every or ganization has a centralized IT team, which works with internal business partners such as the application development team andsupport team of various business units . The IT team manages the demand for IT infrastructure, which includes the cost of all software, hardware, andsupport to manage application hosting. Often, business partners lackunderstanding of the cost drivers for the IT services that they use. Forexample, application development tends to overprovision their developmentor test environment, causing an additional cost. Other factors that get the right sizing and demand forecasting from variousor ganization units can help to match supply and demand. By consolidating all requirements in one place, an or ganization can benefit from economies of scale. Y ou may achieve a lower variable cost because a lar ge contract can achieve higher economies of scale. The right demand from all or ganization units is aggregated, which translates into lower prices. Or ganizations can take one of the following two approaches to manage demand and service: Demand management : T o save costs in your existing IT environments (where you may observe overspending may beprevalent), you can take the demand-led appr oach . It helps to improve cost ef ficiency in the short term, as you are not introducing many new services. Y ou can analyze historical data to understand factors that are driving demand and capture cases of overprovisioning. Y ou should establish a process between the IT team and business partners tostreamline operational costs for IT services.Service catalog management : If there is a demand for new services and you don't have much historical data, then you can take the service- led appr oach . In this approach, you need to understand the demand for the most frequently used services and create a catalog for that. Forexample, if the development team is asking for a Linux server with aMySQL database to create a dev environment, the IT team can create aservice catalog which helps the dev team to acquire a small instance ofLinux and a database server . Similarly , the IT team can identify the most common set of services and attach a granular cost to that. Each approach can have a significant cost saving in the short and long term.However , these transformations present significant challenges as you need to change the project planning and approval process. The business andfinance team need to align and understand the clear relationship betweenbusiness growth and an increase in IT capacity . The cost model needs to be built around the most ef ficient approach by combining of ferings from the cloud, on-premises, and of f-the-shelf. Keeping track of expenditur e By tracking expenditure, you can find individual system costs and tag themto the system or business owner . T ransparent expenditure data helps to identify ROI and reward owners, those able to optimize resources andreduce cost. It can help you to determine what it is costing every month fora department or project. Saving cost is a shared responsibility and you need to have a mechanism tohold everyone accountable for cost-saving. Often, or ganizations introduce a show-back or charge-back mechanism to share cost responsibility between or ganizational units. In the show-back approach, the centralized billing account informs eachor ganization unit regarding their expenditure but doesn't char ge the actual amount. In the char ge-back mechanism, each business unit within an or ganization manages its own budget under a master payee account. The master account char ges back the amount to the business units as per their IT resource consumption on a monthly basis. When you are starting cost control for your or ganization, it is better to start with show-back as a stepping stone and move to char ge-back as the or ganizational model matures. For each business unit, you should create expenditure awareness by configuring notifications, so that teams get analert as they approach the forecasted or budgeted amount of consumption.Y ou should create a mechanism to monitor and control your cost by appropriately assigning them to the right business initiative. Providevisibility to create accountability for cost expenditures for each team. Costtracking will help you to understand team operations. Each workload is dif ferent; you should use the pricing model that suits your workload to minimize cost. Establish mechanisms that ensure businessobjectives are achieved by applying cost optimization best practice. Y ou can avoid overspending by defining a tagging strategy and applying the  check- and-balance   approach. Continuous cost optimization If you follow cost optimization best practices, you should have a good costcomparison with existing activity . It's always possible to reduce the cost of your applications that are migrated and matured over time. Costoptimizations should never end until the cost of identifying money-savingopportunities is more than the amount of money you are going to save.Until that point is reached, you should continually monitor your expenditureand look for new ways to save on cost. Y ou should keep finding an area to save costs by removing  idle r esour ces . For an architecture that is balanced in terms of its cost andperformance, ensure that the cost paid for resources is well utilized andavoids any significantly underutilized IT resources such as server instances.A biased utilization metric showing exceptionally high or low cost willhave an adverse impact on your or ganization’ s business.  Application-level metrics for cost optimization need to be consideredcarefully . For example, introduce archival policies to control data storage capacity . T o optimize the database, you should check for appropriate database deployment needs, such as if multi-location deployments for thedatabase are really necessary or whether provisioned Input/Output Operations Per Second  ( IOPS ) are applicable as per your database utilization needs. T o reduce your administrative and operational overhead, you can use the SaaS model; it will help your employees to focus onapplications and business activities. T o identify a gap and apply necessary changes for cost-saving, you should implement resource management and change control processes during theproject life cycle. Y our goal is to help your or ganization design the architecture as optimally and cost-ef fectively as possible. Keep looking for new services and features that might directly reduce your costs. T echniques for cost optimization T o gain a competitive edge and keep up with rapid growth, enterprises are investing more in technology . W ith economic instability , cost optimization becomes an essential but challenging task. These companies spend a lot oftime and research to reduce costs in the procurement process, operation, andvendors. Many companies even share data centers, call centers, andworkspaces as a cost-saving method. Sometimes or ganizations delay upgrades to avoid buying new expensive hardware. The or ganization can save more if it takes a broader look into the overall information technology architecture across its business units. Improvingexisting architecture can open doors to bring more opportunities andbusiness to the company even if it requires a bit more adjustment in thebudget. Let's identify the focus area where companies can sav e cost and gain opportunities to bring more revenue with techniques such as moving tothe cloud, si mplified architecture, virtualization, and shared resources. Reducing ar chitectural complexity Or ganizations often lack a centralized IT architecture, resulting in each business unit trying to build its own set of tools. Lack of overall controlcauses a lot of duplicate systems and data inconsistency . IT initiatives in individual business units are driven by a short-term goal and are not wellaligned with long-term or ganizational vision such as the  digital transformation of the entire or ganization . Further , it adds complexity to maintain and upgrade those systems. T aking a simple step to define set standards and avoid duplication can help to save costs. In the following diagram, you can see a complex architecture on the left-hand side, where business units are working in their own application withoutany standardization, which is causing duplicate applications with a lot ofdependencies. This kind of architecture results in higher cost and risk. Anynew experiment takes a long time to market, which results in losing thecompetitive edge. A standard process can provide a holistic view and highflexibility to create an agile environment by applying automation, whichhelps to reduce the overall cost and results in a more significant ROI: A r c h i t e c t u r a l s t a n d a r d i z a t i o n T o reduce the architectural complexity , the first thing is to eliminate duplication and identify the function reuse across the business unit. Duringgap analysis of existing architecture, you will find there is so much code, so many existing components, and a project that can be reused again across theor ganization to support your business requirement. T o reduce the complexity of IT architecture, think of an out-of-the-box solution that fits your businessneeds and provides an ROI. Customization should be your last approach ifno other option is available.  Any new application needs to have a more accessible integration mechanismto interact with the existing system using service-oriented ar chitectur e ( SOA ). Harmonizing the user interface design across the application provides a set of standard UI packages, which can be reused for any newapplication. Similarly , other modules can be reutilized with service-oriented design. Y ou learned about SOA patterns in Chapter 6 , Solution Ar chitectur e Design Patterns , which help you keep all the dif ferent pieces of software working separately and still communicating with each other to build a fullsystem. In the modular approach, each team gets the responsibility of developing aservice, which every team across the or ganization can use to avoid duplication. As an architect, you should help the team to create a service-oriented design, where each team handles individual architecturecomponents as a service that can be developed independently . W ith the help of microservices architecture, you can deploy an entire application in amodular way , and if one component is not working, you can rework it without impacting the whole application. For example, a payment servicedeveloped to collect payment from a customer visiting an e-commercewebsite can be used to make payments to vendors in the vendor managementsystem. Once you set up a centralized IT architecture, taking a modular approachhelps you to keep the cost down. Empowering your IT architecture team canhelp to align or ganizational units with the company's vision and support other parallel projects to follow the overall strategy . It also helps to provide consistency in other critical services that are often overlooked, such as legal,accounting, and human resources. W ith the help of the IT architecture team, you can get excellent feedback and make sure that projects are aligned with business needs and requirements.By overseeing the overall architecture across teams, an architect can adviseif whether there is any duplicate ef fort, project, process, or system that is not aligned with the business need. The centralized architecture will reducecomplexity and tech debt, bring more stability , and increase quality . The overall idea of centralized architecture is to increase IT ef ficiency , so let's learn more about that. Incr easing IT efficiency Nowadays, every company is using and consuming IT resources. There istoo much cost used by too many servers, laptops, storage capacity , and the software license. The license is one of the resources that is sometimesunderused, undiscovered, idle, or installed incorrectly , and consumes a lot of funding. A centralized IT team can lead the ef fort for license optimization by keeping track of used software licenses and retiringadditional licenses. They can save costs by negotiating a bulk discount withthe vendor . T o increase IT ef ficiency , you may want to cancel non-compliant projects that take additional funding and resources. Also, you should help teams torevisit the strategy to continuous support or terminate any unused and non-aligned projects. The following methods can be considered for costoptimization: Reevaluate project having high spending but may not be well alignedwith the business vision. Reshape  projects  have high values but not direct reports to IT strategy . De-prioritize projects that have  little to no business value  even though they are aligned with IT strategy . Cancel  non-compliant projects with low business values. Decommission or retire unused applications.Replace old legacy systems by modernizing them to reduce highmaintenance costs.A void duplicate projects by reutilizing existing applications. Wherever possible, consolidate data and develop the integrated datamodel. Y ou will learn about maintaining a centralized data lake in Chapt er 13 , Data Engineering and Machine Learning . Consolidate vendor procurement across the or ganization to save cost on IT support and maintenance expenditure. Consolidate any system that does the same thing as payment andaccess management.Eliminate costly , wasteful, overprovisioned projects and expenditure. Moving to the cloud can be an excellent consideration to increase ITresources ef ficiently and reduce costs. The public cloud, such as A WS , of fers a  pay-as-you-go  model that helps you to only pay for what you are using. For example, the developer desktop can shut down during non-working hours and weekends, which can reduce workspace costs by up to70% . The batch processing system needs to be brought up only to process jobs and can be shut down immediately afterward. It works  just like you switch of f any electrical appliances when not required to save electricity costs. Applying automation is a great mechanism to increase overall IT ef ficiency . Automation not only helps to eliminate costly human labor but reduces thetime spent performing daily routine jobs without error . Automate things wherever possible, for example, by provisioning resources, runningmonitoring jobs, and processing data. Make sure to do the right trade-of f to improve results while deciding to optimize costs. Let's take an example. If you are going to a theme parkwhere you want to go on lots of good rides, then you are willing to pay ahigher price as you can see the value of the money you are spending. T o attract more customers, if the vendor decided to reduce the price and makeadjustments by lowering the number of enjoyable rides, there is a chancethat you will go to another theme park as you are looking for a good time.Here, competitors will gain an edge and attract existing customers, whilethe current vendor will lose business. In this case, cost reduction is addingbusiness risk, which is not the right cost-optimization approach. Y our goal should be measurable, and these measures should focus on both business output and the cost of your system. The quantitative measure helpsyou understand the impact of increasing output and reducing the cost. Theor ganizational and team-level goals  must align with the end users of the application. At an or ganizational level, the goals will be across or ganizational business units. At the team level, they will be more aligned with individual systems. Y ou can set up a goal, for example, to reduce the cost per transaction or output of order by 10 % every quarter or 15% every 6 months. Defining the goal ensures that systems improve over their lifetime. Applying standardization andgovernance Or ganizations need a strategy to analyze misalignment and overconsumption, reduce complexity , and define guidelines to use appropriate and ef ficient systems and implement a process wherever it is required. Creating and implementing these guidelines will help companies todevelop a standard infrastructure and reduce duplicate projects andcomplexity . T o implement governance, you need to set up resource limits across the or ganization. Putting the service catalog in place with infrastructure as code helps to ensure that teams are not overprovisioned with resources beyondtheir allocated capacity . Y ou should have a mechanism to understand and take action on business requirements quickly . T ake both resource creation and decommission into account when applying resource limits and definingthe process to change them. Businesses operate multiple applications by various teams. Those teams canbelong to dif ferent business units, within their revenue stream. The capability to determine resource costs to the application and business unit orteam drives ef ficient usage behavior and helps reduce cost. Y ou can define resource capacity based on cost attribution and the requirements of thegroup, or ganization unit, or department. T o or ganize cost structure, you can use resource tagging along with account structuring. As shown in the following screenshot, you can or ganize your accounts in dif ferent organization units ( OUs ), such as  HR and Finance , and each department under the OU can have its own account. For example, here, HR has separate accounts for Payroll and Marketing , while Finance has separate accounts for  Sales and Marketing : E n t e r p r i s e a c c o u n t s t r u c t u r e f o r o r g a n i z a t i o n a l u n i t s In the preceding account structuring strategy , you can control costs at each business unit and department level. Adopting a char ge-back mechanism for each department increases accountability for cost at a more granular level,which helps to optimize cost. Account structuring helps you to apply highsecurity and compliance standards across the or ganization. As each account is linked to a parent account, you can get a great deal on the mass utilizationof resources from vendors by consolidating expenditure across theor ganization. As shown in the following screenshot, to get full cost visibility andconsolidation across resources, you can tag each resource provisioned at theteam level, which provides more granular control: R e s o u r c e t a g g i n g f o r c o s t v i s i b i l i t y In the preceding diagram, you can see tagging strategy , which indicates that the given server is for application deployment and is utilized by thedevelopment team. This given server is owned by the marketing departmentof the finance business unit. In this way , the or ganization can get a granular level of cost expenditure visibility , and the team will be more frugal in their spending. However , you may want to adopt the show-back mechanism at the team level compared to the char ge-back mechanism at the department and business unit level. Y ou can define your mechanism for tagging, where you can attach a name and value, such as resource name and owner name, to a ny resource . Almost every public cloud provider gives tagging capabilities out of the box. For on-premise, you can embed server metadata such as DNS name or hostname.T agging not only helps you to or ganize costs but also to define a capacity limit, security , and compliance. It can be an excellent tool for inventory management and to keep an eye on the growing need for resources at everylevel of the or ganization. Business leaders should evaluate the overall requirement to create ef ficient IT architectures. It requires collaboration to develop robust IT architectureand define governance across functional teams to set up accountability . Also, set up a standard to review the architecture, create the baseline for any newproject initiative, and explain the process that will make sure that the systemcomplies with the correct architecture and identify the route to improvement. Engage all impacted stakeholders within your business in usage and costdiscussions. CFO and application owners must understand resourceconsumption and purchasing options. Department owners must understandthe overall business model and the monthly billing process. It will help to setup the direction for the business units and the whole company .  Make sure third-party vendors are aligned to your financial goals and canadjust their engagement models. V endors should provide a cost analysis of any application they own and develop. Each team within the or ganization should be able to translate business, cost, or usage factors from themanagement into system adjustments, which help the application toimplement and achieve the desired goals of the company . Monitoring cost usage and r eport Accurate cost factors help you to determine the profitability of business unitsand products. Cost tracking helps you to allocate resources at the right placeto increase ROI. Understanding cost drivers helps you to control yourbusiness expenditure. T o optimize costs, you must know your expenditure pattern across the or ganization. Y ou need to have visibility of IT cost expenditures over a period of time to determine cost-saving opportunities. Y ou can take the required steps for cost optimization and understand the impact by creating avisualization of cost trends, which shows historical cost and forecasts byresources and departments across the or ganization. Y our team needs to gather data by logging all data points, analyze them with monitoring, andthen create a visualization report. T o identify cost-saving opportunities, you need detailed insight into your workload resource utilization. Cost optimization depends upon your abilityto forecast future spending and put methods in place to align cost and usageas per your forecast. The following are the primary areas where you want tohave data visualization for cost-saving: Determine the most significant investments in resourcesAnalyze and understand your expenditure and usage dataBudget and forecastReceive an alert when you exceed your budgeted or forecastedthresholds The following report is showing resources expenditure over 6 months inA WS. Looking at the visualization, you can see that data warehousing service Amazon   Redshift , represented by the purple bar , is consuming maximum costs with an increase in the trend until November . As the business unit can visualize the high cost in October , it prompts the system admin to take an in-depth look at cost optimizations, where they findoverprovisioned resources. The admin performs cleanup by stopping additional server instances in November , which brings down the data warehousing service cost by 90%: R e s o u r c e c o s t a n d u s a g e r e p o r t The preceding report helped business owners to understand cost patterns andtake a reactive approach to cost control. The reactive approach causedhidden costs, which went undetermined for a specified period. For theproactive approach, the forecast can help to make the decision ahead of time. The following report shows daily cost expenditure in the filled blue bars andforecast spending in the empty blue bars. Looking at the report, you can seethat it is likely that cost may increase in the next couple of weeks, and youcan take action to understand cost attributes to control costs: C o s t t r e n d a n d c o s t f o r e c a s t r e p o r t Monitoring your costs against the budget can give you another proactivemeasure to control costs. Setting up an alert when expenditure reaches acertain threshold of the budget (for example, 50% or 80%) helps you toreview and adjust your ongoing costs. In the following report, you can visually determine the current cost againstbudgeted cost, which reached 42% in the first week of the month. At thisrate, your forecast is going over 182% of the budgeted cost and requiresattention to adjust ongoing expenditure: C o s t a n d b u d g e t r e p o r t Cost and budget reports help you to control costs by taking proactive action.Combining your actual running costs with budgets and forecasts provides agreat deal of cost control on a daily basis. Y ou can also set up an alert when your actual cost reaches a certain threshold in the budget or forecast. It alerts you proactively via email or mobilemessage and tells you to take proactive action to control costs. In the following screenshot, you can see an alert has been set up for whenthe actual cost reaches 80% of the budget. Y ou can set up multiple alerts to get information when the cost reaches 50% or 80% of the budget or forecast: A l e r t a g a i n s t a c t u a l c o s t One way to do cost control is by right-sizing your environment withresources monitoring, and trigger alarms for over - or underutilization. Analysis of resources can be performed using monitoring tools such asSplunk or CloudW atch and custom logs, where customized metrics such as application memory utilization of your system can be monitored to performright-sizing. Low utilization of a resource could be a criterion for identifyingopportunities for cost optimization. For example, CPU utilization, RAM utilization, network bandwidth, and the number of connections to theapplication can be analyzed and monitored. Y ou need to be careful when resizing your environment to make sure you are not impacting customer experience. The following are best practices to applywhen you perform right-sizing: Make sure monitoring reflects the end user experience. Select thecorrect period. For example, performance metrics should cover 99% ofthe user's request-response time rather than taking an average responsetime.Select the correct monitoring cycle, such as every hour , day , or week. For example, if you are conducting daily analysis, you might miss aweekly or monthly cycle of high utilization and under provision yoursystem.Assess the cost of change against the cost-saving. For example, youmay have to perform additional testing or engage resources to performresizing. This cost-benefit analysis will help you to assign resources. Measure application utilization against your business requirement, forexample, how many user requests are expected to come by the end of themonth or during peak season. Identifying and optimizing the utilization gapleads you to save costs. For this, use the right tool that covers all dimensionsfrom cost-saving to system utilization and impact on customer experiencedue to changes. And then utilize reports to understand business ROI impactdue to cost changes. Cost optimization in the publiccloud The public cloud, such as A WS , Micr osoft Azur e , and Google Cloud Platform  ( GCP ), provides a great deal of cost optimization with the pay-as- you-go model. The public cloud cost model allows customers to trade capital expenses for variable expenses, paying for IT resources as they consumethem. Operational expenses are usually lower due to economies of scale. Itcould be cost-ef fective to be in the cloud and get the benefit of continued price reductions that occur over time. The other advantage is that you getadditional tools and functionality out of the box with a cloud provider suchas A WS, which helps you to achieve better agility . Y ou need a dif ferent mindset when defining the cloud cost structure model as it is pretty dif ferent from traditional cost, most of which most enterprises have been following for decades. In the cloud, you have all the infrastructureavailable at your fingertips, which requires greater control and regulation.Clouds provide several tools for cost governance and regularization. Forexample, in A WS, you can set up service limits per account, so the dev team cannot utilize more than 10 servers, and production can have the requirednumber of servers and databases with a buf fer . In the cloud, all resources are associated with accounts, so it's easy to keeptrack of IT resource inventories in a single place and monitor theirutilization. In addition to that, you get tools that can collect data acrossvarious IT resources and provide suggestions. As shown in the followingscreenshot, A WS T rusted Advisor crawls through all resources in the account and of fers cost-saving recommendations based on resource utilization: C o s t - s a v i n g s u g g e s t i o n s f r o m A W S T r u s t e d A d v i s o r In the preceding screenshot, T rusted Advisor has detected continuous utilization of the application server ( Elastic Cloud computing, EC2 ) and advises buying a reserve instance by paying 1 year  upfront  with a 40% cost saving. Further checks have identified an underutilized database (AmazonRDS) and suggest shutting it down to make a potential saving. The cloud can provide an excellent value proposition for cost-saving. T o begin with, you can create a hybrid cloud, where you establish connectivitybetween your on-premises data center and the cloud. Y ou can move development and test servers to the cloud to determine cost structure andpotential savings. Once you have set up cost governance in the cloud, movemore workload as per the cost-benefit analysis. However , you need to assess your workload and whether it can be moved to the cloud and define a strategy . Y ou learned about cloud migration in Chapter 5 , Cloud Migration and Hybrid Cloud Ar chitectur e Design . Increasingly , public cloud providers are of fering managed services, which eliminates any infrastructure maintenance cost and overheads for alert andmonitoring configurations. A managed service reduces the total costs ofownership by reducing cost as service adoption increases.  Summary Cost optimization is a continuous ef fort from application inception (proof- of-concept to its implementation and post-production). Y ou need to  continuously review architectures and cost-saving ef forts. In this chapter , you learned about design principles to optimize costs. Before making any purchase decision, you should consider the total cost ofownership for the entire life cycle of software or hardware. Planning abudget and keeping track of forecasts help you to stay on the costoptimization path. Always keep track of your expenditures and look forpossible opportunities for cost optimization without af fecting user experience or business value. Y ou learned about the various techniques for cost optimization, which include reducing architectural complexity by simplifying enterprisearchitecture and setting a standard that everyone can follow . W e recommended avoiding duplication by identifying idle and repeatedresources, and take consolidation ef fort to negotiate the cost of bulk purchase. Apply standardization across the or ganization to limit resource provision and develop a standard architecture. T racking data for your actual costs against budgets and forecasts can help you to take proactive action.Y ou learned about various reports and alerts that can help to control costs. Y ou also learned about cost optimization in the cloud, which can help you further to optimize value. Automation and agility are some major factors that increase resourceef ficiency , and DevOps can provide a great deal of automation. In the next chapter , you will learn about various DevOps components and DevOps strategies to ef ficiently deploy your workload in the most automated way . DevOps and Solution Ar chitectur e Framework In   traditional environments, the development team and the IT operations team work in silos. The development team gathers requirements frombusiness owners and develops the applications. System administrators aresolely responsible for operations and for meeting uptime requirements.These teams generally do not have any direct communications during thedevelopment life cycle and each team rarely understands the processes andrequirements of the other team. Each team has its own set of tools, processes, and approaches that areredundant, and sometimes, ef forts are conflicting. For example, the development and quality assurance ( QA ) teams can be testing the build on a specific patch of the operating system ( OS ). However , the operations team deploys the same build on a dif ferent OS version in the production environment, causing issues and delays in the delivery . DevOps is a methodology that promotes collaboration and coordinationbetween developers and operational teams to deliver products or servicescontinuously . This approach is constructive in or ganizations where the teams rely on multiple applications, tools, technologies, platforms,databases, devices, and so on in the process of developing or delivering aproduct or service. Although there are dif ferent approaches to the DevOps culture, all   are about  achieving  a common goal. DevOps is about delivering a product or service in the shortest amount of time by increasing operational ef ficiency through shared responsibilities. DevOps helps to make delivery withoutcompromising on quality , reliability , stability , resilience, or security . In this chapter , you will learn the various following components of DevOps: Introducing DevOps Understanding the benefits of DevOpsUnderstanding the components of DevOpsIntroducing  DevOps in security  Combining DevSecOps and continuous integration/continuous delivery  ( CI/CD ) Implementing a CD strategyImplementing continuous testing in the CI/CD pipelineUsing DevOps tools for CI/CDImplementing DevOps best practices  By the end of this chapter , you will learn about the importance of DevOps in application deployment, testing, and security . Y ou will learn DevOps best practices, and dif ferent tools and techniques to implement them. Intr oducing DevOps In a DevOps (short for development and operations ) approach, both the development team and the operations team work collaboratively during thebuild and deployment phases of the software development life cycle,sharing responsibilities, and providing continuous feedback. The softwarebuilds are tested frequently throughout the build phase on production-likeenvironments, which allows early detection of defects. Sometimes, you will find a software application development and itsoperations are handled by a single team, where engineers work across theentire application life cycle, from development and deployment tooperations. Such a team needs to develop a range of skills that are notlimited to a single function.  Application testing  and security teams may also work more closely with the operations and development teams, from theinception to the production launch of an application. Speed enables or ganizations to stay ahead in the competition and address customer requirements quickly . Good DevOps practices encourage software development engineers and operations professionals to work better together . This results in closer collaboration and communication, leading to a shortertime to market ( TTM ), reliable release, improved code quality , and better maintenance. Developers benefit from feedback provided by the operations teams andcreate strategies for testing and deployment. System administrators don'thave to implement   defective or untested software on production environments because they participate in the   build phase . As all stakeholders in the software development and delivery life cyclecollaborate, they can also evaluate the tools that they intend to use at eachstep of the process, to verify compatibility between the devices and also todetermine whether any tools can be shared across the teams. DevOps is a combination of culture and practices. It requires or ganizations to change their culture by breaking down the barriers between all teams inthe product development and delivery life cycle. DevOps is not justabout  development  and operations; rather , it involves the entire or ganization, including management, business/application owners, developers, QA engineers, release managers, the operations team, andsystem administrators. DevOps is gaining popularity as the preferredoperating culture, especially for or ganizations that deal with cloud or distributed computing. Understanding the benefits ofDevOps The goal of DevOps is a CD model that is repeatable, reliable, stable,resilient, and secure. These properties improve operational ef ficiency . T o achieve this goal, teams must collaborate and get involved in thedevelopment and delivery process. All technical team members should haveexperience with the processes and tools involved in the developmentpipeline. A mature DevOps process provides benefits, as shown in thefollowing diagram: B e n e f i t s o f D e v O p s These benefits of DevOps are detailed further here: Speed :   Releasing product features at a quick pace helps to accommodate changing business needs of your customers and expandyour market. A DevOps model helps an or ganization to achieve results faster .  Rapid delivery :  DevOps processes facilitate more ef ficiency by automating end-to-end pipelines, from code build to code deploy andproduction launch.  Rapid delivery helps you to innovate faster . Faster release of bug fixes and features helps you to gain a competitive edge. Reliability : DevOps processes provide all checks to ensure delivery quality and safe application updates at a rapid pace. DevOps practicessuch as CI and CD embed automation testing and security checks for apositive end-user experience.Scale : DevOps helps to scale your infrastructure and application on an on-demand basis by including automation everywhere. Impr oved collaboration : The DevOps model builds a culture of ownership whereby the team takes account of their actions. Theoperations and dev teams work together in a shared responsibilitymodel. Collaboration simplifies the process and increases ef ficiency . Security : In an agile environment, making frequent changes requires stringent security checks. The DevOps model automates security andcompliance best practices, monitors them, and takes corrective actionin an automated way . DevOps removes barriers between the developer and operations teams thatused to work in silos. The DevOps model optimizes the productivity of thedevelopment team and the reliability of system operations. As teams closelycollaborate, this helps to increase ef ficiencies and improve quality . T eams take full ownership of the services they deliver , often beyond the traditional scope of their roles, and develop thinking from a customer point of view tosolve any issue. Understanding the components ofDevOps DevOps tools and automation bring together development and systemoperations. The following are critical components of a DevOps practice:  CI/CDContinuous monitoring and improvement Infrastructur e as code ( IaC ) Configuration management ( CM ) A best practice across all the elements is   automation . Automating processes allows you to ef ficiently perform these operations in a fast, reliable, and repeatable fashion. Automation can involve scripts, templates,and other tools. In a thriving DevOps environment, infrastructure ismanaged as code. Automation enables DevOps teams to set up and tune testand production environments rapidly . Let's explore more details about each component. CI/CD In CI, developers commit code frequently to a code repository . The code is built frequently . Each build is tested using automated unit tests and integration tests. In CD, code changes are committed to the repository anddeveloped frequently . Builds are deployed to test environments and are tested using automated, and possibly manual, tests. Successful builds passtests and are deployed to staging or production environments. Thefollowing diagram illustrates the impact of CI versus CD in the softwaredevelopment life cycle: C I / C D As shown in the preceding diagram, CI refers to building and unit testingstages of the software development life cycle. Every update that iscommitted in the code repository creates an automated build and test. CD isan essential aspect of   CI which extends CI process further to deploy build in production. In CI/CD practices, several people work on the code. They allmust use the latest working build for their ef forts. Code repositories maintain dif ferent versions of the code and also make the code accessible to the team. Y ou check out the code from the repository , make your changes or write new code in your local copy , compile and test your code, and then frequently commit your code back to the repository . CI automates most of the software release process. It creates an automatedflow that builds, tests, and then stages the update. However , a developer must trigger the final deployment to a live production environment thatis   not automated . It expands upon CD by deploying all code changes to a testing environment and/or a production environment after the build stage.If CD is implemented correctly , developers will always have a tested and deployment-ready build. The concepts in the following diagram illustrate everything related to theautomation of an application, from code commits into a code repo, to thedeployment pipeline. It shows an end-to-end flow , from build to the production environment where, developer check-in the code changes incode repository which is pulled by CI server . CI server trigger the build to create deployment package with new application binaries andcorresponding dependencies. These new binaries are deployed in tar geted development or testing environment. Also binaries get check-in into artifactrepository for safe version controlled storage: C I / C D f o r D e v O p s In CI/CD, software development life cycle phases such as code, build,deploy , and test are automated using the DevOps pipeline.  The d eploy and provision phase needs to be automated using IaC scripts. Monitoring can beautomated with various monitoring tools. A robust CD pipeline also automates the provisioning of infrastructure fortesting and production environments and enables monitoring andmanagement of test and production environments.   CI/CD provides multiple benefits to the team, such as improving developer productivity by savingtime on building, testing, and deployment of code. It helps the dev team todetect and fix bugs quickly and launch feature updates faster in theproduction environment. CD does not mean that every change committed by the developer goes intoproduction. Instead, it means that every change is   r eady   to go into production. When the changes are staged and tested in the stageenvironment, a manual approval process initiates and gives a green signal todeploy to production. Thus, in CD, deploying to production becomes abusiness decision and is still automated with tools. Continuous monitoring andimpr ovement Continuous monitoring helps to understand application and infrastructureperformance impacts on the customer . By analyzing data and logs, you can learn how code changes impact users. Active monitoring is essential in theera of 24/7 services and constant updates to both applications andinfrastructure. Y ou can be more proactive about monitoring services by creating alerts and performing real-time analysis. Y ou can track various metrics to monitor and improve your DevOps practice. Examples of DevOps-related metrics are: Change volume : This is the number of user stories developed, the number of lines of new code, and the number of bugs fixed.Deployment fr equency : This indicates how often a team is deploying an application. This metric should generally remain stable or show anupward trend. Lead time fr om development to deployment : The time between the beginning of a development cycle to the end of deployment can beused to identify inef ficiencies in the intermediate steps of the release cycle.Per centage of failed deployments : The percentage of failed deployments, including the number of deployments that resulted inoutages, should be a   low number . This metric should be reviewed in conjunction with the change volume. Analyze potential points offailure if the change volume is low but the number of faileddeployments is high. A vailability : T rack how many releases caused failures that possibly resulted in violations of   Service-Level Agr eements   ( SLAs ). What is the average downtime for the application?Customer complain volume : The number of complain tickets filed by customers is an indicator of the quality of your application. Per centage change in user volume : The number of new users signing up to use your application and the resulting increase in traf fic can help you scale your infrastructure to match the workload. After you deploy builds to the production environment, it is essential tomonitor the performance of your application continuously . As we discussed about automating environment , so let explore more details on IaC. IaC Provisioning, managing, and even deprecating infrastructure is a costlyactivity in terms of human capital. Furthermore, repeated attempts to buildand modify environments manually can be fraught with errors. Whetherworking from prior experience or a well-documented runbook, the tendencyfor a human to make a mistake is a statistical probability . W e can automate the task of creating a complete environment. T ask automation can help to ef fortlessly complete repetitive tasks and provide significant value. W ith IaC, we can define our infrastructure in the form of   templates . A single template may consist of a part or the entirety of an environment. More importantly , this template can be used repeatedly to create the same identical environment time and again.  In IaC, infrastructure is spun up and managed using code and CI.An IaC model helps you to interact with infrastructure programmatically   at scale and avoid human errors by allowing you to automate resourceconfiguration. That way ,   you can work with infrastructure   in the same way that you would with code by using code-based tools. As the infrastructure ismanaged through code, so the application can be deployed using astandardized method, and any patches and versions can be updatedrepetitively without any errors.   Some of the most popular infrastructure as code scripting tools are Ansible, T erraform and A WS CloudFormation. Configuration management (CM) CM  is the process of using automation to standardize resource configurations across your entire infrastructure and applications. CM toolssuch as Chef, Puppet, and Ansible can help you manage IaC and automatemost system administration tasks, including provisioning, configuring, andmanaging IT resources. By automating and standardizing resourceconfigurations across the development, build, test, and deployment phases,you can ensure consistency and eliminate failures caused bymisconfiguration. CM can also increase the productivity of your operations by allowing youto automatically deploy the same configuration to hundreds of nodes at thepush of a button. CM can also be leveraged to deploy changes toconfigurations. Although you can use  registry settings or databases to store system  configuration settings, a configuration management application allows you to maintain version control as well, in addition to storage.   CM is also a way to track and audit configuration changes. If necessary , you can even maintain multiple versions of configuration settings for variousversions of your software. CM tools include a controller machine that manages server nodes. Forexample, Chef requires a client agent application to be installed on eachserver that it needs to manage, and a master Chef application installs on thecontroller machine. Puppet also works the same way with a centralizedserver . However , Ansible has a decentralized approach and doesn't require the installation of agent software on the server nodes. The following tableshows a high-level comparison between the popular configurationmanagement tools:   Ansible Puppet Chef Mechanism Controllermachine applieschanges toservers usingSecur e Shell ( SSH ) Mastersynchronizeschanges toPuppet node Chef workstationlooks for changesin Chef serversand pushes themto Chef node Ar chitectur e Any server canbe the controller Centralizedcontrol byPuppetmaster Centralizedcontrol by Chefserver ScriptLanguage Y AML Domain-specific onRuby  Ruby ScriptingT erminology  Playbook androles Manifestsand modules Recipes andcookbooks T est Execution Sequential order Non-sequentialorder Sequential order CM tools provide their own domain-specific language and set of featuresfor automation. Some of these tools have a learning curve whereby the teamhas to spend some time to learn the tool. As security is becoming a priority for any or ganization, so bringing complete automation security is the need of the hour . T o avoid human error , or ganizations are moving to tight security implementation and monitoring, using the DevOps process popularly known as   DevSecOps . Let's explore more about DevSecOps (short for  development, security and operations ) in the next section. Intr oducing DevSecOps W e are now more focused on security than ever . In many situations, security is the only way to win customer focus. DevSecOps is about the automationof security and implementation of security at scale. The development teamis always making changes and the DevOps team is publishing them inproduction (changes are often customer -facing). DevSecOps is required to ensure application security in the overall process. DevSecOps is not there to audit code or CI/CD artifacts. Or ganizations should implement DevSecOps to enable speed and agility , but not at the expense of validating security .  The power  of automation is to increase product-feature-launch agility while remaining secure by implementing therequired security measures. A DevSecOps approach results in built-insecurity and is not applied as an afterthought. DevOps is about addingef ficiency to speed up the product launch life cycle, while DevSecOps validates all building blocks without slowing the life cycle. T o institute a DevSecOps approach in your or ganization, start with a solid DevOps foundation across the development environment, as security iseveryone's responsibility . T o create collaboration between development and security teams, you should embed security in the architecture design frominception. T o avoid any security gaps, automate continuous security testing and build it into the CI/CD pipeline. T o keep track of any security breach, apply to extend monitoring to include security and compliance bymonitoring for drift from the design state in real time. Monitoring shouldenable alerting, automated remediation, and removing non-compliantresources. Codifying everything is a basic requirement that opens up infinitepossibilities. The goal of DevSecOps is to keep the pace of innovation,which should meet the pace of security automation. A scalableinfrastructure needs scalable security , so it requires automatic incident response remediation to implement continuous compliance and validation. Combining DevSecOps and CI/CD A DevSecOps practice needs to be embedded with every step of the CI/CDpipeline. DevSecOps ensures the security of the CI/CD pipeline bymanaging the right access and roles assigned to each server and makingsure the build servers such as Jenkins are hardened to be protected from anysecurity glitch. In addition to that, we need to make sure that all artifacts arevalidated, and code analysis is in place. It's better to be ready for incidentresponse by automating continuous compliance validation and incidentresponse remediation. The following screenshot provides us with multiple stages to test securityboundaries and catch security issues and compliance with policies as earlyas possible: D e v S e c O p s a n d C I / C D At each integration point, you can identify dif ferent issues, as illustrated in the preceding diagram: In the coding phase, scan all code to make sure no secret key or accesskey is hardcoded in between code lines. During the build, include all security artifacts such as the encryptionkey and access token management, and tag them to identify them. During the test, scan the configuration to make sure all securitystandards are met by test security . In the deploy and provision phase, make sure all security componentsare registered. Perform a checksum to make sure there are no changesin the build files.Monitor all security standards during the monitoring phase. Performcontinuous audit and validation in an automated way . DevSecOps CI/CD gives us confidence that code is validated against thecorporate security policy . It helps to avoid any infrastructure and application failure in later deployment due to dif ferent security configurations. DevSecOps maintains agility and ensures security at scalewithout af fecting DevOps' pace of innovation. Implementing a CD strategy CD provides seamless migration of the existing version to the new versionof the application. Some of the most popular techniques to achieve throughCD are: In-place deployment : Update application in a current server . Rolling deployment : Gradually roll out the new version in the existing fleet of servers.Blue-gr een deployment : Gradually replace the existing server with the new server . Red-black deployment : Instant cutover to the new server from the existing server . Immutable deployment : Stand up a new set of servers altogether . Let's explore each option in more detail. In-place deployment In-place deployment is a method of rolling out a new application version onan existing fleet of servers. The update is done in one deployment action,thereby requiring some degree of downtime. On the other side, there arehardly any infrastructure changes needed for this update. There is also noneed to update existing Domain Name System ( DNS ) records. The deployment process itself is relatively quick. If the deployment fails,redeployment is the only option for restoration. As a simple explanation,you are replacing the existing application version (v1) on the applicationinfrastructure with the new version (v2). In-place updates are low-cost andfast to deploy . Rolling deployment W ith a rolling deployment, the server fleet is divided into a group, so it doesn't need to be updated at the same time. The deployment process runsboth old and new versions of software on the same server fleet but withdif ferent subgroups. A rolling deployment approach helps to achieve zero downtime as, if a new version deployment fails, then only a subgroup ofservers is impacted from the entire fleet, and risk is minimal because half ofthe fleet will still be up and running. A rolling deployment helps to achievezero downtime; however , deployment time is little more than in-place deployment. Blue-gr een deployment The idea behind blue-green deployment is that your blue environment isyour existing production environment carrying live traf fic. In parallel, you provision a green environment, which is identical to the blue environmentother than the new version of your code. When it's time to deploy , you route production traf fic from the blue environment to the green environment. If you encounter any issues with the green environment, you can roll it backby reverting traf fic to the original blue environment. DNS cutover and swapping Auto Scaling groups are the two most common methods used tore-route traf fic in blue-green deployment. Using Auto Scaling policies, you can gradually replace existing instanceswith instances hosting the new version of your application as yourapplication scales out. This option is best used for minor releases and smallcode changes. Another option is to leverage DNS routing to performsophisticated load balancing between dif ferent versions of our application. As illustrated in the following diagram, after creating a productionenvironment that hosts the new version of our application, you can use theDNS route to shift a small portion of traf fic to  the  new  environment : B l u e - g r e e n d e p l o y m e n t D N S g r a d u a l c u t o v e r T est the green environment with a fraction of production traf fic; this is called   canary analysis . If the environment has functional issues, you'll be able to tell right away and switch traf fic back before impacting your users significantly . Continue to gradually shift traf fic, testing the ability of the green environment to handle the load. Monitor the green environment todetect issues, providing an opportunity to shift traf fic back, thus limiting the blast radius. Finally , when all the metrics are right, decommission the blue environment and release the resources. Blue-green deployment helps to achieve zero downtime and provides easyrollback. Y ou can customize the time to deploy as per your needs. Red-black deployment In red-black deployment, before standing up a new version of a system,first, perform canary testing. The canary replaces around 1% of its existingproduction system with the latest version of the application and monitorsthe newest version for errors. If the canary clears this initial test, the systemis deemed ready for deployment. In preparation for the switchover , a new version of the system stands up side by side with the old version of the system. The initial capacity of thenew system is set manually by examining how many instances are currentlyrunning in production and setting this number as the desired capacity for thenew Auto Scaling group. Once the new system is up and running, bothsystems are red. The current version is the only version accepting traf fic. Using the DNS service, the system is then cut over from the existingversion to the new version. At this point, the old version is regarded asblack; it is still running but is not receiving any traf fic. If any issues are detected with the new version, then reverting becomes as simple as pointingthe DNS server back to the load balancer hosting the old version. Red -black deployment is also known as   dark launch   and is slightly dif ferent from blue-green deployment. In red-black deployment, you do sudden DNS cutover from the old version to the new version, while in blue-green deployment, the DNS gradually increases traf fic to the new version. Blue-green deployments and dark launches can be combined to deploy bothversions of software side by side. T wo separate code paths are used, but only one is activated. A feature flag activates the other code path. Thisdeployment can be used as a beta test where you can explicitly enable thenew features. Immutable deployment An immutable or disposable upgrade is an easier option if your applicationhas unknown dependencies. An older application infrastructure that hasbeen patched and re-patched over time becomes more and more dif ficult to upgrade. This type of upgrade technique is more common in an immutableinfrastructure. During the new release, a new set of server instances are rolled out byterminating older instances. For disposable upgrades, you can set up acloned environment with deployment services such as Chef, Puppet,Ansible, and T erraform, or use them in combination with an Auto Scaling configuration to manage the updates. In addition to downtime, you need to consider the cost while designing yourdeployment strategy . Consider the number of instances you need to replace and your deployment frequency to determine the cost. Use the approachthat best fits, taking your budget and downtime into consideration. In this section, you learned about various CD strategies that help you tomake your application release more ef ficient and hassle-free. For high- quality delivery , you need to perform application testing at every step, which often requires significant ef fort. A DevOps pipeline can help you to automate the testing process and increase the quality and frequency offeature releases. Let's learn more about continuous testing in the CI/CDpipeline. Implementing continuous testing inthe CI/CD pipeline DevOps is key for the continually changing business scenarios based oncustomer feedback, demand for new features, or shifts in market trends. Arobust CI/CD pipeline ensures further features/feedback incorporated in lesstime, and customers get to use the new features faster . W ith frequent code check-ins, having a good testing strategy   baked   into your CI/CD pipeline ensures you close that feedback loop with quality . Continuous testing is essential in balancing the CI/CD pipeline. Whileadding software features at a rapid pace is a good thing, ensuring that thefeatures adhere to the right quality is achieved by continuous testing. Unit tests form the lar gest amount of your testing strategy . They typically run on the developer's machine and are the fastest and cheapest. A generalrule of thumb is to incorporate   70%   of your testing ef forts in unit testing. Bugs caught at this stage can be fixed relatively faster , with fewer complexities. Unit tests are often performed by the developer , and once code is ready , it is deployed for integration and system testing. These tests require their ownenvironments and sometimes separate testing teams, which makes the testingprocess costlier . Once the team ensures that all intended features are working as expected, the operations team needs to run performance and compliancetests. These tests need production-like environments and are  costlier . Also, User Acceptance T esting ( UA T ) needs a replica of production-like environments as well, causing more expense. As illustrated in the following diagram, in  the  development phase , developers perform unit tests to test code changes/new features. T esting is usually done on a developer's machine after coding is complete. It is alsorecommended to run static code analysis on the code changes and do code coverage, adherence to coding guidelines, and so on. Smaller unit tests withno dependencies run faster . Therefore, the developer can find out quickly if the test has failed: C o n t i n u o u s t e s t i n g i n C I / C D The   build   phase   is the first phase to test for integration between dif ferent components and individual components themselves. The build phase is alsoan excellent time to test if the code committed by a developer breaks anyexisting feature and to perform regression testing. A   staging envir onment   is a mirror of the production environment. An end- to-end system test is performed at this stage (UI, backend logic, and API aretested extensively). Performance testing tests the application performanceunder a particular workload. Performance tests include load tests and stresstests. UA T is also performed at this stage, in readiness for production deployment. Compliance testing is done to test for industry-specificregulatory compliance. In the   pr oduction phase , a strategy such as A/B testing or canary analysis is used to test the new application version. In A/B testing, the new applicationversion is deployed to a small percentage of production servers and testedfor user feedback. Gradually , depending on how well the users receive the new application, the deployment is increased to span all production servers. A/B testing Often, in software development, it isn't clear which implementation of afeature will be most successful in the real world. An entire computerscience discipline— Human/Computer Interaction ( HCI )—is devoted to answering this question. While UI experts have several guidelines to helpthem design suitable interfaces, often, the best choice of design can only bedetermined by giving it to users and seeing whether they can use the designto complete a given task. As shown in the following diagram, A/B testing is a testing methodology inwhich two or more dif ferent versions of features are given to dif ferent sets of users. Detailed metrics on the usage of each implementation aregathered, and UI engineers examine this data to determine whichimplementation should be adopted going forward: Split users by feature experiment using A/B testing It's easy to launch several dif ferent versions of your application, each containing dif ferent implementations of a new feature. DNS routing can be used to send the majority of traf fic to the current system, while also sending a fraction of the existing traf fic to the versions of the system running the new features. DNS round-robin resolution is supported by most DNSresolvers and is an ef fective way to spread incoming traf fic. Load and performance testing are another important factor . For Java-based applications, you can use JMeter to load-test a relational database byissuing Java Database Connectivity ( JDBC ) commands. For MongoDB, you can use Mongo-Perf, which can generate a reproducible load on thedatabase and record the response time. Y ou can then hit the components and services that use the database and also simultaneously test the database. One common way to measure the load on instances is through what iscalled   micr o-benchmarking . In micro-benchmarking, you measure the performance of a small sub-component of your system (or even a snippet ofcode) and then attempt to extrapolate general performance data from thistest result. In the case of testing a server , you may test a slice of the system on a new instance type and compare that measurement to the same slicemeasured on your currently running system, which is now using anotherserver type and configuration. Using DevOps tools for CI/CD T o build a CI/CD pipeline, a developer requires various tools. These include a code editor , a source repository , a build server , a deployment tool, and orchestrating an overall CI pipeline. Let's explore some popular technologychoices of developer tools for DevOps, both in the cloud and on-premises. Code editor DevOps is a hands-on coding role, where you often need to write a script toautomate the environment. Y ou can use the   ACE editor   or the   cloud-based A WS Cloud9 integrated development envir onment ( IDE ); you can use a web-based code editor on your local computer or install a code editor inyour local server that connects to the application environments—such asdev , test, and prod—to interact.   An environment is a place where you store your project's files and where you run the tools to develop your apps. Y ou can save these files locally on the instance or server , or clone a remote code repository into your environment. The A WS Cloud9 IDE is the cloud- native  IDE provided as a managed service. The Ace editor lets you write code quickly and easily . It's a web-based code editor but provides performance similar to popular desktop-based codeeditors such as Eclipse, V im, and V isual Studio Code ( VSCode ), and so on. It has standard IDE features such as live syntax andmatching  parentheses highlighting, auto-indentation and completion, toggling between tabs,  integration with  version control tools, and multiple cursor selections. It works with lar ge files, having hundreds of thousands of lines without typing lag. It has built-in support for all of the popular codinglanguages along with debugging tools, and you can also install your owntools. For a desktop-based IDE, VS Code and Eclipse are other popularcode editor options that DevOps engineers can choose. Sour ce code management There are multiple choices available for your source code repository . Y ou can set up, run, and manage your Git server , where you will be responsible for everything. Y ou can choose to use a hosting service such as GitHub or Bitbucket. If you are looking for a cloud solution, then   A WS CodeCommit   of fers a secure, highly scalable, and managed source control system where you can host private Git repositories. Y ou need to set up authentication and authorization for your code repository to provide access to authorize team members for code to read or write. Y ou can apply data encryption in transit and at rest. When you push into thecode repository ( git push ), it encrypts the data and then stores it. When you pull from the code repository ( git pull ), it decrypts the data and then sends the data back to the caller . The user must be an authenticated user with the proper access level to the code repository . Data can be encrypted in transit by transmitting through encrypted network connections using HTTPS orSSH protocols. CI server A CI  s erver   is also known as a   build server . W ith teams working on multiple branches, it gets complicated to mer ge back into the master . CI, in this scenario,   plays a key role. CI server hooks   provide a way to trigger the build based on the event when code is committed to the repository . Hooks, which are incorporated in almost every version control system, refer tocustom scripts that are triggered by specified necessary actions that alsooccur in a repository . Hooks can run either on the client side or on the server side. Pull requests are a common way for developers to notify and review eachother's work before it is mer ged into common code branches. A CI server provides a web interface to review changes before adding them to the finalproject. If there are any problems with the proposed changes,   source code can be sent back to the developer  to tweak as per the or ganization's coding requirements. As shown in the following diagram, server -side hooks in combination with the CI server are used to increase the velocity of integration: A u t o m a t i o n o f C I As illustrated in the preceding diagram, using post-receive , you can direct new branches to trigger tests on a CI server to verify that the new buildintegrates correctly and that all units function correctly . The developer is notified of test failures and then knows to mer ge their branch with the mainline only after the problems have been fixed. The developer can buildfrom their branch, test the changes there, and get feedback on how well theirchanges work before deciding whether to mer ge their branch into the mainline.  Running integration and unit tests significantly reduces resistance when thatbranch is mer ged into the mainline. Hooks can also be customized to test mer ges into the mainline and block any mer ges that don't pass. Integration is all accomplished best with a CI server . Jenkins is the most popular choice to build the CI server . However , you have to maintain security and patching of the server by yourself. For native cloudoptions and managed services,   you can use managed code-build services such as A WS CodeBuild, which  eliminates  the need for server administration and reduces costs significantly with a  pay-as-you-go model. The service scales as per your demand. Y our team is empowered to focus on pushing code and lets a service build all the artifacts. As illustrated in the following diagram, you can host the Jenkins cluster inthe A WS Elastic Compute Cloud ( EC2 ) server's fleet and auto-scale as per build load: A u t o S c a l i n g o f J e n k i n s C I s e r v e r s The   Jenkins Master   of fload builds to the slave   node instance in the case of overload. When the load goes down, the Jenkins Master automatically terminates slave instances. While a CI server helps you to build the right version of code from a sourcecode repository by collaborating across team members of the developmentteam, code deployment helps the team to get code ready for testing andrelease for end-user consumption. Let's learn about code deployment in moredetail. Code deployment Once your build is ready , you can use the Jenkins server for deployment or choose A WS CodeDeploy as a cloud-native managed service. Y ou can use other popular tools such as Chef or Puppet to create a deployment script. Theoptions for specifying a deployment configuration are: OneAtA T ime : Only a single instance in a deployment group at a time installs a new deployment. If a deployment on a given instance fails, thedeployment script will halt the deployment and return an error responsedetailing the number of successful versus the number of failedinstallations. HalfAtA T ime : Half of the instances in the deployment group install a new deployment. The deployment succeeds if half of the instancessuccessfully install the revision. HalfAtA T ime can again be a good option for production/test environments where half of the instances areupdated to a new revision, and the other half remain available inproduction at an older revision.  AllAtOnce : Each instance installs the latest revision available whenever it next polls the deployment service. This option is best usedfor development and test deployments as it has the potential to install anon-functioning deployment on every instance in a deployment group.Custom : Y ou can use this command to create a custom deployment configuration specifying a fixed number of healthy hosts that must existin a deployment group at any given time. This option is a more flexibleimplementation of the OneAtA T ime option. It allows for the possibility that a deployment may fail on one or two instances that have becomecorrupt or are improperly configured. The following diagram illustrates life cycle events during deployment: D e p l o y m e n t l i f e c y c l e e v e n t The deployment agent runs through a series of steps to execute adeployment. These steps are called life cycle events. In the precedingdiagram, steps highlighted in blue can be controlled by human intervention;however , steps highlighted in green are automated and controlled by a deployment agent. Here are more details about each step: ApplicationStop : In order to trigger a deployment, the first requirement is to stop the application server so that traf fic stops serving while files are copied. Examples of software application servers areT omcat, JBoss, or W ebSphere servers. DownloadBundle:   After stopping the application server , the deployment agent starts downloading a pre-built deployment bundlefrom an artifactory such as JFrog Artifactory . The artifactory stores the application binary , which can be deployed and tested for application before the new version launch. Befor eInstall : The deployment agent triggers pre-install steps such as creating a backup of the current version and any required configurationupdate via a script.Install : In this step, deployment agents start the installation—for example, running an Ant or Maven script for installation of a Javaapplication.AfterInstall : The deployment agent triggers this step after your application installation is completed. It may include updating post-installation configuration, such as local memory setting and logparameters.ApplicationStart : In this step, the agent starts the application and notifies the operations team of success or failure.V alidateService : The validation step fires after everything else is done and gives you a chance to do a sanity check on the app. I t includes steps such as performing automated sanity tests and integration tests to verifyif the new version of the application has installed properly . The agent also sends a notification to the team when testing is successful. Y ou have learned about various code deployment strategies and steps as an independent component. However , to set up an automated CI/CD pipeline, you need to stitch all DevOps steps together . Let's learn more about the code pipeline, which can help you to build an end-to-end CI/CD pipeline. Code pipeline The code pipeline is about orchestrating everything together to achievecontinuous deployment ( CD ). In CD, the entire software release process is fully automated, including build and deployment to the production release.Over some time with experiments, you can set up a mature CI/CD pipelinein which the path to the production launch is automated, thus enabling rapiddeployment of features and immediate customer feedback. Y ou can use cloud-native managed services such as A WS CodePipeline to orchestrate the overall code pipeline, or you can use the Jenkins server . The code pipeline enables you to add actions to stages in your CI/CDpipeline. Each action can be associated with a provider that executes theaction. The code pipeline action's categories and examples of providers areas follows: Sour ce : Y our application code needs to be stored in a central repository with version control called sour ce code r epositories . Some of the popular code repositories are A WS CodeCommit, Bitbucket, GitHub, Concurr ent V ersions System ( CVS ), Subversion  ( SVN ), and so on. Build : The build tool pulls code from the source code repository and creates an application binary package. Some of the popular build toolsare A WS CodeBuild, Jenkins, Solano CI, and so on. Once the build is completed, you can store binaries in an artifactory such as JFrog.Deploy : The deployment tool helps you to deploy application binaries in the server . Some popular deployment tools are A WS Elastic Beanstalk, A WS CodeDeploy , Chef, Puppet, Jenkins, and so on. T est : Automated testing tools help you to complete and perform post- deployment validation. Some popular test validating tools are Jenkins,BlazeMeter , Ghost Inspector , and so on. Invoke : Y ou can use an events-based script to invoke activities such as backup and alert. Any scripting language such as a shell script, PowerShell, and Python can be used to invoke various customizedactivities.Appr oval : Approval is an important step in CD. Y ou can either ask for manual approval by an automated email trigger or approval can beautomated from tools. In this section, you learned about various DevOps tools to managethe  Softwar e Development Life Cycle ( SDLC ) such as a code editor , a repository , and build, test, and deployment tools. As of now , you have learned about various DevOps techniques for each SDLC phase. Let's learnmore about applying best practices and anti-patterns. Implementing DevOps bestpractices While building a CI/CD pipeline, consider your need to create a project andadd team members to it. The project dashboard provides visibility to thecode flow through the deployment pipeline, monitoring the build, triggeringalerts, and tracking application activities. The following diagram illustrates awell-defined DevOps pipeline: C I / C D w o r k f l o w b e s t p r a c t i c e Consider the following points while designing the pipeline: Number of stages : Stages could be development, integration, system, user acceptance, and production. Some or ganizations also include dev , alpha, beta, and release stages.T ype of tests in each stage : Each stage can have multiple types of tests such as unit tests, integration tests, system tests, UA T s, smoke tests, load tests, smoke tests, and A/B tests at the production stage. The sequence of a test : T est cases can be run in parallel or need to be in sequence.Monitoring and r eporting : Monitor system defects and failures and send notifications as failures occur . Infrastructur e pr ovisioning : Methods to provision infrastructure for each stage.Rollback : Define the rollback strategy to fall back to the previous version if required. Having a system that requires manual intervention where it's avoidableslows down your process. So, automating your process using CD willaccelerate your process. Another common anti-pattern is keeping configuration values for a buildinside of the code itself, or even having developers use dif ferent tools in their build processes, which can lead to inconsistent builds betweendevelopers.   It takes lots of time and ef fort trying to troubleshoot why particular builds work in one environment and not in others. T o overcome this, it is better to store build configurations outside of code. Externalizingthese configurations to tools which keeps them consistent between builds,enables better automation, and allows your process to scale much morequickly . Not using a CD process can lead to last-minute, middle-of-the-night rushes to get a build to work. Design your CD process to   fail fast , to reduce the likelihood of any last-minute surprises. Nowadays, most applications are built as a web app and utilize a cloudplatform. T o apply architecture best practice at each step of application development, the twelve-factor methodology can be used, as recommendedby T he T welve-Factor App ( https://12factor.net/ ),  the twelve-factor methodology adopted by enterprises for end-to-end development anddelivery of web applications. This applies to all coding platforms regardlessof programming languages. Y ou have learned details about each factor throughout dif ferent chapters of the book. Summary In this chapter , you have learned about the key components of a strong DevOps practice, which includes CI, CD, and continuous monitoring andimprovement. The agility of CI/CD can be achieved only by applyingautomation everywhere. T o automate, you learned about IaC and configuration management. Y ou also looked at various automation tools such as Chef, Puppet, and   Ansible to automate configuration management. As security is the priority , you learned about DevSecOps, which is DevOps in Security . CD is one of the key aspects of DevOps. Y ou learned about various deployment strategies, including rolling, blue-green, and red-blackdeployment. T esting is another aspect of ensuring the quality of your product, and you learned about the concept of continuous testing inDevOps, and also, how A/B testing can help to improve the product bytaking direct feedback from a customer in the live environment. Y ou have learned about stages in a CI/CD pipeline. Y ou have learned about the tools and services that you can use and best practices that you canfollow for a robust CI/CD pipeline. Y ou have learned how individual services work and also discussed how to integrate services to build asophisticated solution. Until this point, you have learned about various aspects of solutionarchitecture as every or ganization has lots of data, and they put a great ef fort into getting an insight into their data. In the next chapter , you will learn about how to collect, process, and consume data to get a more in-depth insight. Data Engineering and MachineLearning In the era of the internet and digitization, data is being generatedeverywhere with high velocity and quantity . Getting insight from these huge amounts of data at a fast pace is challenging. W e need to innovate continuously to ingest, store, and process this data to derive businessoutcomes. W ith the conver gence of many cloud, mobile, and social technologies, advancements in many fields such as genomics and life sciences aregrowing at an ever -increasing rate. T remendous value is found in mining this data for more insight. A fundamental dif ference between streaming systems and batch systems is the requirement to process unlimited data.Modern stream processing systems need to produce continual results at lowlatency on data with high and variable input rates. The concept of big data  is more than just the collection and analysis of the data. The actual value for or ganizations in their data is when it can be used to answer questions and used to create competitive advantages for theor ganization. Not all big data solutions must end in visualization. Many solutions such as Machine Learning ( ML ) and other predictive analytics feed these answers programmatically into other software or applications,which extract the information and respond as designed. As with most things, getting faster results costs more, and big data is noexception. Some answers might not be needed immediately , and so the latency and throughput of the solution can be flexible enough to take hoursto be completed. Other responses, such as in the fields of predictiveanalytics or ML, may be needed as soon as the data is available. In this chapter , you will learn about the following topics to handle and manage your big data and ML needs: What is big data architecture?Designing for big dataData ingestionData storageData processing and analyticsData visualizationThe Internet of Things ( IoT ) What is ML?Data science and MLML model overfitting versus underfittingSupervised and unsupervised ML model By the end of this chapter , you will learn about how to design big data and analytics architecture. Y ou will learn dif ferent steps of the big data pipeline including data ingestion, storage, processing, and visualization. Y ou will learn about the basics of ML and model evaluation techniques. What is big data ar chitectur e? The sheer volume of collected data can cause problems. W ith the accumulation of more and more data, managing and moving the data alongwith its underlying big data infrastructure becomes increasingly dif ficult. The rise of cloud providers has facilitated the ability to move applications tothe data. Multiple sources of data result in increased volumes, velocity , and variety . The following are some common computer -generated data sources: Application server logs : Application logs and games Clickstr eam logs : From website clicks and browsing Sensor data : W eather , water , wind ener gy , and smart grids Images and videos : T raf fic and security cameras Computer -generated data can vary from semi-structured logs to unstructured binaries. This data source can produce pattern-matching or correlations indata that generate recommendations for social networking and onlinegaming in particular . Y ou can also use computer -generated data to track applications or service behavior such as blogs, reviews, emails, pictures, andbrand perceptions. Human-generated data includes email searches, natural language processing,sentiment analysis on products or companies, and product recommendations.Social graph analysis can produce product recommendations based on yourcircle of friends, jobs you may find interesting, or even reminders based onyour circle of friend's birthdays, anniversaries, and so on. In data architecture, the general flow of a significant data pipeline starts withdata and ends with insight. How you get from start to fi nish depends on a host of factors. The following diagram illustrates a data workflow pipelinethat needs to design for data a rchitecture: B i g d a t a p i p e l i n e f o r d a t a a r c h i t e c t u r e d e s i g n As shown in the preceding diagram, the standard workflow of the big datapipeline includes the following steps: 1 . Data is collected (ingested) by an appropriate tool. 2 . The data is stored in a persistent way . 3 . The data is processed or analyzed. The data processing/analysissolution takes the data from storage, performs operations, and thenstores the processed data again. 4 . The data is then used by other processing/analysis tools or by the sametool again to get further answers from the data. 5 . T o make answers useful to business users, they are visualized by using a business intelligence ( BI ) tool or fed into an ML algorithm to make future predictions. 6 . Once the appropriate answers have been presented to the user , this gives them insight into the data that they can then take and use to makefurther business decisions. The tools you deploy in your pipeline determine your time-to-answer , which is the latency between the time your data was created and when you can getinsight from it. The best way to architect data solutions while consideringlatency is to determine how to balance throughput with cost, because ahigher performance and subsequently reduced latency usually results in ahigher price. Designing big data pr ocessing pipelines One of the critical mistakes many big data architectures make is trying tohandle multiple stages of the data pipeline with one tool. A fleet of servershandling the end-to-end data pipeline, from data storage and transformationto visualization, may be the most straightforward architecture, but it is alsothe most vulnerable to breakdowns in the pipeline. Such tightly-coupled bigdata architecture typically does not provide the best possible balance ofthroughput and cost for your needs. It is recommended that big data architects decouple the pipeline. There areseveral advantages to decoupling storage and processing in multiple stagesin particular , including increased fault tolerance . For example, if something goes wrong in the second round of processing and the hardware dedicated tothat task fails, you won't have to start again from the beginning of thepipeline; your system can resume from the second storage stage. Decouplingyour storage from various processing tiers gives you the ability to read andwrite to multiple data stores. The following diagram illustrates various tools and processes to considerwhen designing a big data architecture pipeline: T o o l s a n d p r o c e s s e s f o r b i g d a t a a r c h i t e c t u r e d e s i g n The things you should consider when determining the right tools for your bigdata architectures include the following: The structures of your dataThe maximum acceptable latencyThe minimum acceptable throughputThe typical access patterns of your system's end users Y our data structure has an impact on both the tools you use to process it and where you store it. The ordering of your data and the size of each object you're storing and retrieving are also essential considerations. The time-to-answer is determined by how your solution weighs latency/throughput andcost. User access patterns are another important component to consider . Some jobs require the rapid joining of many related tables regularly , and other jobs require daily or less frequent use of stored data. Some jobs require acomparison of data from a wide range of data sources, and other jobs pulldata from only one unstructured table. Knowing how your end users willmost often use the data will help you determine the breadth and depth ofyour big data architecture. Let's dive deep into each process and the toolsinvolved in big data architecture. Data ingestion Data ingestion is the act of collecting data for transfer and storage. There arelots of places that data can come from. Predominantly , data ingestion falls into one of the categories from databases, streams, logs, and files. Amongthese, databases are the most predominant. These typically consist of yourmain upstream transactional systems that are the primary data storage foryour applications. They take on both relational and non-relational flavors,and there are several techniques for extracting data out of them. Streams are open-ended sequences of time-series data such as clickstreamdata from websites or IoT devices, usually published into an API that wehost. Logs get generated by applications, services, and operating systems. Adata lake is a great place to store all of those for centralized analysis. Filescome from self-hosted file systems or via third-party data feeds via FTP orAPIs. As shown in the following diagram, use the type of data yourenvironment collects and how it is collected to determine what kind ofingestion solution is ideal for your needs: T y p e o f d a t a i n g e s t i o n As shown, transactional data must be able to store and retrieve data quickly . End users need quick and straightforward access to the data, which makesapp and web servers the ideal ingestion methods. For the same reasons,databases such as NoSQL and Relational Database Management Systems ( RDBMS ) are usually the best solutions for these kinds of processes. Data transmitted through individual files is typically ingested fromconnected devices. A lar ge amount of file data does not require fast storage and retrieval compared to transactional data. For file data, often transfer isone-way , where data is produced by multiple resources and ingested into a single object or file storage for later use. Stream data such as clickstream logs should be ingested through anappropriate solution such as an Apache Kafka or Fluentd . Initially , these logs are stored in stream storage solutions such as Kafka, so they're availablefor real-time processing and analysis. Long-term storage of these logs is bestin a low-cost solution such as object storage. Streaming storage decouples your collection system (producers) from theprocessing system (consumers). It provides a persistent buf fer for your incoming data. The data can be processed and you can pump the data at arate dependent on your needs.T echnology choices for data ingestion Let's looks at some popular open source tools for data ingestion andtransfer: Apache DistCp : DistCp stands for distributed copy and is part of the Hadoop ecosystem. The DistCp tool is used to copy lar ge data within a cluster or between clusters. DistCp achieves the ef ficient and fast copying of data by utilizing parallel processing distribute capabilitywhich comes with MapReduce. It distributes directories and files intomap tasks to copy files partitions from source to tar get. DistCp also does error handling, recovery , and reporting across clusters. Apache S qoop : Sqoop is also part of the Hadoop ecosystem project and helps to transfer data between Hadoop and relational data storessuch as RDBMS. Sqoop allows you to import data from a structureddata store into HDFS and to export data from HDFS into a structureddata store. Sqoop uses plugin connectors to connect to relationaldatabases. Y ou can use the Sqoop extension API to build a new connector , or you can use one of the included connectors that support data exchange between Hadoop and common relational databasesystems.Apache Spark Str eaming : Spark Streaming helps to ingest  live data streams  with high throughput and in a fault-tolerant, scalable manner . Spark Streaming divides the incoming data streams into batches beforesending them to the Spark engine for processing. Spark Streaming usesDStreams, which are sequences of r esilient distributed datasets ( RDDs ). Apache Kafka : Kafka is one of the most popular open source streaming platforms that helps you publish and subscribe to a datastream. A Kafka cluster stores a recorded stream in a Kafka topic. Aproducer  can publish data in a Kafka topic and consumers can take the outputted data stream by subscribing to the Kafka topic.  Apache Flume : Flume is open source software and is mainly used to ingest a lar ge amount of log data. Apache Flume collects and aggregates data to Hadoop reliably and  in a distributed manner . Flume facilitates streaming data ingestion and allows analytics. Apache Flink : Flink is another open source platform for streaming data and batch data processing. Flink consists of a streaming dataflowengine that can process bounded and unbounded data stream. Thebounded data stream has a defined start and end, while an unboundeddata stream has a start but no end. Flink can perform batch processingas well on its streaming engine and supports batch optimizations. There are more open source projects available for streaming, such asApache Storm and Apache Samza, to provide a means of reliablyprocessing unbounded streams of data. Ingesting data to the cloud Public cloud providers such as A WS provide an array for big data services to store and process data on a lar ge scale. The following are some options to move your data to the A WS cloud and utilize the scalability of fered by the cloud provider: A WS Dir ect Connect : A WS Direct Connect provides up to 10 Gbps of private connectivity between the A WS cloud and your data center . A dedicated network connection reduces network latency andincreases  bandwidth throughput . It provides more reliable network speed compared to internet connections where data has to hop throughmultiple routers. Direct Connect creates a cross-connect between therouter managed either by you or a Direct Connect partner , depending on whether you are co-located in one of A WS Direct Connect locations, and the router in that location that A WS owns. The circuit itself provides both a public and a private V irtual Interface ( VIF ). Y ou can use the private VIF to directly access the resources running within your V irtual Private Cloud ( VPC ) on A WS, and the public VIF to access the public endpoints for A WS services such as Amazon Simple Storage Service ( S3 ). A WS Snowball : If you want to transfer a lar ge amount of data, such as hundreds of terabytes ( TB ) or petabytes ( PB ) to the cloud, it could take years over the internet. A WS Snowball provides a tamper -proof 80 TB storage appliance that can transfer a lar ge amount of data. It works like a lar ge hard disk that you can plug in to your on-premise data storage server , load all data, and ship it to A WS. A WS will place your data in a designated location in the cloud storage. A WS Snowball has other flavors, such as Snowball Edge, which comes with computepower along with 100 TB of storage and fulfills the use case ofhandling data in a remote location, such as on a cruise ship or an oilring. It is like a small data center where you can load data and performsome analytics using the built-in compute functionality . Data can be loaded to the cloud as soon as the appliance comes online. If you havepetabytes of data, then you can use Snowmobile, which is a physical 45-foot shipping container with which you can transfer 100 PB of datain one go from your data center to the A WS cloud.  Amazon Kinesis : Amazon Kinesis of fers three capabilities. The first is a place to store a raw data stream to perform any downstreamprocessing of the records that you desire. T o facilitate transferring these records into common analytic environments such as Amazon S3,ElasticSearch, Redshift, and Splunk, it of fers the Amazon Kinesis Firehose service. Firehose will automatically buf fer up all the records in the stream and flush out to the tar get as a single file or set of records based on either a time or data-size threshold that you can configure, orwhichever is reached first. If you wish to perform some window-basedanalytics on the records in the stream, use Kinesis Analytics. KinesisAnalytics allows you to flow streams together and perform SQLoperations over the records based on the time windows that youconfigure. The output can subsequently flow into further streams youcreate, so you can build an entire serverless streaming pipeline.A WS Data Migration Service (DMS) : A WS DMS makes it easy to securely migrate or replicate your databases and data warehouses toA WS. In DMS, you can create a data migration task, which will be connected to on-premise data via a source endpoint and uses A WS- provided storage such as RDS and Amazon S3 as the tar get endpoint. DMS supports full data dumps and ongoing change data captur e ( CDC ). DMS also supports homogeneous (MySQL-to-MySQL) and heterogeneous (MySQL-to-Amazon-Aurora) database migrations. A WS provides more tools such as  A WS Data Sync for continuous file transfer to A WS from on-premise and A WS T ransfer for SFTP to ingest data from the SFTP server securely . As you ingest the data, it needs to be put in suitable storage to fulfill the business needs. Let's learn more abouttechniques to choose the right storage and the available storage choices. Storing data One of the most common mistakes made when setting up storage for a bigdata environment is using one solution, frequently an RDBMS, to handle allof your data storage requirements. Y ou will have many tools available, but none of them are optimized for the task they need to complete. One singlesolution is not necessarily the best for all of your needs; the best solution foryour environment might be a combination of storage solutions that carefullybalance latency with the cost. An ideal storage solution uses the right tool forthe right job. Choosing a data store depends upon various factors: How structur ed is your data?  Does it adhere to a specific, well- formed schema, as is the case with Apache web logs (l ogs are generally not well structured and so not suitable for relational databases ), standardized data protocols, and contractual interfaces? Is it completelyarbitrary binary data, as in the cases of images, audio, video, and PDFdocuments? Or , is it semi-structured with a general structure but with potentially high variability across the records, as in the case of JSON orCSV?How quickly does new data need to be available for querying(temperatur e)? Is it a real-time scenario, where decisions are made as new records stream in, such as with campaign managers makingadjustments based on conversion rates or a website making productrecommendations based on user -behavior similarity? Is it a daily , weekly , or monthly batch scenario, such as in model training, financial statement preparation, or product performance reporting? Or is itsomewhere in between, such as with user engagement emails, where itdoesn't require real-time action, but you can have a buf fer of a few minutes or even a few hours between the user action and thetouchpoint?The size of the data ingest : Is the data ingested record by record as data comes in, such as with JSON payloads from REST APIs thatmeasure at just a few KBs at best? Is it a lar ge batch of records arriving all at once, such as with system integrations and third-party data feeds? Or is it somewhere in between, such as with a few micro-batches ofclickstream data aggregated together for more ef ficient processing? T otal volume of data and its gr owth rate : Are you in the realm of GBs and TBs, or do you intend to store PBs or even exabytes ( EB )? How much of this data is required for your specific analytics use cases?Do the majority of your queries only require a specific rolling windowof time? Or , do you need a mechanism to query the entirety of your historical dataset? What the cost will be to stor e and query the data in any particular location :   When it comes to any computing environment, we generally see a triangle of constraints  between performance, resiliency , and low cost. The better the performance and the higher the resilience that youwant your storage to have, the more expensive it will tend to be. Y ou may wish to have quick queries over petabytes of data, which is thekind of environment that Redshift can provide, but decide to settle onquerying TBs of data compressed in the Parquet format using Athena tomeet your cost requirements. Finally , what type of analytic queries will run against the data? W ill it be powering a dashboard with a fixed set of metrics and drill-down? W ill it participate in lar ge numerical aggregations rolled up by various business dimensions? Or , will it be used for diagnostics, leveraging string tokenization for full-text searching and pattern analysis? The followingdiagram combines multiple factors related to your data and the storagechoice associated with it: U n d e r s t a n d i n g d a t a s t o r a g e When you determine all characteristics of your data and understand the datastructure, you can then assess which  solution you need to use for your d ata storage. Let's learn about the various solutions for storing data. T echnology choices for data storage As we discussed, a single tool can't do everything. Y ou need to use the right tool for the right job, and the data lake enables you to build a highlyconfigurable big data architecture to meet your specific needs. Businessproblems are far too broad, deep, and complex for one tool to solveeverything, and this is especially true in the big data and analytics space. For example, hot data will need to be stored and processed in-memory , so caches or in-memory databases like Redis or SAP Hana are appropriate.A WS of fers the ElastiCache service, providing a managed Redis or Memcached environment. NoSQL databases are ideal when facing highvelocity but small-sized records, for example, user -session information or IoT data. NoSQL databases are also useful for content management to storedata catalogs. Structur ed data stor es Structured data stores have been around for decades and are the mostfamiliar technology choice when it comes to storing data. Most of thetransactional databases such as Oracle, MySQL, SQL Server , and PostgreSQL are row-based due to dealing with frequent data writes fromsoftware applications. Or ganizations often repurpose transactional database for reporting purposes, where frequent data reads are required, but muchfewer data writes. Looking at high data-read requirements, there is moreinnovation coming into an area of query on structured data stores, such asthe columnar file format, which helps to enhance data read performance foranalytics requirements.  Row-based formats store the data in rows in a file. Row-based writing is thefastest way to write the data to the disk but it is not necessarily the quickestread option because you have to skip over lots of irrelevant data. Column-based formats store all the column values together in the file. This leads tobetter compression because the same data types are now grouped together . It also typically provides better read performance because you can skipcolumns that are not required. Let's look at common choices for the structured data store. T ake an example where you need to query the total number of sales in a given month fromthe order table, which has fifty columns. In a row-based architecture, thequery will scan the entire table with all fifty columns, but in columnararchitecture, the query will just scan the order sales column, thus improvingdata query performance. Let's look into more details about relationaldatabases, focusing on transaction data and data warehousing to handle dataanalytics needs. Relational databases RDBMS is more suitable for Online T ransaction Pr ocessing  ( OL TP ) applications. Some popular relational databases are Oracle, MSSQL,MariaDB, PostgreSQL, and so on. Some of these traditional databases havebeen around for decades. Many applications, including e-commerce,banking, and hotel booking, are backed by relational databases. Relationaldatabases are very good at handling transaction data where  complex joint queries between tables are required.  Looking at transaction data needs, the relational database should adhere to the  Atomicity , Consistency , Isolation, Durability  ( ACID ) principles, as follows: Atomicit y : Atomicity means the transaction will executed fully from end to end, and, in the case of any error , the entire transaction will roll back.Consistency : Consistency means as soon as transactions are completed, all data should be committed to the database. Isolation : Isolation requires that multiple transactions can run concurrently in isolation without interfering with each other . Durability : In case of any interruption, such as a network or power failure, the transaction should able to resume the last known state. Often, data from the relational databases are of floaded to data warehousing solutions for reporting and aggregation purposes. Let's learn more aboutdata warehousing. Data war ehousing Data warehouse databases are more suitable for Online Analytical Pr ocessing ( OLAP )  applications. Data warehouses provide fast aggregation capability over vast volumes of structured data. While thesetechnologies, such as Amazon Redshift, Netezza, and T eradata, are designed to execute complex aggregate queries quickly , they are not optimized for high volumes of concurrent writes. So, data needs to beloaded in batches, preventing warehouses from being able to serve real-timeinsights over hot data. Modern data warehouses use a columnar base to enhance queryperformance. Examples of this include Amazon Redshift, Snowflake, andGoogle Big Query . These data warehouses provide very fast query performance due to columnar storage and improve I/O ef ficiency . In addition to that, data warehouse systems such as Amazon Redshift increasequery performance by parallelizing queries across multiple nodes and takeadvantage of massive parallel pr ocessing ( MPP ). Data warehouses are central repositories that store accumulations of datafrom one or multiple sources. They store current and historical data used tohelp create analytical reports for business data analytics. However , data warehouses store data centrally from multiple systems but they can not betreated as a data lake. Data warehouses handle only structured relationaldata while d ata lakes work with both structured relational data and unstructured data such as JSON, logs, and CSV data. Data warehouse solutions such as Amazon Redshift can process petabytesof data and provide decoupled compute and storage capabilities to savecosts. In addition to columnar storage, Redshift uses data encoding, datadistribution, and zone maps to increase query performance. More traditionalrow-based data warehousing solutions include Netezza, T eradata, and Greenplum. NoSQL databases NoSQL databases such as Dynamo DB, Cassandra, and Mongo DB addressthe scaling and performance challenges that you often experience with arelational database. As the name suggests,  NoSQL means a non-relational database.  NoSQL databases store data without an explicit and structured mechanism to link data from dif ferent tables (no joins, foreign keys, and with normalization enforced).  NoSQL utilizes a number of data models including columnar ,  key-value, search,  document, and graph. NoSQL databases provide scalable performance, high availability , and resilience. NoSQL typically does not enforce strict schema and e very item can have an arbitrary number of columns (attributes),  which means one row can have four columns, while another row can have ten columns in the same table. The partition key isused to retrieve values or documents containing related attributes.  NoSQL databases are highly distributed and can be replicated. NoSQL databases aredurable and don't experience performance issues when highly available. SQL versus NoSQL databases SQL databases have been around for decades and most of us are probablyalready very familiar with relational databases. Let's learn some significantdif ferences between SQL and NoSQL databases: Pr operties SQL Databases NoSQL Databases Data model In SQL databases, therelational model normalizesdata into tables containingrows and columns. A schemaincludes tables, number ofcolumns, relationshipsbetween tables, index, andother database elements. NoSQL databases donot enforce a schema.A partition key iscommonly used toretrieve values fromcolumn sets. It storessemi-structured datasuch as JSON, XML,or other documentssuch as data catalogsand file indexes. T ransaction SQL-based traditionalRDBMS support and arecompliant with thetransactional data properties ofACID . T o achieve horizontal scaling and datamodel flexibility , NoSQL databasesmay trade someACID properties oftraditional RDBMS. Performance SQL-based RDBMS were For NoSQL, used to optimize storage whenthe storage was expensive andto minimize the footprint ondisk. For traditional RDBMS,performance has mostly reliedon the disk. T o achieve performance queryoptimizations, index creationand modifications to the tablestructure are required. performance dependsupon the underlyinghardware cluster size,network latency , and how the application iscalling the database. Scale SQL-based RDBMS databasesare easiest to scale verticallywith high configurationhardware. The additional ef fort requires relational tables tospan across distributedsystems such as performingdata sharding. NoSQL databases aredesigned to scalehorizontally by usingdistributed clusters oflow-cost hardware toincrease throughputwithout impactinglatency .   Depending upon the of your data, there are various categories of NoSQLdata stores that exists to solve a specific problem. Let's understand the typesof NoSQL databases. T ypes of NoSQL data stor e Following are the major NoSQL databases types: Columnar databases : Apache Cassandra and Apache HBase are the popular columnar databases. The columnar data store helps you to scana particular column when querying the data, rather than scanning theentire row . If an item table has 10 columns with one million rows and you want to query the number of a given item available in inventory , then the columnar database will just apply the query to the itemquantity column rather than scanning the entire table. Document databases : Some of the most popular document databases are MongoDB, Couchbase, MarkLogic, Dynamo DB, and Cassandra.Y ou can use a document database  t o store semi-structured data in JSON and XML formats.Graph databases : P opular graph database choices include Amazon Neptune, JanusGraph, T inkerPop, Neo4j, OrientDB, GraphDB, and GraphX on Spark.  A graph database stores vertices and links between vertices called edges . Graphs can be built on both relational and non- relational databases. In-memory key-value stor es : S ome of the most popular in-memory key-value stores are Redis and Memcached. They store data inmemory for read-heavy applications. Any query from an applicationfirst goes to an in-memory database and, if the data is available in thecache, it doesn't hit the master database. The in-memory database isgood for storing user -session information, which results in complex queries and frequently requests data such as user profiles.  NoSQL has many use cases, but to build a data search you need to index allyour data. Let's learn more about search data stores. Sear ch data stor es The Elasticsearch service is one of the most popular search engines for bigdata use cases such as  clickstream and log analysis. Search engines work well for warm data that can be queried in an ad hoc fashion across anynumber of attributes, including string tokens. Elasticsearch is a very popularsearch engine. General binary or object storage is appropriate for files thatare unstructured, not indexable, and otherwise opaque without specializedtools that understand their format. Amazon Elasticsearch Service manages the administration of Elasticsearchclusters and provides API access. It also provides Kibana as a visualizationmechanism to conduct a search on indexed data stores inthe  Elasticsearch  cluster . A WS manages  capacity , scaling, and patching of the cluster , removing any operational overhead. Log search and analysis is a popular big data use case where  Elasticsearch  helps you to analyze log data from websites, servers fleet, IoT sensors, and so on.  Elasticsearch is  utilized by a lar ge variety of applications in industry such as banking, gaming, marketing, application monitoring, advertisement technology , fraud detection, recommendations, and IoT . Unstructur ed data stor es When you look at the requirements for an unstructured data store, it seemsthat Hadoop is a perfect choice because it is scalable, extensible, and veryflexible. It can run on consumer hardware, has a vast ecosystem of tools,and appears to be cost ef fective to run. Hadoop uses a master -and-child- node model, where data is distributed between multiple child nodes and the master node co-ordinates jobs for running queries on data. The Hadoopsystem is based on massively parallel pr ocessing ( MPP ), which makes it fast to perform queries on all types of data, whether it is structured orunstructured. When a Hadoop cluster is created, each child node created from the servercomes with a block of the attached disk storage called a local Hadoop Distributed File System  ( HDFS ) disk store. Y ou can run the query against stored data using common processing frameworks such as Hive, Ping, andSpark. However , data on the local disk persists only for the life of the associated instance.  If you use Hadoop's storage layer (that is, HDFS) to store your data, thenyou are coupling storage with compute. Increasing storage space meanshaving to add more machines, which increases your compute capacity aswell. For maximum flexibility and cost ef fectiveness, you need to separate compute and storage and scale them both independently . Overall object storage is more suited to data lakes to store all kinds of data in a cost-ef fective and performant manner . Cloud-based data lakes backed by object storage provide flexibility to decouple compute and storage. Let's learnmore about data lakes. Data lakes A data lake is a centralized repository for both structured and unstructureddata. The data lake is becoming a popular way to store and analyze lar ge volumes of data in a centralized repository .  It stores data as is, using open source file formats to enable direct analytics.  As data can be stored as is in its current format, you don't need to convert data into a predefined schema,which increases the speed of data ingestion. As illustrated in the followingdiagram, the data lake is a single source of truth for all data in youror ganization: O b j e c t s t o r e f o r d a t a l a k e The following are the benefits of a data lake: Data ingestion fr om various sour ces : Data lakes let you store and analyze data from various sources such as relational, non-relationaldatabase, and streams in one centralized location for a single source oftruth. This answers questions such as why is the data distributed in many locations?  and wher e is the single sour ce of truth? Collecting and efficiently storing data : A data lake can ingest any kind of data structure, including semi-structured and unstructured datawithout the need of any schema. This answers questions such as  how can I ingest data quickly fr om various sour ces and in various formats, and stor e it efficiently at scale? Scale up with the volume of generated data : Data lakes allow you to separate the storage layer and compute layer to scale each componentseparately . This answers questions such as  how can I scale up with the volume of data generated? Applying analytics to data fr om differ ent sour ces : W ith a data lake, you can determine schema on read and create a centralized datacatalog on data collected from various resources. This enables you toperform quick ad hoc analysis. This answers questions such as is ther e a way I can apply multiple analytics and pr ocessing frameworks to the same data? Y ou need an unlimited scalable data storage solution for your data lake. Decoupling your processing and storage provides a significant number ofbenefits, including the ability to process and analyze the same data with avariety of tools. Although this may require an additional step to load yourdata into the right tool, using Amazon S3 as your central data store provideseven more benefits over traditional storage options. The beauty of the Data Lake is that you are future-proofing yourarchitecture. T welve months from now , there may be new technology you want to use. W ith your data in the data lake, you can insert this new technology into your workflow with minimal overhead. By buildingmodular systems in your big data processing pipeline, with common objectstorage such as Amazon S3 as the backbone, you can replace specific modules when they become obsolete or when a better tool becomesavailable. One tool can not do everything. Y ou need to use the right tool for the right job, and data lakes enable you to build a highly configurable big dataarchitecture to meet your specific needs. Business problems are far toobroad, deep, and complex for one tool to solve everything, and this isespecially true in the big data and analytics space. Pr ocessing data and performing analytics Data analytics is the process of ingesting, transforming, and visualizing datato discover useful insights for business decision-making. Over the previousdecade, more data is collected and customers are looking for greater insightinto their data. These customers also wanted this insight in the least amountof time, and sometimes even in real time. They wanted more ad hoc queriesto answer more business questions. T o answer these questions, customers needed more powerful and ef ficient systems. Batch processing typically involves querying lar ge amounts of cold data. In batch processing, it may take hours to get answers to business questions. Forexample, you may use batch processing to generate a billing report at the endof the month. Stream processing in real time typically involves queryingsmall amounts of hot data and it takes only a short amount of time to getanswers. MapReduce-based systems such as Hadoop are examples ofplatforms that support the batch jobs category . Data warehouses are examples of platforms that support the query engine category . Streaming data processing activities ingest a sequence of data andincrementally update functions in response to each data record. T ypically , they ingest continuously produced streams of data records, such as meteringdata, monitoring data, audit logs, debugging logs, website clickstreams, andlocation-tracking events for devices, people, and physical goods. The following diagram illustrates a data lake pipeline for processing,transforming, and visualizing data using the A WS cloud tech stack: D a t a l a k e E T L p i p e l i n e f o r b i g d a t a p r o c e s s i n g Here, the Extract, T ransform, Load  ( ETL ) pipeline uses Amazon Athena for ad hoc querying of data stored in Amazon S3. The data ingested fromvarious data sources (for example, web application servers) generate logfiles that persist into S3. These files are then transformed and cleansed into aset form required for meaningful insights using Amazon ElasticMapReduce ( EMR ) and loaded into Amazon S3. These transformed files are loaded into Amazon Redshift using the COPY command and visualized using Amazon QuickSight. Using Amazon Athena,you can query the data directly from Amazon S3 when the data is stored andalso after transformation (that is, with aggregated datasets). Y ou can visualize the data in Amazon QuickSight. Y ou can easily query these files without changing your existing data flow . Let's looks at some popular tools for data processing. T echnology choices for data pr ocessing and analysis Following are some of the most popular data processing technologies,which help you to perform transformation and processing for a lar ge amount of data: Apache Hadoop  uses a distributed processing architecture in which a task is mapped to a cluster of commodity servers for processing. Eachpiece of work distributed to the cluster servers can be run or re-run onany of the servers. The cluster servers frequently use HDFS to storedata locally for processing. In the Hadoop framework, Hadoop takes abig job, splits it into discrete tasks, and process them in parallel. Itallows for massive scalability across an enormous number of Hadoopclusters. It's also designed for fault tolerance, where each of the workernodes periodically reports its status to a master node, and the masternode can redistribute work from a cluster that doesn't respondpositively . Some of the most popular frameworks used with Hadoop are Hive, Presto, Pig, and Spark.Apache Spark  is an in-memory processing framework. Apache Spark is a   massively parallel  processing  system with dif ferent executors that can take apart a Spark job and run tasks in parallel. T o increase the parallelism of a job, add nodes to the cluster . Spark supports batch, interactive, and streaming data sources. Spark uses  dir ected acyclic graphs  ( DAGs )  for all the stages during the execution of a job. The DAGs can keep track of the transformations of your data or lineage ofthe data during the jobs and ef ficiently minimizes the I/O by storing the DataFrames in memory . Spark is also partition-aware to avoid network-intensive shuf fles. Hadoop User Experience ( HUE ) enables you to run queries and scripts on your cluster through a browser -based user interface instead of the command line. HUE provides the most common Hadoopcomponents in a user interface. It enables browser -based viewing and tracking of Hadoop operations. Multiple users can access the cluster via Hue's login portal, and administrators can manage access manuallyor with LDAP , P AM, SPNEGO, OpenID, OAuth, and SAML2 authentication. HUE allows you to view logs in real time and providesa metastore manager to manipulate Hive metastore contents. Pig  is typically used to process lar ge amounts of raw data before storing it in a structured format (SQL tables). Pig is well suited to ETLoperations such as data validation, data loading, data transformation,and combining data from multiple sources in multiple formats. Inaddition to ETL, Pig also supports relational operations such as nesteddata, joins, and grouping. Pig scripts can use unstructured and semi-structured data (such as web server logs or clickstream logs) as input.In contrast, Hive always enforces a schema on input data. Pig Latinscripts contain instructions on how to filter , group, and join data, but Pig is not intended to be a query language. Hive is better suited toquerying data. The Pig script compiles and runs to transform the databased on the instructions in the Pig Latin script.Hive  is an open source data warehouse and query package that runs on top of a Hadoop cluster . SQL is a very common skill that helps the team to make an easy transition into the big data world. Hive uses aSQL-like language called Hive Query  language ( HQL ), which makes it easy to query and process data in a Hadoop system. Hive abstractsthe complexity of writing programs in a coding language such as Javato perform analytics jobs.Pr esto  is a Hive-like query engine but it is much faster . It supports the ANSI SQL standard, which is easy to learn and the most popular skillset. Presto supports complex queries, joins, and  aggregation  functions. Unlike Hive or MapReduce, Presto executes queries in memory , which reduces latency and improves query performance. Y ou need to be careful while selecting the server capacity for Presto, as it needs tohave high memory . A Presto job will restart in the event of memory spillover . HBase  is a NoSQL database developed as a part of the open source Hadoop project. HBase runs on HDFS to provide non-relationaldatabase capabilities for the Hadoop ecosystem. HBase helps to storelar ge quantities of data in columnar format with compression. Also, it provides a fast lookup because lar ge portions of the data cache are kept in memory while cluster instance storage is still used.Apache Zeppelin  is a web-based editor for data analytics built on top of the Hadoop system, also known as Zeppelin Notebook. It uses theconcept of an interpreter for its backend language and allows anylanguage to be plugged into Zeppelin. Apache Zeppelin includes somebasic charts and pivot charts. It's very flexible in terms of  any output from any language backend that can be recognized and visualized. Ganglia  is a Hadoop-cluster monitoring tool. However , you need to install Ganglia on the cluster during launch. The Ganglia UI runs onthe master node, which you can see using an SSH tunnel. The Gangliais an open source project designed to monitor clusters without impacton their performance. Ganglia can help to inspect the performance ofthe individual servers in your cluster and the performance of clustersas a whole. JupyterHub  is a multi-user Jupyter notebook. Jupyter notebooks are one of the most popular tools among data scientists to perform dataengineering and ML. The JupyterHub notebook server provides eachuser with their Jupyter notebook web-based IDE. Multiple users canuse their Jupyter notebooks simultaneously to write and execute codefor exploratory data analytics.Amazon Athena  is an interactive query service for running queries on Amazon S3 object storage using standard ANSI SQL syntaxes.Amazon Athena is built on top of Presto and extends ad hoc querycapabilities as a managed service. The Amazon Athena metadata storeworks like the Hive metadata store so that you can use the same DDLstatements from the Hive metadata store in Amazon Athena. Athena isa serverless and managed service, which means all infrastructure andsoftware handling and maintenance is taken care of by A WS and you can directly start running your query in the Athena web-based editor . Amazon Elastic MapReduce ( EMR )   is essentially Hadoop in the cloud. Y ou can utilize the Hadoop framework with the power of the A WS cloud using EMR. EMR supports all the most popular open source frameworks including Apache Spark, Hive, Pig, Presto, Impala,Hbase, and so on. EMR provides decoupled compute and storagewhich means you don't have to always keep running a lar ge Hadoop cluster; you can perform data transformation and load results intopersistent Amazon S3 storage and shut down the server . EMR provides autoscaling and saves you from the administrative overhead ofinstalling and updating servers with various software.A WS Glue  is a managed ETL   service, which helps in data processing, data cataloging, and ML transformations to find duplicate records.A WS Glue Data Catalog is compatible with the Hive data catalog and provides a centralized metadata repository across a variety of datasources including relational databases, NoSQL, and files. A WS Glue is built on top of a warm Spark cluster and provides ETL as a managedservice. A WS Glue generates code in PySpark and Scala for common use cases, so that you are not starting from scratch to write ETL code.Glue job-authoring functionality handles any error in the job andprovides logs to understand underlying permission or data formattingissues. Glue provides workflow , which helps you to build an automated data pipeline with simple drag and drop functionality . Data analysis and processing are huge topics that warrant a book on theirown. In this section, you had a very high-level overview of popular andcommon tools used for data processing. There are many more proprietaryand open source tools available. As a solution architect, you need to beaware of various tools available on the market to make the right choice foryour or ganization's use case. T o identify data insights, business analysts can create reports and dashboards and perform ad hoc queries and analysis. Let's learn about datavisualization in the next section. V isualizing data Data insights are used to answer important business questions such asrevenue by customer , profit by region, or advertising referrals by site, among many others. In the big data pipeline, enormous amounts of data arecollected from a variety of sources. However , it is dif ficult for companies to find information about inventory per region, profitability , and increases in fraudulent account expenses. Some of the data you continuously collect forcompliance purposes can also be leveraged for generating business. The two significant challenges of BI tools are the cost of implementationand the time it takes to implement a solution. Let's look at some technologychoices for data visualization. T echnology choices for data visualization Following are some of the most popular data visualization platforms, whichhelp you to prepare reports with data visualization as per your businessrequirements: Amazon QuickSight  is a cloud-based BI tool for enterprise-grade data visualizations. It comes with a variety of visualization graph presetssuch as a line graphs, pie charts, treemaps, heat maps, histograms, andso on. Amazon QuickSight has a data-caching engine known asSuper -fast, Parallel, In-memory Calculation Engine ( SPICE ), which helps to render visualizations quickly . Y ou can also perform data preparation tasks such as renaming and removing fields, changingdata types, and creating new calculated fields. QuickSight alsoprovides ML-based visualization insights and other ML-based featuressuch as auto forecast predictions. Kibana  is an open source data visualization tool used for stream data visualization and log exploration. Kibana of fers close integration with Elasticsearch and uses it as a default option to search for data on top ofthe  Elasticsearch service . Like other BI tools, Kibana also provides popular visualization charts such as histograms, pie charts, and heatmaps, and of fers built-in geospatial support. T ableau  is one of the most popular BI tools for data visualization. It uses a visual query engine, which is a purpose-built engine, used toanalyze big data faster than traditional queries. T ableau of fers a drag and drop interface and the ability to blend data from multipleresources.Spotfir e  uses in-memory processing for faster response times, which enables an analysis of extensive datasets from a variety of resources. Itprovides the ability to plot your data on a geographical map and shareit on T witter . W ith Spotfire recommendations, it inspects your data automatically and makes suggestions about how to best visualize it. Jaspersoft  enables self-service reporting and analysis. It also of fers drag and drop designer capability . PowerBI  is a popular BI tool provided by Microsoft. It provides self- service analytics with a variety of visualization choices. Data visualization is an essential and massive topic for solution architects.As a solution architect, you need to be aware of the various tools availableand make the right choice as per your business requirements for datavisualization. This section provided a very high-level overview of populardata visualization tools. As internet connectivity is increasing, there are small devices everywherewith small memory and compute capacities. These devices connect variousphysical entities and there is a need to collect and analyze data frommillions of these connected devices. For example, weather data collectedfrom various sensors can be utilized to forecast weather for wind ener gy and farming. Let's cover some more details about IoT , which is the growing global network of small interconnected devices. Understanding IoT IoT refers to a network ecosystem of physical devices that have an IPaddress and are connected to the internet. While IoT devices are growingrapidly in number , growing with them is the complexity of leveraging your IoT devices correctly . IoT helps or ganizations to perform a variety of tasks such as predictive maintenance, monitoring connected buildings and citysystems, ener gy ef ficiency monitoring, and safeguarding manufacturing facilities, among others. Y ou need to ingest data from IoT sensors, store them for analysis either via streaming data or stored data, and provide resultsquickly . These are some of the most critical challenges to any IoT devicearchitecture. Y ou need to ensure the security and management of devices. Cloud providers have managed service of ferings to achieve scalability to millions of devices. Let's look into A WS IoT of ferings to understand the working of IoT systems. Because IoT solutions can be complex andmultidimensional, you need to remove the complexity of implementing IoTin the business and help customers to securely connect any number ofdevices to the central server .  The A WS cloud helps in processing and acting upon device data and reading and setting device state at any time. A WS  provides the infrastructure to scale as needed, so or ganizations can gain insight into their IoT data, build IoT applications and services that better serve their customers, and help movetheir businesses toward full IoT exploitation. The f ollowing diagram illustrates the components of A WS IoT : A W S I o T The following are details in each IoT component and how are theyconnected as shown in the preceding diagram: IoT Gr eengrass : A WS IoT Greengrass is installed on edge devices and helps to send IoT message to the A WS cloud. The IoT Device SDK : The A WS IoT Device SDK helps to connect IoT devices to your application. The IoT Device SDK provides an API toconnect and authenticate devices to application. It helps to exchangemessages between device and A WS IoT cloud services using the   MQTT or HTTP  protocols . The IoT Device SDK supports C,  Arduino, and  JavaScript. Authentication and authorization : A WS IoT facilitates mutual authentication and encryption to exchange data with only authorizeddevices. A WS IoT uses authentication mechanisms such as SigV4 and X.509 certificates. Y ou can attach authentication to all connected devices by attaching a certificate and handle authorization remotely .  IoT message br oker : The message broker supports the MQTT and HTTP protocols and establishes secure communication between IoT devices and cloud services such as the A WS IoT Rules Engine, Device Shadows, and other A WS services. The IoT Rules Engine : The IoT Rules Engine helps to setup managed data pipelines for IoT data processing and analytics. The Rules Enginelooks at IoT data to perform streaming analytics and connect to otherA WS storage services such as Amazon S3, DynamoDB, ElasticSearch, and so on. Device Shadow Service : Device Shadow Service helps you to maintain a device's status when it is of fline due to loss of network connectivity in a remote area. As soon as the device comes online, it can resume itsstate from Device Shadow . Any application connected to the device can continue working by reading data from the shadow using a RESTfulAPI.Device Registry : The Device Registry helps to identify IoT devices and helps to manage millions of devices at scale. The Registry stores devicemetadata such as the version, manufacturer , and reading method (Fahrenheit versus Celsius), and so on. As of now , you've learned about various business analytics services that aim at answering questions about past events. But businesses also need to answerquestions about the possibilities of future events, which is done using ML.Let's learn more details  about  ML. What is ML? Let's say your company wants to send marketing of fers to potential customers for a new toy launch and you have been tasked to come up with asystem to identify whom to tar get for the marketing campaign. Y our customer base could be millions of users to which you need to applypredictive analytics, and ML can help you to solve such a complexproblem. ML is about using technology to discover trends and patterns and computemathematical predictive models based on past factual data. ML can help tosolve complex problems such as the following: When you may not know how to create complex code rules to make adecision. For example, if you want to recognize people's emotions inimage and speech, there are just no easy ways to code the logic toachieve that.When you need human expertise to analyze a lar ge amount of data for decision-making, but the volume of data is too lar ge for a human to do it ef ficiently . For example, with spam detection, while a human can do it, the amount of data volume makes it impractical to quickly do thismanually . When relevant information may only become available dynamically incases where you need to adapt and personalize user behaviors based onindividual data. Examples are individualized product recommendationsor website personalization.When there are a lot of tasks with a lot of data available but you cannottrack the information fast enough to make a rule-based decision—forexample, in fraud detection and natural language processing. Humans handle data prediction based on the results of their analyses andtheir experience. Using ML, you can train a computer to provide expertisebased on available data and get a prediction based on new data. The main idea behind ML is to make available a training dataset to anML   algorithm and have it predict something from a new dataset, for example, feeding some historical stock market trend data to an ML modeland having it predict how the market will fluctuate in the next 6 months to 1year . In previous sections, you learned about the various tools to process the data.ML is all about data—the better quality the data you can feed in fortraining, the better the prediction you get. Let's learn how data science goeshand in hand with ML in the next section. W orking with data science and ML ML is all about working with data. The quality of the training data and labelsis crucial to the success of an ML model. High-quality data leads to a moreaccurate ML model and the right prediction. Often in the real world, yourdata has multiple issues such as missing values, noise, bias, outliers, and soon. Part of data science is the cleaning and preparing of your data to get itready for ML. The first thing about data preparation is to understand business problems.Data scientists are often very eager to jump into the data directly , start coding, and start producing insights. However , without a clear understanding of the business problem, any insights you develop have a high chance ofbecoming a solution which is unable to address a problem. It makes muchmore sense to start with a clear user story and business objectives beforegetting lost in the data. After building a solid understanding of the businessproblem, you can begin to narrow down the ML problem categories anddetermine whether ML will be suitable to solve your particular businessproblem. Data science includes data collection, analysis, preprocessing, and featureengineering. Exploring the data provides us with necessary information suchas data quality and cleanliness, interesting patterns in the data, and likelypaths forward once you start modeling. As shown in the following diagram, data preprocessing and learning tocreate a ML model are interconnected—your data preparation will heavilyinfluence your model, while the model you choose heavily influences thetype of data preparation you will do. Finding the correct balance is highlyiterative and is very much an art (or trial and error): M L w o r k f l o w As shown in the preceding diagram, the ML workflow includes thefollowing phases: Pr epr ocessing : In this phase, the data scientist preprocesses the data and divides it into training, validation, and testing datasets. Y our ML model gets trained with the training dataset to fit the model, and isevaluated using the validation dataset. Once the model is ready , you can test it using a testing dataset. T aking into the amount of data and your business case, you need to divide the data into training, testing, andvalidation sets, perhaps keeping 70% of the data for training, 10% forvalidation, and 20% for testing. Features are independent attributes of your dataset that may or maynot influence the outcome. Feature engineering involves finding theright feature, which can help to achieve model accuracy . The label is your tar get outcome, which is dependent on feature selection. T o choose the right feature, you can apply dimensionality reduction,which filters and extracts the most ef fective feature for your data. Learning : In the learning phase, you select the appropriate ML algorithm as per the business use case and data. The learning phase isthe core of the ML workflow , where you train your ML model on your training dataset. T o achieve model accuracy , you need to experiment with various hyperparameters and perform model selection.Evaluation : Once your ML model gets trained in the learning phase, you want to evaluate the accuracy with a known dataset. Y ou use the validation dataset kept aside during the preprocessing phase to assessyour model. Required model tuning needs to be performed as per theevaluation result if your model prediction accuracy is not up to theexceptions as determined by validation data.Pr ediction : Prediction is also known as inference. In this phase, you deployed your model and started making a prediction. Thesepredictions can be made in real time or in batches. As per your data input, often, the ML model can have overfitting orunderfitting issues, which you must take into account to get the rightoutcome. Evaluating ML models – overfittingversus underfitting In overfitting, your model fails to generalize. Y ou will determine the overfitting model when it performs well on the training set but poorly on thetest set. This typically indicates that the model is too flexible for the amountof training data, and this flexibility allows it to memorize the data, including noise. Overfitting corresponds to high variance, where small changes in thetraining data result in big changes to the results. In underfitting, your model fails to capture essential patterns in the trainingdataset. T ypically , underfitting indicates the model is too simple or has too few explanatory variables. An underfit model is not flexible enough tomodel real patterns and corresponds to high bias, which indicates the resultsshow a systematic lack of fit in a certain region. The following graph illustrates the clear dif ference between overfitting versus underfitting as they correspond to a model with good fit: M L m o d e l o v e r f i t t i n g v e r s u s u n d e r f i t t i n g In the preceding graphs, the ML model is trying to categorize between twodata point categories illustrated by the red points and green crosses. The ML model is trying to determine whether a customer will buy a given product ornot. The graph shows predictions from three dif ferent ML models. Y ou can see an overfitted model (on the right) traversing through all red data pointsin training and failing to generalize the algorithm for real-world data outsideof the training dataset. On the other hand, the underfitted model (on the left)leaves out several data points and produces an inaccurate result. A goodmodel (shown in the middle) provides clear predictions of the data points inmost of the cases. Creating a good ML model is like creating art and you canfind the right fit with model tuning. The ML algorithm is at the heart of the overall ML workflow , which is categorized into supervised and unsupervised learning. Understanding supervised andunsupervised ML In supervised learning, the algorithm is given a set of training exampleswhere the data and tar get are known. It can then predict the tar get value for new datasets, containing the same attributes. For supervised algorithms,human intervention and validation are required, for example, in photoclassification and tagging. In unsupervised learning, the algorithm is provided with massive amountsof data, and it must find patterns and relationships between the data. It canthen draw inferences from datasets. In unsupervised learning, human intervention is not required, for example,auto-classification of documents based on context. It addresses the problem,where correct output is not available for training examples, and thealgorithm must find patterns in data using clustering. Reinforcement learning is another category where you don't tell thealgorithm what action is correct, but give it a reward or penalty after eachaction in a sequence, for example, learning how to play soccer . Following are the popular ML algorithm types used for supervised learning: Linear r egr ession : Let's use the price of houses as a simple example to explain linear regression. Say we have collected a bunch of datapoints that represent the prices of houses and the sizes of the houses onthe market, and we plot them on a two-dimensional graph. Now we tryto find a line that best fits these data points and use it to predict theprice of a house of a new size.Logistic r egr ession : Estimates the probability of the input belonging to one of two classes, positive and negative.Neural networks : In a neural network, the ML model acts like the human brain where layers of nodes are connected to each other . Each node is one multivariate linear function, with a univariate nonlineartransformation. The neural network can represent any non-linearfunction and address problems that are generally hard to interpret, suchas image recognition. Neural networks are expensive to train but fastto predict. K-near est neighbors : It chooses the number of k neighbors. Find the k- nearest neighbors of the new observation that you want to classify and assign the class label by majority vote. For example, you want tocategorize your data in five clusters, so your k value will be five.  Support V ector Machines ( SVM ): Support vectors are a popular approach in research, but not so much in industry . SVMs maximize the mar gin, the distance between the decision boundary (hyperplane), and the support vectors (the training examples closest to the boundary).SVM is not memory ef ficient because it stores the support vectors, which grow with the size of the training data.Decision tr ees : In a decision tree, nodes are split based on features to have the most significant Information Gain ( IG ) between the parent node and its split nodes. The decision tree is easy to interpret andflexible; not many feature transformations are required.Random for ests and ensemble methods : Random forest is an ensemble method where multiple models are trained and their resultscombined, usually via majority vote or averaging. Random forest is aset of decision trees. Each tree learns from a dif ferent randomly sampled subset. Randomly selected features are applied to each treefrom the original features sets. Random forest increases diversitythrough the random selection of the training dataset and a subset offeatures for each tree, and it reduces variance through averaging. K-means clustering uses unsupervised learning to find data patterns. K-means iteratively separates data into k clusters by minimizing the sum of distances to the center of the closest cluster . It first assigns each instance to the nearest center and then re-computes each center from assignedinstances. Users must determine or provide the k number of clusters. Zeppelin and Jupyter are the most common environments for data engineersdoing data discovery , cleansing, enrichment, labeling, and preparation for ML model training. Spark provides the Spark ML library , containing implementations of many common high-level estimator algorithms such asregressions, page rank, k-means, and more. For algorithms that leverage neural networks, data scientists useframeworks such as T ensorFlow and MxNet, or higher -level abstractions such as Keras, Gluon, or PyT orch. Those frameworks and common algorithms can found in the Amazon SageMaker service, which provides afull ML model development, training, and hosting environment. Data scientists leverage the managed Jupyter environment to do datapreparation, set up a model training cluster with a few configurationsettings, and start their training job. When complete, they can one-click-deploy the model and begin serving inferences over HTTP . ML model development is performed almost exclusively these days on files in HDFS,and thus querying the Amazon S3 Data Lake directly is the perfect fit forthese activities. Machine learning is a very vast topic and warrants a full book to understandin more detail. In this section, you just learned about an overview ofmachine learning models.  Summary In this chapter , you learned about big data architecture and components for big data pipeline design. Y ou learned about data ingestion and various technology choices available to collect batch and stream data forprocessing. As the cloud is taking a central place in storing the vastamounts of data being produced today , so you learned about the various services available to ingest data in the A WS cloud ecosystem. Data storage is one of the central points when it comes to handling big data.Y ou learned about various kinds of data stores, including structured and unstructured data, NoSQL, and data warehousing, with the relevanttechnology choices  associated with each . Y ou learned about data lake architecture and benefits. Once you collect and store data, you need to perform data transformation toget insight into that data and visualize your business requirements. Y ou learned about data processing architecture along with technology choices tochoose open source and cloud-based data processing tools as per your datarequirements. These tools help to get data insight and visualization as perthe nature of your data and or ganizational requirements. Now , there are millions of small devices connected to the internet, referred to collectively as the IoT . Y ou learned about the various components available in the cloud to collect, process, and analyze IoT data to producemeaningful insights. Y ou learned about the ML workflow , which includes data preprocessing, modeling, evaluation, and prediction. Also, you learnedabout supervised and unsupervised learning with an overview of variouspopular ML algorithms and available ML frameworks.  W ith time, or ganizations tend to accumulate the technology debt, and many legacy applications are sitting in the data center , creating costs and consuming resources. In the next chapter , you will learn about legacy application transformation and modernization. Y ou will learn about challenges with legacy systems and the techniques used to modernizethem. Ar chitecting Legacy Systems Legacy systems are the applications that have been deployed in your datacenter for decades without under going many changes. In a fast-changing technology environment, these systems get outdated and are challenging tomaintain. Legacy systems are not only defined by their age but sometimesby their inability to meet growing business needs, due to the underlyingarchitecture and technology . Often, lar ge enterprises deal with legacy applications to run crucial day-to- day business tasks. These legacy systems are spread across industries suchas healthcare, finance, transportation, and supply chain industries, and soon. Companies have to spend heavily on maintenance and support of thesesystems, which warrants the need to architect legacy systems.Rearchitecting and modernizing legacy applications helps or ganizations to be more agile and innovative, and also optimizes cost and performance. In this chapter , you will learn about challenges and issues with legacy applications, and techniques to rearchitect them. Rewriting complex legacyapplications may pose an additional risk of business disruption, so you willlearn about refactoring applications or considering the option to migrateinto a more flexible infrastructure. The following major topics will becovered in this chapter: Learning the challenges of legacy systemsDefining a strategy for system modernizationLooking at legacy system modernization techniquesDefining a cloud migration strategy for legacy systems By the end of the chapter , you will have learned about various challenges and modernization drivers for legacy systems. Y ou will learn various strategies and techniques for the  modernization of legacy systems. As the public cloud is becoming a go-to strategy for many or ganizations, you will learn about cloud migration of legacy systems.  Learning the challenges of legacysystems A legacy application puts significant challenges in front of an or ganization. On the one hand, there are critical applications that an or ganization has been using for decades, and, on the other hand, there are legacy applicationsholding back the or ganization's pace of innovation. Now , in a huge competitive environment, the end users are looking for the most modern, technologically advanced applications. All new featuresusually come with the latest software, and legacy applications limit yourability to add those features and provide benefits to end users. The followingdiagram shows some significant challenges that or ganizations are facing with legacy systems: C h a l l e n g e s w i t h a l e g a c y s y s t e m As illustrated, the following points are the significant challenges that youwill face with legacy systems: Dif ficulty in keeping up with user demand Higher cost of maintenance and update Shortage of skills and documentationV ulnerable to corporate security issues Incompatibility with other systems Before we dive into the solution, it's better to understand the issues clearly . Let's explore more about the challenges of a legacy system to understandthese better . Difficulty in keeping up with userdemand Customer focus is the key to business success, and being unable to keep upwith the latest technology trends can harm a business significantly . Y ou can take the example of Nokia mobile phones, which used to lead the worldmobile-phone market. As smartphones came into play nearly a decade ago,Nokia still stuck with a legacy system, and this resulted in near bankruptcy . It was a similar story with Kodak—one of the lar gest businesses in the photo-camera industry . Kodak was unable to move with digital innovation and adopt this into its systems, which resulted in Kodak becoming bankruptin 2012. There are many such examples where lar ge enterprises have been unable to survive due to a lack of legacy modernization and innovation. In the current climate of fast-changing technology and fierce competition,users are very demanding. Now , or ganizations have to change as per the user's terms, as they have multiple demands. As technology moves, the usermoves with it and starts using the most recent and popular app. Y our competitor can jump ahead if they are providing new features as per theuser demand. A legacy system also poses challenges for enterprise applications with aninternal user base. An old system that was built on mainframes mostly usesthe command line, which is not very user -friendly in the digital age, whereas new-generation workers demand a more user -friendly system to perform their routine tasks. However , you may face significant resistance from management, who may have been working with legacy systems fordecades and are settled with this. Higher cost of maintenance andupdate As legacy systems have been all set up and working for decades, they maylook less expensive. But over time, the total cost of ownership turns out tobe higher , as support and updates of older systems are usually more costly . Often, those updates are not available out of the box, and lots of manualworkarounds are required to maintain the system. Most legacy systems arenot very automation-friendly , resulting in more human ef fort. Legacy systems mostly have a lar ge chunk of proprietary software, which results in significantly higher license fees. In addition to that, older softwareno longer receives support from providers, and buying additional supportout of the life cycle could be very costly . On the other hand, modern systems mostly adopt open source technologies that drive the cost down.The operational outage from a legacy system can take more time to recoverfrom and drives up operational expenses. People with the skill set tomaintain legacy systems (such as DB2, COBOL, Fortran, Delphi, Perl, andso on) are hard to find, which can increase hiring costs and system risksignificantly . Legacy systems are pretty significant in the aspect of code. Unused codeadds another layer of unnecessary maintenance and complexity to a system.Legacy applications have been in operation for decades, and over time,many new changes will have been accommodated without code beingcleaned, which amounts to lots of technical debt. Any initiative to reducetechnical debt could be risky due to unknown impacts and dependencies. Asa result, or ganizations are forced to invest in unnecessary code and system maintenance due to fear of breaking the system by making any majorchanges. However , modernizing legacy systems may be costly due to unknown dependencies and outages. Careful cost-bene fit analysis ( CBA ) n eeds to be taken into consideration, along with determining the r eturn on investment ( ROI ) when deciding to proceed with modernization. As stakeholders don't see the immediate benefit of modernization, procuring finances for legacymodernization can be challenging.Shortage of skills anddocumentation Legacy technologies (such as mainframes) have multiple complexcomponents that depend on each other . These are extensive proprietary and costly servers that are not readily available if someone wants to developskills on their own. It is challenging to retain application-developmentresources, and even more challenging to hire people with hands-onexperience in old technology and operating systems. Often, legacy systems are two or more decades older , and most of the workforce with those skills has retired. Also, these systems don't have theproper documentation to keep up the record of years of work. There arechances of significant knowledge loss as an older workforce rotates with anewer workforce. A lack of knowledge makes it riskier to change thesystem due to unknown dependencies. Any small feature requests arechallenging to accommodate, due to system complexity and skill-setshortages. New cutting-edge technologies such as big data, machine learning, andInternet of Things ( IoT ) mostly build around new technology platforms. As new technologies are not well integrated with legacy systems, anor ganization may lose out to a competitor if not able to use the full capabilities of emer ging technologies. A modern system helps to build an or ganization brand as an innovative company where most of the new generation of the workforce wants to work, and they lose interest inworking with a legacy system. Development and training is an even moresignificant source of expense for legacy technologies. Often, automation helps to reduce costs by reducing human ef fort. There are many tools available around modern systems to build automation—suchas DevOps pipelines, code review , and automation testing—that a legacy system may not be able to utilize, resulting in additional cost. V ulnerable to corporate security issues Security is the top priority for any or ganization and system. A legacy application that runs on an old operating system (such as W indows XP or W indows 2008) is more vulnerable to security issues due to lack of vendor support. Software vendors continuously determine new security threats andrelease patches to accommodate them in the latest software version, tosecure them. Any legacy software that is announced as End of Life ( EOL ) from a vendor doesn't get a new security patch, which leaves yourapplication running in the old software version, exposed to a number ofsecurity threats. System health checks are often ignored for legacy applications, which makethem more vulnerable to be the tar get of security attacks. The skills gap makes it dif ficult to provide continuous support and help, which means systems are run in an in secure manner . A single vulnerability can pose a high risk of exposing your application, database, and critical information toattackers. In addition to a security vulnerability , legacy applications are hard to maintain due to compliance. As compliances keep changing with time toenforce more tight security around data handling and usage, legacy systemsrequire changes to adhere to local governance and compliance needs .  For example, the new European Union's General Data Pr otection Regulation  ( GDPR ) compliance requires each system to avail features where a user can request to delete their data. While modern systems canprovide these features out of the box in an automated and self-servicemanner , in legacy systems this may need to be performed manually and becomes more complex. Adhering to compliance needs can lead to moreoperation costs and time-consuming maintenance. Incompatibility with other systems In addition to end users, every system needs to integrate with other ITsystems. Those systems may be a part of dif ferent departments, clients, partners, or suppliers. The various systems need to exchange data in astandard format that evolves over time. Almost every few years, files anddata format standards get changed to increase data exchange ef ficiency , and most systems require changes to adopt these. Hard-to-change legacysystems that stick to using an old format could result in systemincompatibility and a system that your supplier and partner may not want touse. The inability to accommodate standard needs adds greater risk tobusinesses due to complex workarounds and lost productivity . Adding a workaround for simple business needs may make a system morecomplex. Modern systems are built on a service-oriented architecture,which makes it easier to accommodate any new requirement by adding anew service independently . Old systems are often built on a monolithic architecture, and adding any new feature means you need to rebuild and testthe entire system. Modern architectures are API-oriented and can be easily integrated withanother system to of fload heavy lifting. For example, a taxi-booking app uses Google Maps for Global Positioning System ( GPS ) navigation or Facebook and T witter for user authentication. A lack of APIs makes these integrations harder in a legacy system, resulting in complex custom code. As more load increases from another dependent upstream system, a legacyapplication can face a scalability issue . Often, legacy applications are built on a monolithic architecture and are hardware-dependent. S calability is a big challenge with a monolithic system, as it cannot scale horizontally dueto hardware dependency and v ertical scaling being limited to the maximum system capacity . Breaking monolithic applications into microservices can solve scaling challenges and help keep up with demand. In addition to software maintenance, legacy applications are also costly forhardware infrastructure as they run on a particular version. They spreadacross multiple databases with duplicate data and similar functionality . Due to their monolithic nature, it's hard to perform consolidation and use theflexibility of cloud-based infrastructure to save costs. Let's look at theapproach to the modernization of legacy systems. Defining a strategy for systemmodernization Often, a legacy system gets left out of an overall enterprise digital strategy , and issues get addressed on a need basis. T aking a reactive approach holds back or ganizations from executing overall system modernization and benefits.  If your legacy system has serious business challenges such as security andcompliance issues or is unable to address the business need, you can takea  big-bang approach. In the big-bang method, you can build a new system from scratch and shut down the old system. This approach has more risk butaddresses a business need that can be mitigated from the existing legacysystem. The other approach you can take is a  phased  approach, where you upgrade one module at a time and keep running both old and new systems. Aphased  approach has less risk but takes a long time and may be more expensive as you need to maintain both environments, with additionalnetwork and infrastructure bandwidth. T aking any of these approaches can provide various benefits once the modernization of an application is completed. Let's look at some criticalbenefits of system modernization. Benefits of system modernization Creating a future digital strategy by addressing the growing need for legacysystem modernization can have many advantages, as shown in thefollowing diagram: B e n e f i t s o f l e g a c y s y s t e m m o d e r n i z a t i o n The following are the significant benefits of an application modernization: Customer satisfaction : Using the latest technology gives a better user interface ( UI ) and omnichannel experience. Y ou don't need to build dif ferent variations of the UI; it can be built once and deployed across devices such as laptops, tablets, and smartphones. A fast and slick UIleads to better customer experience and business growth.Futur e-pr oof business strategy : Y our application modernization allows you to be more agile and innovative. The team canaccommodate the changing needs of the business comfortably andevolve with new technology . Stay ahead of the competition : Users always looking for the latest stuf f and tend to move into the new application, which gives a better experience. The modernization of your application helps you to stayahead of the competition by adding the latest trends. For example,voice integration is extensively being provided in apps, and you canenhance security with face detection. This is only possible when yourapplication is adopting the latest technology . Applicating r eliability and performance : Every new version of a software API and an operating system tries to address and improveperformance issues. Using the latest software and hardware helps youto achieve better performance, scalability , and high availability . Application modernization helps you to reduce operational outage andenhance security . Ability to use cutting-edge technology : Legacy systems prevent you from getting insight from data that could help you to grow yourbusiness significantly . By modernizing your database and creating a data lake, you can perform big data and machine learning to get allkinds of insight. This also helps you to retain employees when peopleget the opportunity to work in new technologies.Cost saving : Overall, any modernization leads to cost-saving by reducing operational maintenance and providing a more naturalupgrade. Utilization of open source software reduces licensing costs,hardware flexibility helps to adopt a cloud pay-as-you-go model, andautomation helps to reduce the human resources needed for routinejobs and improves overall ef ficiency . However , there are several benefits of legacy system modernization, but these could be very complex and require lots of ef fort. A careful assessment needs to be conducted to take the right approach. Let's explore more on theassessment techniques of a legacy application. Assessment of a legacy application There may be multiple legacy systems in an or ganization, with tens of thousands, to millions, of lines of code. In a modernization approach, alegacy system needs to align with the business strategy and the initial costof investment. Also, there is a possibility to reutilize some parts of it orcompletely write it from scratch, but the first step is to conduct theassessment and understand the overall system better . The following points are the primary areas that solution architects need to focus on whenconducting an assessment: T echnology assessment : As a solution architect, you need to understand the technology stack used by the existing system. If thecurrent technology in use is entirely outdated and lacks vendorsupport, then you might need to replace it entirely . In the case of a better version of the technology being available, then you can considerupgrading. Often, newer versions are backward-compatible, withminimal changes required.Ar chitectur e assessment : Y ou need to understand the overall architecture to make it future-proof. There may be a case where youdetermine a minor upgrade in the technology , but the overall architecture is monolithic and not scalable. Y ou should audit the architecture in the aspects of scalability , availability , performance, and security . Y ou may find significant architecture changes are required to align the application with business needs.Code and dependency assessment : Legacy systems often have 100,000 lines of code in a monolithic setting. V arious modules tied to each other make the system very complicated. The code appears not tobe in use in one module might impact some other modules if removedwithout much due diligence. These code lines may have been writtendecades back and have missed regular refactoring and review . Even if the technology and architecture look fine, you need to determine if thecode is upgradable and maintainable. W e also need to understand if any UI-related upgrade is required to make the user experience better . As a solution architect, you want to determine dependencies across variousmodules and code files. Modules may tightly couple, and you need todefine an approach to perform simultaneous upgrades when modernizingthe overall architecture.Defining the modernizationappr oach For stakeholders, there may be no immediate incentive for applicationmodernization. Y ou need to choose the most cost-ef fective method and deliver results faster . The following diagram shows the modernization approach: L e g a c y s y s t e m m o d e r n i z a t i o n a p p r o a c h After your system assessment, you need to understand the existingarchitecture pattern and limitations. As per your tech stack, you need toevaluate migration tools. For example, you may choose to use an emulatorfor mainframe migration or vCenter if rehosting your application toVMware. Y ou can select various modernization approaches and create a pr oof of concept  ( POC ) to identify any gaps. Some approaches are listed here: Ar chitectur e-driven modernization : Architecture-driven approach requires to achieve the most agility . Often, an architectural approach is language-independent and platform-independent by applying service-oriented patterns, which gives the development team the flexibility tobe more innovative. Y ou may want to choose this approach if you determine significant architectural changes from your assessment.Start implementing the most critical feature first, and build a POC todecide on gaps and ef fort required. T ake the microservices approach to achieve scalability and ensure better integration with other systems,depending on the system.System r e-engineering : In the re-engineering approach, the solution architect needs to understand the legacy system deeply and performreverse engineering to build a new modernized application. Y ou need to be sure to make technology choices that help you to create a future-proof system. Y ou may want to take this approach if the legacy system is over -complicated and requires long-term projects. Start with application modernization first and upgrade the database as a finalcutover in a phased approach. Y ou need to build a mechanism where both legacy and upgraded modules co-exist, with the ability tocommunicate in a hybrid manner . Migration and enhancements : Y ou can take migration and minor enhancement approaches if your existing system technology worksrelatively well but is restricted due to hardware limitation and cost. Forexample, you can lift and shift the entire workload to the cloud forbetter infrastructure availability and cost optimization. In addition tothat, a cloud provider extends several out-of-the-box tools, which helpsyou to make changes more frequently and apply better automation. Amigration approach helps you to modernize your application with lessef fort and makes it future-proof, where it stays relevant for the long term. However , lift and shift has its limitations, and may not be suited for all kinds of workload. Documentation and support For the long-term sustainability of a new system and graceful migration toit, make sure to prepare proper documentation and support. Document yourcoding standards, which everyone can follow and which helps to keep thenew system up to date. Keep your architecture documents as workingartifacts and keep them updated as technology trends change. Keeping yoursystem up to date will ensure that you don't fall into the legacy systemmodularization situation again. Prepare a comprehensive runbook to support new and old systems. Y ou may want to keep the old system for some time until the new system canaccommodate all business requirements and run satisfactorily . Update the support runbook, and make sure that you don't lose knowledge due toemployee attrition, and that the overall knowledge base is not processed in apeople-dependent manner . Keeping track of system dependencies helps you to determine the impact ofany changes in the future. Prepare training content to train staf f on the new system and make sure they can support it in case of an operational outage. Looking at legacy systemmodernization techniques As per your existing application analysis, you can take a dif ferent approach to upgrade your legacy system. The most straightforward approach will bemigration and rehosting, where you don't need to make many changes in theexisting system. However , a simple migration may not solve the long-term problem or provide a benefit. Y ou can take a more complex approach, such as rearchitecting or redesigning the entire application if the system is nolonger meeting the business need. The following diagram illustrates theef fort and impact of the various methods: L e g a c y s y s t e m m o d e r n i z a t i o n t e c h n i q u e s Let's look in more detail at the various modernization techniques shown inthe preceding diagram. Encapsulation, r ehosting, and r e- platforming Encapsulation is the simplest approach, and you may want to take this if the system is business-critical and needs to communicate with otherapplications running on the latest technology . In the encapsulation approach, you need to build an API wrapper around your legacy system,which will allow other business applications to communicate with a legacyapplication. An API wrapper is a common approach whereby you startmigrating your applications to the cloud but need to keep the legacyapplication in the on-premises data center for modernization in the laterphase. Y ou can choose the encapsulation option if your legacy code is well written and maintained, but, again, you will not be able to get the benefit oftechnology advancement and hardware flexibility . The r ehosting approach is also among the most straightforward approaches, whereby you want to migrate your application into another hardwareprovider such as the Cloud without any changes in code. Again, as withencapsulation, the rehosting option can save some cost due to vendorcontracts but may not give you benefits from technology advancement andhardware flexibility . An or ganization often takes this approach when it needs to move out of an existing contract quickly . For example, you can take the first step to the cloud in phase one and go for applicationmodernization in phase two. The r e-platforming approach may get a bit more complex than the rehosting approach and provides the immediate benefit of a new operatingsystem. Or ganizations often choose this approach if the server is going End of Life ( EOL ), where no support is available and an upgrade becomes necessary to handle security issues. For example, if W indows Server 2008 is going EOL, you may want to upgrade the operating system to theW indows 2012 or 2016 version. Y ou need to rebuild your binaries with the new operating system and perform testing to make sure everything works properly , but there are no significant changes in the code as such. Again, as with rehosting, re-platforming may not give you benefits from technologyadvancement. However , it will allow you to have continuous support from the vendor . While the preceding three approaches are the simplest ones, they are notable to provide the full benefit of the application upgrade. Let's look atapproaches that help you to take full advantage of applicationmodernization. Refactoring and r ear chitecting In the r efactoring approach, you can refactor your code to accommodate the new system. In refactoring, the overall architecture will be the same, yetyou are upgrading your code to make it more suited for the latest version ofthe programming language and operating system. Y ou can refactor the portion of code to apply automation and perform feature enhancement. Y ou may want to take this approach if your technology is still relevant and ableto accommodate business needs with code changes. In the r ear chitecting approach, you decide to change the system architecture by reutilizing the existing code as much as possible. Forexample, you want to create a microservices architecture out of yourexisting monolithic architecture. Y ou can take one module at a time and convert it into a service-oriented architecture by giving each module aRESTful endpoint. The rearchitecting option helps you to achieve thedesired scalability and reliability; however , overall performance results may be average due to the reutilization of existing code. Redesigning and r eplacing The r edesigning approach is most complex but provides maximum benefit . Y ou can choose this approach if the legacy system is completely outdated and is not able to accommodate business needs at all. W ith redesigning, you need to build the entire system from scratch while keeping the overall scopeintact. The following diagram shows the legacy mainframe system migrationto A WS Cloud: L e g a c y m a i n f r a m e s y s t e m m o d e r n i z a t i o n t o t h e c l o u d Here, a legacy mainframe system is rearchitected and refactored to similarcloud services as a modernization appr oach . Building a cloud-native application helps you utilize and fully benefit from cloud services in aspectsof scalability , performance, reliability , and cost. It helps your team to be more agile and innovative by accommodating rapidly changing technologyin your system. Redesigning a legacy system requires a long-term project with lots of ef fort and increased cost. Before kicking of f a lar ge modernization ef fort, as a solution architect, you should do careful analysis if any Softwar e-as-a- Service ( SaaS ) product or  commer cially available off-the-shelf ( COTS ) products are able to handle your business need with relatively lower cost. It is essential to do a CBA for r edesign versus pur chase before proceeding with the redesigning option. Sometimes, it's more beneficial to replace the existing legacy system withnew third-party software. For example, your or ganization may have a decade-old Customer Relationship Management ( CRM ) system that is not able to scale and provide the desired feature. Y ou may look for the option to subscribe to SaaS products such as Salesforce CRM to replace the legacysystem. SaaS products are subscription-based and of fer per -user licenses, so they may be the right choice if you have a smaller number of users. For avast enterprise with thousands of users, it may be more cost-ef fective to build its application. Y ou should conduct a CBA to understand ROI when investing in SaaS products. Defining a cloud migration strategyfor legacy systems As the cloud is becoming ever -more popular , more or ganizations are looking to migrate into the cloud for their legacy application modernization needs.Y ou learned about various cloud migration techniques in Chapter 5 , Cloud Migration and Hybrid Cloud Ar chitectur e Design . Cloud provides you with the flexibility to scale your application while keeping costs low and helps toachieve desirable performance, high availability , and reliability while maintaining application security . Cloud providers such as Amazon W eb Services ( A WS ) provide many options out of the box, which can help you to modernize your system. Forexample, you can take a serverless approach to build a microservice usingthe A WS Lambda function and Amazon API Gateway , using Amazon DynamoDB as a backend. W e discussed various legacy system modernization techniques in the previous section, as well as all of theapplications in the context of moving to the cloud. The flow illustrated in thefollowing diagram will help you decide whether to use cloud migration tomodernize your legacy application: C l o u d m i g r a t i o n p a t h f o r l e g a c y s y s t e m m o d e r n i z a t i o n As shown in the preceding diagram, if your application is still heavily usedby businesses and is generating revenue, you may want to continue withminimal changes. In that situation, you can refactor your application into thecloud or you can re-platform it into the cloud if the server is going EOL. In case you don't want to make any changes in existing applications tosustain business, and still want to move to the cloud entirely to save andoptimize costs, then take the  lift and shift appr oach to  rehost the legacy application in the cloud. If your legacy application is replaceable, then youcan buy a cloud-native SaaS version of the product and retire your legacyapplication. Sometimes, you may want to retain your legacy system in theon-premises data center if there are too many business dependencies and itcannot move into the cloud due to incompatibility .  Y ou should perform a  total cost of ownership ( TCO ) analysis to understand the advantages of moving to the cloud. It is recommended to take the mostcomplex module of the legacy application and build a POC to make sureyour entire system will be cloud-compatible before starting the full project.A detailed POC that covers the critical business cases will help you toidentify any gaps and reduce migration risk significantly . Summary In this chapter , you learned about various challenges with legacy applications and why it is essential to modernize your legacy application.Y ou learned about dif ferent benefits an or ganization can get by upgrading their application into the latest technology . Application modernization can be a complicated and risky task but is often worth the ef fort. The outcome you get from the upgrade is a trade-of f for the amount of investment and ener gy you put into it. Before defining the modernization approach, it's essential to understand your legacy system thoroughly . Y ou learned various assessment attributes of an application in the aspects oftechnology , architecture, and code.  After the assessment, the next step is to define the modernization approach.Y ou learned about various modernization approaches, including architecture-driven, system re-engineering, and migration approaches. Y ou also learned about multiple techniques of system modernization, whichincluded the simplest approaches (encapsulation and rehosting) andcomplex approaches (rearchitecting and redesigning). Cloud can provide asignificant value proposition, and you learned about the decision-makingapproach you need to take for modernization on the cloud. Y ou focused on the various technical aspects of solution architecture; however , documentation is one of the critical elements of architecture design to keep your system maintainable in the long run. In the nextchapter , you will learn about   various documentation required for a solution architect to prepare, contribute, and maintain to maximize busine ss value. Solution Ar chitectur e Document In previous chapters, you learned about various aspects of solutionarchitecture design and optimization. As the solution architect works on thedesign, it is important to have consistent communication for successfulapplication delivery . The solution architect needs to communicate a solution design to all technical and non-technical stakeholders. The Solution Ar chitectur e Document ( SAD ) provides an end-to-end view of application and helps everyone to be on the same page. In this chapter , you will learn about various aspects of the SAD, which addresses the needfor all stakeholders associated with the development of the application. Y ou will learn about the structure of the SAD and other types of documents of which the solution architect needs to be aware, such as the r equest for pr oposal ( RFP ), where the solution architect needs to provide input to make strategic decisions. Y ou will learn the following topics to gain a deeper understanding of the documentation involved in solutionarchitecture: Purpose of the SADV iews of the SAD Structure of the SADIT procurement documentation for a solution architecture By the end of this chapter , you will learn about the SAD, its structure, and the various details that need to be accommodated in the documentation. Y ou will learn about various IT procurement documentation such as the RFP , the r equest for information ( RFI ), and the r equest for quotation ( RFQ ), in which a solution architect participates to provide feedback. Purpose of the SAD The need for architecture documentation often gets ignored, and teams startworking on implementation without understanding the overall architecture.A SAD provides a broad view of the overall solution design to keep allstakeholders informed. The SAD helps to achieve the following purposes: Communicate the end-to-end application solution to all stakeholders.Provide high-level architecture and dif ferent views of the application design to address the application's service-quality requirements such asreliability , security , performance, and scalability . Provide traceability of the solution back to business requirements andlook at how the application is going to meet all functional and non- functional r equir ements ( NFRs ). Provide all views of the solution required for design, build, testing, andimplementation.Define the impacts of the solution for estimation, planning, anddelivery purposes.Define the business process, continuation, and operations needed for asolution to work uninterrupted after the production launch. SADs not only define the purpose and goal of the solution but also addresscritical components such as solution constraints, assumptions, and risks thatoften get overlooked by the implementation team. The solution architectneeds to make sure they create the document in an easy language thatbusiness users can understand and relate business context with technicaldesign. Documentation helps to retain knowledge due to resource attritionand makes the overall design process a people-independent one. For existing applications where modernization ef fort is needed, a SAD presents an abstract view of current  and future architecture,  along with a transition plan. The solution architect understands the existing system dependencies and documents them to uncover any potential risk in advance.The migration plan helps businesses to understand the tools and technologyrequired to handle the new system and plan resources accordingly . During solution design, the solution architect conducts various assessmentsby building a pr oof of concept ( POC ) or through market research. A SAD should list all architecture assessments and their impact, along withthe choice of technology . A SAD presents a conceptual view of the current and tar get state of the solution design and maintains a record of change. Let's understand various aspects of a SAD, in the next section.  V iews of the SAD The solution architect needs to create a SAD in such a way that it should beunderstandable by both business users and technical users. A SAD bridgesthe communication gap between the business user and the developmentteam to understand the function of the overall application. The best way tocapture all stakeholders' input is by putting yourself in their situation andlooking at problems from the stakeholders' perspective. The solutionarchitect evaluates both the business and technical aspects of architecturedesign so that they can take cognizance of all technical and non-technicalusers' requirements. As illustrated in the following diagram, the holistic view of the SADcomprises of various views derived from business requirements, to coverdif ferent aspects: S A D v i e w s Solution architects can choose standard diagrams such as a  Unified Modeling Language  ( UML ) diagram or a block diagram from Micr osoft V isio to represent various views. Overall, the diagram should be easy to read and understandable by all business and technical stakeholders. A SADshould include the following views, wherever possible, to addresseveryone's needs: Business V iew : Architecture design is all about addressing business concerns and solving business purposes. The Business V iew shows the value proposition of the overall solution and product. T o simplify , the solution architect may choose to detect high-level scenarios related tobusiness and present these as a use-case diagram. The Business V iew also describes stakeholders and the required resources to execute theproject. Y ou can define the Business V iew as a use-case view as well. Logical V iew : This presents various packages on the system so that business users and designers can understand the various logicalcomponents of the system. The logical view of fers a chronicled order of the system in which it should build. It shows how the multiplepackages of the system are connected and how the user can interactwith them. For example, in a banking application, the user first needsto authenticate and authorize using a security package, and then log into the account using the account package, or apply for a loan using aloan package, and so on. Here, each package represents a dif ferent module and can be built as a microservice .  Pr ocess V iew : This presents more details, showing how the key processes of the system work together . It can be reflected using a state diagram. The solution architect can create a sequence diagram if youwant to show more details. In a banking application, a process viewcan present the approval of a loan or account.Deployment V iew : This presents how the application is going to work in the production environment. It shows how dif ferent components of the system (such as network firewall, load balancer , application servers, database, and so on) are connected. The solution architectshould create a simple block diagram that business users canunderstand. Y ou can add more details to the UML deployment diagram to show various node components and their dependencies for technicalusers, such as the development and DevOps teams. The deploymentview represents the physical layout of the system.Implementation V iew : This is the core of the SAD, and represents architectural and technology choices. The solution architect needs toput the architecture diagram here—for example, if it is 3-tier , N-tier , or event-driven architecture, along with the reasoning behind it. Y ou also need to detail technology choices—for example, using Java versusNode.js, along with the pros and cons of using them. Y ou want to justify the resources and skills required to execute the project in theimplementation view . The development team uses an implementation view to create a detailed design such as a class diagram, but thatdoesn't need to be part of the SAD. Data V iew : As most applications are data-driven, this makes the data view important. The data view represents how data is going to flowbetween the dif ferent components and how it will be stored. It can also be used to explain data security and data integrity . The solution architect can use the entity-r elationship ( ER ) diagram to show the relationship between dif ferent tables and schemas in the database. The data view also explains the reports and analytics needed.Operational V iew : This explains how the system is going to be maintained post-launch. Often, you define service-level agr eements ( SLAs ), alert and monitoring functionality , a disaster recovery plan, and a support plan for the system. The operational view also providesdetails of how system maintenance is going to be carried out, such asby deployment of a bug fix, patching, backup and recovery , handling security incidents, and so on. All the views listed make sure the SAD covers all aspects of the system andstakeholders. Y ou may choose to include additional views—such as  a physical architecture view , a network architecture view , and a security (controls) architecture view , and so on— as per the stakeholder's requirement. As a solution architect, you need to provide a comprehensiveview of system functioning and understanding. Let's explore the structure ofthe SAD in more detail, in the next section. Structur e of the SAD The structure of the SAD can dif fer from project to project as per stakeholder requirements and the nature of the project. Y our project could be creating a new product from the ground, modernizing a legacy application,or moving the entire system to the cloud. For each project, the SADdocument may dif fer , but, overall, it should consider various stakeholders' views and consider the necessary sections, as shown in the followingscreenshot: S t r u c t u r e o f a S A D In the preceding SAD structure, you can see dif ferent sections covering multiple aspects of solution architecture and design. The solution architectmay choose to add additional subsections or remove some sections as per theproject requirement. For example, you can add another introduction sectionto talk about the document's purpose, with a summary . For a transition project, you may add a subsection to present the existing architecture andcompare it with the tar get architecture, and so on. Let's look into the details of each section. Solution overview In the solution overview section, you need to give a brief introduction aboutthe solution in a couple of paragraphs, describing the functioning of thesolution and its dif ferent components at a very high level. It's nice to add a high-level block diagram, showing various components in one place. Thefollowing diagram illustrates the solution overview of an e-commerceplatform: S o l u t i o n o v e r v i e w o f a n e - c o m m e r c e p l a t f o r m Y ou need to provide a brief about each component in simplified language so that the business user can understand the overall working of the solution.Major subsections include: Solution purpose : This provides a brief about a business concern that the solution is solving and the justification to build a given solution.Solution scope : This states the business scope that the proposed solution will address. Clarity describes out-of-scope items that the solution will not accommodate. Solution assumptions : List down all the assumptions based on which solution architect came up with the solution—for example, minimumnetwork bandwidth availability . Solution constraints : List all technical, business, and resource constraints. Often, constraints come from industry and governmentcompliances, and these need to be listed in this section. Y ou can also highlight the risk and mitigation plan.Solution dependencies : List all upstream and downstream dependencies. For example, an e-commerce website needs tocommunicate with a shipping system such as UPS or FedEx to ship apackage to customers.Key ar chitectur e decisions : List major problem statements and the corresponding proposed solution options. Describe the pros and cons ofeach option, why a particular decision was made, and the rationalebehind it. After giving a solution overview , you want to relate it to the business context. Let's look at the business context view in more detail, in the nextsection. Business context In the business context section, the solution architect needs to provide ahigh-level overview of business capabilities and requirements that thesolution is going to address. This section only contains an abstract view ofrequirements. Detailed requirements need to be a part of a separaterequirements document. However , the external link of the requirements document can be provided here. Y ou should include the following primary subsections: Business capabilities : Provide a brief description of business capabilities for which the solution is being designed. Make sure toinclude the benefits of capabilities and how they will address customerneeds.Key business r equir ements : List all key business concerns that the solution is going to address. Provide a high-level view of keyrequirements and add a reference to the detailed requirementsdocument.Key business pr ocesses : Solution architects should show key processes with a business process document. The following diagramillustrates a simplified view of an e-commerce application businessprocess model: B u s i n e s s p r o c e s s d i a g r a m o f a n e - c o m m e r c e p l a t f o r m Business stakeholders : List stakeholders who are directly or indirectly impacted by the project. This includes sponsors, developers, end users, vendors, partners, and so on.NFRs : Solution architects need to focus more on NFRs as these often get missed by the business user and development team. At a high level,an NFR should include: Scalability : How can the application scale as workload fluctuates? (For example, scale from 1,000  transactions per second  to 10,000 transactions per second in a given day or month.)A vailability and r eliability : What is acceptable downtime for system availability? (For example, 99.99% availability or 45minutes' downtime per month.)Performance : What is the performance requirement? Where can the system handle the load increase without impacting the end-user experience? (For example, the catalog page needs to loadwithin 3 seconds.)Portability : Can the application run on multiple platforms without any additional work? (For example, the mobile app needsto run in the iOS and Android operating systems.)Capacity : What is the maximum workload that the application can handle? (For example, the maximum number of users, thenumber of requests, expected response time and expectedapplication load, and so on.) The conceptual view of architecture is a sweet spot that provides a goodsystem overview for both business and technical stakeholders. Let's learnmore about the conceptual view in more detail. Conceptual solution overview The conceptual solution overview section provides an abstract-level diagramthat captures a big-picture view of the whole solution, which includes bothbusiness and technical aspects. It provides a basis for analyses and trade-of f studies that can help refine and optimize the solution architecture insuf ficient detail, to support solution design and implementation. The following diagram illustrates a conceptual architecture diagram of an e-commerce platform: C o n c e p t u a l a r c h i t e c t u r e d i a g r a m o f a n e - c o m m e r c e p l a t f o r m The preceding diagram shows an abstract view of significant modules andinformation flow between them. The conceptual architecture provides a goodunderstanding of the overall architecture for both business and technicalusers. However , technical users need further architectural depth. Let's dive deeper into the solution architecture, in the next section.Solution ar chitectur e  The solution architecture section dives deep into each part of the architectureand provides dif ferent views that the technical team can use to create a detailed design and work on implementation. These views could tar get dif ferent user groups such as developers, infrastructure engineers, DevOps engineers, security engineers, user experience ( UX ) designers, and so on. Let's get into the following major subsections to learn more details: Information ar chitectur e : This section provides a user navigation flow to the application. At a high level, the solution architect needs toput in an application navigation structure. As shown in the followingdiagram, for an e-commerce website, it is taking three clicks for theuser to navigate to the desired page: I n f o r m a t i o n a l a r c h i t e c t u r e d i a g r a m o f a n e - c o m m e r c e p l a t f o r m Solution architects can add more details such as website navigation,taxonomy , or a high-level wireframe that UX designers can use to generate a detailed wireframe. Application ar chitectur e : This section tar gets the development team. It provides more implementation details upon which a softwarearchitect or development team can build a detailed design. Thefollowing diagram shows the application architecture for an e-commerce website, with technology building blocks such as caching,networking, content distribution, data store, and so on: A p p l i c a t i o n a r c h i t e c t u r e d i a g r a m o f a n e - c o m m e r c e p l a t f o r m For an application modernization architecture, this section lists allapplication modules that need to retire, retain, re-platform, andtransform. Data ar chitectur e : This section is primarily utilized by the database admin and development team to understand database schemas and howtables are related to each other . Often, this section includes an ER diagram, as shown in the following screenshot: E R d i a g r a m o f a n e - c o m m e r c e p l a t f o r m The data architecture section lists all data objects that need to beconsidered during application development. Integration ar chitectur e : This section mainly tar gets vendors, partners, and other teams. For example, as shown in the followingdiagram, it shows all integration points with other systems for an e-commerce application: I n t e g r a t i o n a r c h i t e c t u r e d i a g r a m o f a n e - c o m m e r c e p l a t f o r m The integration architecture section lists all upstream anddownstream systems, and dependencies among them, regarding yourapplication. Infrastructur e ar chitectur e : This section is primarily tar geted at the infrastructure team and system engineers. The solution architect needsto include the deployment diagram, which can give a view of thelogical server location and its dependencies. For example, the followingdiagram illustrates the production deployment diagram for an e-commerce application. Y ou can produce a separate diagram for other environments such as dev , quality assurance ( QA ), and User Acceptance T esting ( UA T ) environments: D e p l o y m e n t d i a g r a m o f a n e - c o m m e r c e p l a t f o r m This section lists all server configuration, databases, networks, andswitches to deploy the application. Security ar chitectur e : This section includes all the security and compliance aspects of the application, including: Identity and Access Management ( IAM ) such as Active Dir ectory ( AD ), user authentication, authorization management, and so on.Infrastructur e security  such as firewall configuration, intrusion pr evention system ( IPS )/ intrusion detection system ( IDS ) needed, antivirus software, and so on.Application security  such as W AF , distributed denial-of-service ( DDoS ) protection, and so on. Data security at rest and in-transit using Secur e Sockets Layer ( SSL ), encryption algorithms, key management, and so on. Overall, the solution architect can include an application threat model toidentify any potential vulnerabilities such as cr oss-site scripting ( XSS ), SQL injection ( SQLi ), and so on, and plan to protect the application from any security threat. Solution delivery The solution delivery section includes essential considerations to developand deploy a solution. It can consist of the following major subsections: Development : This section is essential for the development team. It talks about development tools, programming language, coderepository , code versioning, and branching, with the rationale behind choices.Deployment : This section mainly focuses on DevOps engineers, and talks about the deployment approach, deployment tools, variousdeployment components, and deployment checklist, with the rationalebehind choices.Data migration : This section helps the team to understand data migration and the ingestion approach, scope of data migration, variousdata objects, data ingestion tools used, source of data and data format,and so on.Application decommissioning : This section lists existing systems that need to be decommissioned and an exit strategy for the current systemif the  r eturn on investment ( ROI ) is not being realized. The solution architect needs to provide an approach and timeline fordecommissioning the old system and carry out an overall impactassessment. The SAD includes a development approach and tools. However , it does not include an application-level detailed  design, such as a class diagram or adding pseudocode, as such details need to handled by the softwarearchitect or senior developer under the corresponding software applicationdetails design document. As a solution gets deployed, it needs to bemanaged in production. Let's learn about the details that go into the solutionmanagement section. Solution management The solution management section is focused on production support andongoing system maintenance across other non-product environments. Thesolution management section is primarily tar geted at the operations management team. This section addresses the following areas: Operational management such as system patching and upgrades of dev , test, staging, and prod environments.T ools to manage application upgrades and new releases. T ools to manage system infrastructure. System monitoring and alerts; operations dashboard.Production support, SLA, and incident management.Disaster recovery and Business Pr ocess Continuation  ( BPC ). During solution design, a solution architect needs to do research and collectdata to validate the right solution. Such kinds of additional details can beput in the Appendix section. Let's learn more details of the Appendix section of a SAD. Appendix section of SAD Like every business proposal document, a SAD also has an Appendix section that is pretty open, to put any data that supports your choicesregarding the overall architecture and solution. In the Appendix section, the solution architect can include open issues, any research data such as theoutcome of the POC, tools comparison data, vendors' and partners' data, andso on. In this topic, you got a good overview of the SAD structure with dif ferent sections. A SAD should include the major sections mentioned previously;however , the solution architect may choose to exclude some sections or include additional sections as per or ganization and project requirements. As with other documents, it's essential to continue to iterate upon SADs andlook for an opportunity to improve. More robust SADs lead to well-definedimplementation guidelines and reduce any risk of failure.  A SAD is a running document that gets created during the initial stages andis kept up to date over the years based on various changes throughout theapplication life cycle.  In addition to the SAD, solution architecture often gets involved in a significant procurement proposal that has a specificrequirement and is known as a r equest for x ( RFx ) document. Let's familiarize ourselves with RFx documents. IT pr ocur ement documentation for a solution ar chitectur e  IT procurement documents are popularly known as RFx documents . This a term that includes dif ferent stages of the procurement process. When you refer to RFx, it references the formal requesting process. RFx documentsare categorized as RFP , RFI,  or RFQ documents . Solution architects are often involved in the procurement process to providetheir input or lead them. These procurements may be related to outsourcing,contracting, procuring a software such as a database or development tools,or buying SaaS solutions. As these documents could be highly technical andhave a long-term broad impact, the solution architect needs to provideinput, or respond to any procurement requirement and prepare the invite.Let's understand the dif ference between dif ferent RFx documents, as follows: RFI : RFI comes early in the procurement process, where buyers invite information from dif ferent vendors to make an informed decision regarding their choice of procurement for a later stage. An RFIdocument collects information about the capabilities of the varioussuppliers, where the buyer can compare all suppliers in a similarparameter and proceed to the next proposal steps with shortlistedsuppliers. RFP : In this process, shortlisted suppliers from the RFI process have more information about the outcome of the project. An RFP documentis more open than an RFI one, where suppliers can provide the bestway to acquire solutions for the buyer . The supplier can include multiple choices, with pros and cons of each approach.RFQ : In this process, buyers narrow down the requirement compared to the RFP and list down the exact requirement of work, equipment,and supplies. Suppliers need to provide a cost for the listed requirements, and the buyer can choose the best quotation among themto award the contract. RFP is the most popular choice, as often, to speed up the process, buyer'sor ganization choose to ask for the RFP document only from potential vendors. In such a situation, the RFP document needs to have the structurein place so that that buyer can put a clear comparison between preferredvendors in terms of capabilities, solution approach, and cost to make aquick decision. Due to technicalities of procurement in IT or ganizations, solution architects play an essential role in evaluating vendors' capabilities and approachesfrom the buyer side or responding to RFP documents from the supplier side. Summary The purpose of a SAD is to keep all stakeholders on the same page and getformal agreement on solution design and requirements. As stakeholderscomprise of both business and technical users, you learned about variousviews of the SAD that the solution architect needs to consider . Y ou need to include views for non-technical users, such as business, process, and logicalviews. For technical users, include views such as application, development,deployment, and operational views. Y ou learned about the detailed structure of the SAD, with major sections and subsections. V arious sections of the SAD include details such as an overview of the solution, business, and conceptual architecture. Y ou also learned about various architecture views such as application, data,infrastructure, integration, and security , with reference to the architecture diagram. Y ou learned about other sections for solution delivery consideration and operation management. It was a long journey of learning. Y ou are almost at the end of the book, but before closing, you need to learn some tips to becoming a solution architectand continuing to improve on your knowledge. In the next and final chapter , you will learn about various soft skills such as communication style, ownership, critical thinking, and continuous learningtechniques to become a better solution architect. Learning Soft Skills to Become aBetter Solution Ar chitect In the previous chapters, you learned about how a solution architect needsto accommodate all stakeholders' needs. Even if the solution architect's roleis a technical one, they need to work across the or ganization, from senior management to the development team. T o be a successful solution architect, soft skills are essential and critical factors. A solution architect should keep themself up-to-date with currenttechnology trends, keep evolving their knowledge, and always be curious tolearn new things. Y ou can become a better solution architect by applying continuous learning. In this chapter , you will learn about methods to learn new technologies, and how to share and contribute back to the technicalcommunity . A solution architect needs to define and present an overall technical strategyto address business concerns, which requires excellent presentation skills. Asolution architect needs to work across business and technical teams tonegotiate the best solution, which requires excellent communication skills.In this chapter , you will learn the soft skills a solution architect must have, such as: Acquiring pre-sales skillsPresenting to C-level executivesT aking ownership and accountability Defining strategy execution and Objectives and Key Results ( OKRs ) Thinking bigBeing flexible and adaptableDesign thinkingBeing a builder by engaging in coding hands-onBecoming better with continuous learningBeing a mentor to others Becoming a technology evangelist and thought leader By the end of this chapter , you will learn about the various soft skills required for a solution architect to be successful in the role. Y ou will learn about a method to acquire strategic skills (such as pre-sales and executivecommunication), and develop design thinking and personal leadership skills(such as thinking big and ownership). Y ou will learn about techniques to establish yourself as a leader and continue improving your skill set. Acquiring pr e-sales skills Pre-sales is a critical phase for complex technology procurement, wherebythe customer collects detailed information to make a buying decision.  In the customer or ganization, a solution architect is involved in the pre-sales cycle to procure technology and infrastructure resources from various vendors. Inthe vendor or ganization, the solution architect needs to respond to customers'  r equests for pr oposals  ( RFP ) and present a potential solution to acquire new business for an or ganization.  Pre-sales requires a unique skill set that combines strong technical knowledge with soft skills, including: Communication and negotiation skills : Solution architects need to have excellent communication skills to engage the customer with thecorrect and latest details. Presenting precise details of the solutionalong with industry relevance helps customers to understand how yoursolution can address their business concerns. Solution architects workas a bridge between the sales and technical teams, which makescommunication and coordination a critical skill. Solution architects also need to get an agreement by collaboratingwith customers and internal teams, which requires excellentnegotiation skills. In particular , strategic-level decisions have a significant impact across multiple groups, where solution architectsneed to negotiate between the team, work on trade-of fs, and come up with an optimizing solution.  Listening and pr oblem-solving skills : Solution architects need to have strong analytical skills to identify the right solution as per thecustomer need. The first thing is to listen to and understand customeruse cases by asking the right questions to create a good solution. Y ou need to understand gaps and come up with a solution to result inimmediate business impact with long-term r eturns on investment ( ROIs ). For some customers, performance is more important, while others may be more focused on cost, based on their application user base. The solution architect needs to provide the right solution as pertheir customer's primary key performance indicator  ( KPI ) goal. Customer -facing skills : Often, the solution architect needs to work with both the internal team and the external customer's team. Theyinfluence stakeholders at all levels, from C-level executives todevelopment engineers. They present solutions and demos to seniormanagement, where they look at your proposal more from a business-prospecting perspective. C-level executive support and commitment to initiatives alwaysresult in the success of the adopted solution, which makes customer - facing skills very important. The C-level executive needs details ofthe solution in a defined time-bound meeting, and the solutionarchitect needs to utilize the allotted time to their best advantage.Y ou will learn more information about the executive conversation in the next section of this chapter— Pr esenting to C-level executives .   W orking with teams : The solution architect establishes a relationship with both the business team and the product team. T o prepare an optimal application, the solution architect needs to work with thebusiness team and technical team at all levels. The solution architectneeds to be a good team player and work with multiple teams, shareideas, and find a way of working. The aforementioned skills are not only required for pre-sales but are alsoapplicable to the solution architect's day-to-day job functions. Solutionarchitects come from a technical background, and, being in such a role, theyneed to acquire critical skills to communicate at an executive level. Let'slearn more about the executive conversation in the next section. Pr esenting to C-level executives A solution architect needs to handle various challenges from a technical andbusiness perspective. However , one of the most challenging tasks could be to get executive buy-in. Senior executives such as the Chief Executive Officer ( CEO ), Chief T echnology Officer ( CT O ), Chief Financial Officer ( CFO ), and Chief Information Officer ( CIO ) are regarded as C- level as they have a tight schedule and need to make lots of high-stackdecisions. As a solution architect, you may have lots of details to present,but your C-level meetings are very time-bound. Here, they need to make themaximum value of their meeting in the allotted time slot.  The primary question is: How to get senior executives' attention and support in a limited time?  Often, during any presentation, people tend to put a summary slide at the end, while, in the case of executive meetings, yourtime may further reduce as per their priority and agenda. The key to anexecutive presentation is to summarize the primary points upfront in thefirst 5 minutes. Y ou should prepare in such a way that if your 30-minutes slot reduces to 5 minutes, you should still be able to convey your points andget buy-in for the next step. Explain your agenda and meeting structure even before the summary . Executives ask lots of questions to make proper utilization of their time, andyour agenda should convey that they will get the chance to ask aclarification question. Support your summary based on facts and data thatalign with their industry and or ganization. Keep details with you in case they want to dive deep into a particular area; you should be able to pull upand show all data. Don't try to present everything in detail by stating information that mayseem relevant from your perspective but maybe doesn't make much sensefor an executive audience. For example, as a solution architect, you mayfocus more on benefits from the technical implementation. However , senior management focuses more on ROI by reducing operational overhead and increasing productivity . Y ou should be ready to answer the following questions that concern executives more: How the pr oposed solution will benefit our customers? : Business revolves around the customer , while executives are looking at their company growth, but that is only possible if their customers aresatisfied. Make sure to do your research on their customer base andtheir needs. Be ready to present benefits backed by reliable data.What assumption did you make to baseline the solution? : Often, these meetings are at the initial phase when you may not have enoughdetails. Solution architects always need to make some assumptions tobaseline the solution. List down your hypothesis in bullet points, andhave a mitigation plan associated with it in case things don't work asper assumption.What will be my ROI? : Executives are always looking for  ROI  by determining the  total cost of ownership  ( TCO ). Be ready with data to provide an estimated cost of ownership, solution maintenance cost,training cost, overall cost savings, and so on.What happens if we continue as it is today and do nothing? : Senior management may go into extreme vetting mode to identify ROI. Theywant to understand if the investment is worth it. Y ou need to be ready with your market research—for example, technology trends, customertrends, and competitive situation.What will be our competitor's r eaction in r egard to your solution? : Competition is everywhere, and often, the executive worries moreabout it. They want to understand if your solution is innovative to beatthe competition and give their or ganization the edge. It's better to do some upfront research and add competitiveness data relating to theirindustry and customer base.What is your suggestion, and how can I help? : Y ou should always have a crisp list of action items as the next step while providing yoursuggestion. Y ou need to get buy-in from executives and make them feel involved by asking for help. For example, you can ask the CIO toconnect you with the engineering team or product team to take anoverall solution to the next step. T ill now , in this chapter , we have talked about various soft skills such as communication, presentation, listening, and so on. Let's now look more atthe leadership skills a solution architect should have as a technical leaderfor the or ganization. T aking ownership and accountability T aking ownership and positioning yourself as a leader helps you to win trust with accountability . Ownership doesn't mean that you need to execute things alone; it is more about taking new initiatives and holding on to themas it is your or ganization. Y ou can have ideas that can benefit your or ganization in terms of productivity , agility , cost-saving, and increasing the customer base. Sometimes, you may not have the time or resources toexecute your idea, but you should always try to bring it forward as a newinitiative, and engage others for execution. Accountability is about taking responsibility to drive the outcome.Ownership and accountability go hand in hand, where you are not onlycreating initiative but working on getting the result. People can trust you toexecute any job and drive results. Accountability helps you to build trustwith your customers and team, which ultimately results in a better workenvironment and achieving a goal. As a solution architect, when you take ownership it helps you to see thingsfrom the customer's and sponsor's perspective. Y ou feel motivated and a part of something meaningful that you enjoy doing. Make sure to define andcreate key successes and the objective key result. The goal/objective shouldbe measurable using specific key results, and they must be time-bound.Let's learn more about Objectives and Key Results ( OKRs ). Defining strategy executionand OKRs Strategy execution is complex and challenging. Excelling in strategyexecution is essential for realizing the or ganizational vision, mission, and goals. The idea needs to be brought into actionable elements to keep teamsaligned and everyone moving in the same direction. Goal setting andmanagement of goals is one of the best-established ways to get things donein an or ganization. OKRs are principles and practice (vision and execution) of goal setting.OKR is a strategy management system that focuses on strategy execution.OKR is a simple framework that lets you define the or ganization's primary strategy and its priorities. Objectives are the principles, and key results arethe practice—it is a what and how  of or ganizational vision. OKRs are based on four superpowers, as illustrated in the following diagram: S u p e r p o w e r s o f O K R s OKRs' superpowers include: Focus : Start with the question:  What ar e our main priorities, and wher e should people concentrate their efforts? Commit to what truly matters and provide clarity on what is essential.Alignment : Make goals public and transparent. Connect with the team and get cross-team, bottom-up, and sideways alignment.T racking : V isually track down the key results of each objective, down to the percentage point.Str etching Goals : Create ambitious goals to achieve something remarkable. Stretching goals allows people to reimagine and rethink. OKRs provide visibility and a meaningful outcome to all stakeholders atvarious levels, from executive sponsors to teams. OKRs make the visionand mission of the or ganization clear . T eam members that are working on day-to-day activities need visibility and clarity to the mission. They need to see how their everyday work has an impact on that or ganizational mission. The OKR framework allows you to define this link and provide visibilityand meaning for everyone who is on the team. Thinking big Solution architects should have the ability to see the big picture and thinkahead. A solution architect creates a foundation upon which the team putsbuilding blocks and launches the product. Thinking big is one of the criticalskills that solution architects should possess to think about the long-termsustainability of an application. Thinking big doesn't mean you need to takea very unrealistic goal. Y our goal should be high enough to challenge you and bring you out of your comfort zone. Thinking big is critical for successat both a personal and an or ganizational level. Y ou should never doubt your capability while thinking big. Initially , it may seem challenging to achieve, but as you start working toward the goal, youwill find the way . Believe in yourself and you will notice that others start supporting and believing in you. Thinking big helps to inspire peoplearound you to become a part of your success. Set up long-term goals, suchas Wher e do you want to see yourself and your or ganization in the next decade?  T ake one step at a time to gear a short-term goal to a long-term goal. Once you set up the stretching goal by thinking big, it will help you to takelar ge initiatives and explore new challenges. However , to deliver the result, you need support from your peers and team, who can provide you with theright feedback and extend help as needed. Become a person whom peoplewant to help; and, of course, this is a two-way door . T o get help, you need to be open to helping others. Adaptability is another critical skill forsolution architects to work with others. Let's learn more about it. Being flexible and adaptable Adaptability and flexibility go hand in hand, and you need to be flexible toadapt to the new environment, working culture, and technology . Adaptability means you are always open to new ideas and to working withthe team. T eams may adopt a process and technology that is best suited for them. As a solution architect, you need to be flexible in accommodatingteam requirements during solution design.  A simple example of flexibility in a technical implementation can befacilitated by a microservices architecture. In a microservices architecture,each service communicates with the others via a standard RESTful APIover the HTTP protocol. Dif ferent teams may choose to write code in a dif ferent language of their choice, such as Python, Java, Node.js, or .NET . The only requirement is that teams need to expose their APIs securely sothat the entire system can build upon utilizing them. T o get a more innovative solution, you need a dif ferent mindset and perspective to look into the problem. Encouraging teams to fail fast andinnovate helps an or ganization to be competitive. The personal traits of flexibility are demonstrated by: Thinking about various solutions to solve a problem with the team andtake the best approach.Helping team members to of fload their work. V olunteering to fill up a place if a team member needs to take time of f for weeks due to personal work reasons.Being able to collaborate ef fectively with teams across dif ferent locations and time zones. Y ou need to be open-minded and adaptable to changes in technology and processes. Y ou may face resistance when bringing change to your team or or ganization. Y ou need to encourage others to be flexible and convey the importance of change. For example, when an or ganization wants to move its workload from on-premises to cloud, they often face a situation ofresistance, as people have to learn a new platform. Y ou need to explain the value proposition of the cloud and how it will help them to be more agileand innovate faster . As a solution architect, you need to be adaptable to carry out multipleassignments and set the right priority for execution. Y ou should have the ability to adjust to the situation and work under pressure. A solutionarchitect needs to have critical design thinking to create an innovativesolution. Let's learn more about design thinking in the next section. Design thinking A solution architect has the primary role of system design, which makesdesign thinking an essential skill.  Design thinking is one of the most successful approaches adopted across industries to solve a challenging andunclear problem. Design thinking helps you to look at problems andsolutions from a dif ferent perspective, which you might not have considered in the first instance. Design thinking is more focused on delivering results byproviding a solution-based approach to solve the problem. It helps to prettymuch question the problem, solution, and associated risk, to come up withthe most optimized strategy . Design thinking helps you to redefine problems in a more human-centricway by putting yourself in place of end users and customers. The followingdiagram illustrates the primary  principles of design thinking: P r i n c i p l e s   o f d e s i g n t h i n k i n g The following points are some design-thinking principles: Emphasis on people : Collect feedback from various users and put yourself in their place to understand the problem from a dif ferent perspective.Cr oss-collaboration : Bring in people from dif ferent backgrounds to look for problems in a diversified way , and make sure solutions accommodate everyone's needs.Think thr ough the design pr ocess : Understand the overall design process, with clear goals and methods.Show and tell : Present your thoughts in visuals so that it will be easy to grasp for everyone in the room. Define the pr oblem clearly : Create a well-defined and clear vision for a given challenge, which can help others understand clearly andencourage them to contribute more.Experiment often : Create a prototype to understand the implementation of the idea in real-life situations. Adopt a fail-faststrategy and experiment more often.Bias for action : The ultimate design to deliver a solution rather than just thinking. Be proactive in pushing forward and coming up withactivities that can result in a workable solution. Design thinking has a solid foundation to apply empathy and create a holisticview of the given problem. T o adopt design thinking, there is a five-phase model proposed by d.school ( https://dschool.stanford.edu/resources/getting-start ed-with-design-thinking ). They are a pioneer in teaching and applying design thinking. The following diagram illustrates the five phases of designthinking: F i v e p h a s e s o f d e s i g n t h i n k i n g Design thinking is an iterative approach that needs to evolve continuously . The output from one phase can recursively be input to other phases until thesolution gets solidified. A brief overview of the phases follows: Empathize : Empathy is the building block and foundation of design in the human context. T o empathize, you should observe your user behaviors and engage with them to understand the actual issue. T ry to immerse yourself in—and experience—the problem by putting yourselfin the situation.Define : Empathizing helps to define the problem as you experience the user's needs and the problem they are facing. In the define mode, youapply your insight and define the problem clearly , which can fuel brainstorming to find an innovative yet simple solution.Ideate : The ideation phase is about moving from problem to solution. Y ou work with the team to find various alternative solutions by challenging assumptions. Y ou need to get an obvious solution out of your head and work collaboratively to find all possible solutions, whichallows for innovation.Pr ototype : The prototype phase helps to convert ideas into a concrete solution. Prototyping can provide lots of learning and help to resolvedisagreements by showing a pr oof of concept ( POC ). It helps to find out gaps and risks. Y ou should build a quick prototype without lots of investment, which allows you to handle failure and increase learning. T est : The test phase is about getting feedback on your solution and reiterating accordingly . The test phase helps to redefine the solution and learn more about your users. Design thinking accommodates all the phases required to come up with alogical and practical solution. Y ou can relate the phases and principles of design thinking in your real life when designing an application architecture.There is special stress on prototyping, as that is the only way to solidify yourproposal and existing solutions with data and facts. A s olution architect's primary job is to understand the business concern and create a technicalsolution design with a prototype that the team can implement.  T o build a prototype, the solution architect needs to get their hands dirty and engage incoding hands-on. Let's learn more about it. Being a builder by engaging incoding hands-on A solution architect is a builder who learns by doing. A prototype is worth athousand pictures. It helps to reduce miscommunication and ideatesolutions. Presenting a POC and prototyping is an integral part of thesolution architect's role. Prototyping is the pre-solution phase, which helpsto deepen your understanding of the application design and user . It helps you to think and build multiple solution paths. W ith the testing of the prototype, you can refine your solution and inspire others, such as teams,customers, and investors, by demoing your vision. A solution architect is a technical leader who works closely with thedevelopment team. In the empowered agile team of developers, a solutionarchitect needs to show a piece of code as a POC, in addition to aPowerPoint presentation. A solution architect doesn't need to be part of thedevelopment team but works in collaboration to convey the solution to thedev team in their language. Successful delivery is only possible if thesolution architect can understand the deep technical aspect of a solution thatcomes with continuous coding, hands-on. A solution architect is often seen as a mentor and player -coach; having some hands-on coding helps them to establish credibility . A solution architect needs to take a strategic decision regarding which programminglanguages and tools the team should use. A hands-on approach helps toidentify any gaps that may not fit for your team or solution need—alwayslearning new technology helps the solution architect to make a betterdecision on behalf of the or ganization. Let's learn more about the techniques of continuous learning. Becoming better with continuouslearning Solution architects need to continually absorb new knowledge and enhancetheir skill set to help the or ganization in better decision making. Continuous learning keeps you relevant and builds confidence. It opens up your mindand changes prospects. Learning could be challenging with a full-time joband a busy family life. Continuous learning is about developing the habit ofalways learning something new , whereby you have to be motivated and disciplined. Y ou first need to set up learning goals and apply ef fective time management to achieve them. This often slips through the net when you getbusy with regular daily work. Everyone has their own style of learning. Some people may like formaleducation; some may read books; others may want to listen to and watchtutorials. Y ou need to find the learning style that is most ef fective for you and suited to your lifestyle. For example, you can choose to listen toaudiobooks and tutorials when commuting to work. Y ou can read books during a business-trip flight or watch video tutorials during exercise hoursin the gym. Overall, you need to make some adjustments to put time asidefrom your busy work life for continuous learning. Here are some of theways to engage yourself in constant learning: Learning new technologies, frameworks, and languages by tryingthem out : Solution architects are the builders and are ready to experiment hands-on. As a successful solution architect, you need tokeep learning new technologies by building a small POC. Getting anunderstanding of modern programming languages and frameworks willhelp you to provide the best advice on technology adoption for anor ganization and team.  Learning new skills by r eading books and tutorials : Online learning has brought a revolution and has made it easy to learn and dive deep in any area. Y ou now have massive knowledge bases at your fingertips, to learn anything. An online platform such as Udemy or Courseraprovides thousands of video tutorial courses in all areas that you canwatch online or download to your device for of fline learning. Similarly , there are millions of books available in Kindle to read anytime and anywhere. Audiobook platforms such as Audible andGoogle Play's audiobooks can help you to listen to the book duringyour commute. There are so many convenient resources availablethat there is no excuse not to apply continuous learning. Keeping up with technology news and developments by r eading articles on websites and blogs : The best way to keep yourself updated with technology trends is by subscribing to technical news and blogs.T echCrunch.com, W ired.com, and Cnet.com are some of the popular websites to get the latest technology trends. A major newspaper suchas CNBC  or  The New Y ork T imes , BBC News and CNN channels, and so on have technology articles that give a good insight into industrytrends. Y ou can subscribe to blogs for new learning in the respective technology area. For example, for cloud platform learning, you cansubscribe to Amazon W eb Services ( A WS ) blogs, which has thousands of articles and use cases in the area of A WS Cloud, and similar blogs are available from other public clouds such as Azure andGoogle Cloud Platform ( GCP ). W riting your blog, whitepaper , and book : Sharing knowledge is the best way to learn as you think through use cases when trying to presentto others. Publishing blogs and articles in popular blog-publishingplatforms such as medium.com, Blogger , and T umblr helps you to share your learning and also learn from others. Active participation inquestion-and-answer platforms helps you to find an alternativesolution for any given problem. Some popular question/answerplatforms are Quora, Reddit, StackOverflow , Stack Exchange, and so on. Solidify your knowledge by teaching others : T eaching others helps you to collaborate and get a dif ferent perspective of your knowledge. Often, use cases asked by participants give you dif ferent ways of finding a solution. Running a full-day workshop with a hands-on laband concept building helps you to solidify your learning and learn withothers. T aking online classes : Sometimes, you want to go for formal learning to be more disciplined, and you want to be flexible. Online coursesprovide flexibility and help you to adjust to other priorities and savetime. Online courses can of fer you an or ganized way to learn new technology and help to enhance knowledge.  Learning fr om teammates : T eammates share the same context, and you spend most of the day with them. Learning with team memberscan help to speed up your learning. The team can adopt a divide-and-conquer strategy whereby each team member can share the topicsamong them and present dive-deep brown-bag sessions. Brown-bagsessions are a standard method used by many or ganizations to conduct regular learning sessions among team members. Each team membershares their new learning in a weekly brown-bag learning session, andeveryone quickly learns new topics.Attending and participating in user gr oups and confer ences : All lar ge vertical industry and technology or ganizations conduct a conference to provide insight on new technology trends and hands-onsessions. Participating in industry conferences and user group meetingshelps to develop networking and understand technology trends. Someof the lar ge technology conferences from industry leaders include A WS re:Invent, Google Cloud Next, Microsoft Ignite, SAP SAPPHIRE, Strata Data conference, and so on. Y ou can create a local user group and conduct a meet-up in your local area, which will helpyou to collaborate with professionals across industries andor ganizations. A solution architect plays a technical leadership role, and a good leadershipwarrant to prepare more leaders like you, which is possible throughmentorship. Solution architects should play a player -coach role and mentor others. Let's look at this in more detail. Being a mentor to others Mentoring is about helping others and setting them up for success based onyour learning and experience. It is an ef fective way to develop leaders by having one-to-one mentor/mentee relationships. T o be a good mentor , you need to establish an informal communication style where the mentee candevelop a comfort zone. The mentee can seek advice in multiple areas suchas career development, or in personal aspects such as work-life balance.Y ou should do an informal needs assessment and set up mutual goals and expectations. Mentorship is more about listening. Sometimes, people need someone tolisten to them and advise as required. Y ou should listen carefully first and understand their point of view . Help them to make their own decisions as this will make them feel more accomplished. As a good mentor , when advising for a career , you need to be open to advise what is the best fit for the mentee, even if it may not necessarily be the best fit for the company . Always provide honest, constructive feedback to help them identify gapsand overcome them. The critical trait of a mentor is the ability to inspire people. Often, peoplemay choose you as a mentor if they see a role model in you. Help yourmentee to realize their full potential without putting your view forward, andhelp them achieve what they never thought of earlier . There are always mutual benefits to being a mentor; you also learn a lot from mentees aboutpeople's behaviors and growth. Being a mentor to others will ultimatelyhelp you to become a better leader and person. Becoming a technology evangelistand thought leader T echnology evangelism is about being an expert, to advocate technology and your product. Some or ganizations with a lar ge product base roll out a separate technology evangelist role, but often, a solution architect needs totake the role of an evangelist as part of their job. As a technologyevangelist, you need to be between people to understand real-worldproblems and advocate your technology to solve their business concerns. T echnology evangelism involves participating in an industry conference as a public speaker and promoting your respective platform. It allows you tobecome a thought leader and an influencer , which can help the or ganization to increase the adoption of its platform and product. Public speaking is oneof the critical skills required for a solution architect, in order to interreact onvarious public platforms and present in front of a lar ge audience. An evangelist also creates and publishes content such as blog posts,whitepapers, and microblogs to advocate their product. They socialize thecontent to increase adoption and interreact with the user to understand theirfeedback. An evangelist works backward from the customer andcommunicates feedback to the internal team to help to make the productbetter . W ith time, as an evangelist, you will refine the message that works in the best interests of the or ganization. Overall, a solution architect is a role with multiple responsibilities, andtaking more ownership will help you to better succeed in your career . Summary In this chapter , you learned about the various soft skills required for a solution architect to be successful. A solution architect needs to have pre-sales skills such as negotiation, communication, problem-solving, andlistening, which help them to support the pre-sales cycle for theor ganization, such as with the RFP . Y ou learned about the presentation skills required for executive conversation and buy-in. Y ou learned about the strategic understanding that a solution architect should have to define key objectives and results for an or ganization. T o execute at various levels, solution architects should have the ability to thinkbig and be flexible and adaptable. Y ou learned details about solution architects taking ownership and being accountable for their actions. A solution architect's role has the primary responsibility of architecturedesign. Y ou learned about design thinking, with its principles and phases. Y ou also learned about the importance of continuous learning and dif ferent techniques to keep learning and keep yourself up-to-date with markettrends. Y ou also learned about the additional responsibilities of the solution architect—to work as a mentor and evangelist. It was a long journey where you went through 16 chapters to learn all aboutsolution architects, from their roles and responsibilities to a dif ferent aspect of solution design and architecture optimization. I hope you have learned alot, and that it will help you to develop your career as a solution architect orhelp you to succeed in your current role. Happy learning!!! Other Books Y ou May Enjoy If you enjoyed this book, you may be interested in these other books byPackt: Softwar e Ar chitect's Handbook Joseph Ingeno ISBN: 978-1-78862-406-0 Design software architectures using patterns and best practicesExplore the dif ferent considerations for designing software architectureDiscover what it takes to continuously improve as a software architectCreate loosely coupled systems that can support changeUnderstand DevOps and how it af fects software architecture Integrate, refactor , and re-architect legacy applications A WS SysOps Cookbook - Second Edition Eric Z. Beard, Rowan Udell, Et al ISBN: 978-1-83855-018-9 Secure your account by creating IAM users and avoiding the use of theroot loginSimplify the creation of a multi-account landing zone using A WS Control T ower Master Amazon S3 for unlimited, cost-ef ficient storage of data Explore a variety of compute resources on the A WS Cloud, such as EC2 and A WS Lambda Configure secure networks using Amazon VPC, access control lists,and security groupsEstimate your monthly bill by using cost estimation toolsLearn to host a website with Amazon Route 53, Amazon CloudFront,and S3 Leave a r eview - let other r eaders know what you think Please share your thoughts on this book with others by leaving a review onthe site that you bought it from. If you purchased the book from Amazon, please leave us an honest review on this book's Amazon page. This is vital so that other potential readers can see and use your unbiased opinion tomake purchasing decisions, we can understand what our customers thinkabout our products, and our authors can see your feedback on the title thatthey have worked with Packt to create. It will only take a few minutes ofyour time, but is valuable to other potential customers, our authors, andPackt. Thank you!","libVersion":"0.5.0","langs":""}